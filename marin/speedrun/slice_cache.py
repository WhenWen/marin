"""
Step to read a Levanter cache and produce a subsample of that cache.
"""

import dataclasses
import logging
import os
from dataclasses import dataclass

import fsspec
import humanfriendly
from jax.random import PRNGKey
from levanter.data.text import (
    HfDatasetSourceConfig,
    LMDatasetSourceConfig,
    LmDatasetSourceConfigBase,
    UrlDatasetSourceConfig,
)
from levanter.store import SerialCacheWriter, TreeCache
from transformers import AutoTokenizer

from marin.execution import THIS_OUTPUT_PATH, ExecutorStep, InputName
from marin.processing.tokenize.tokenize import TokenizeConfigBase

logger = logging.getLogger(__name__)


@dataclass
class SliceCacheConfig(TokenizeConfigBase):
    """Configuration for slicing a Levanter cache."""

    input_config: LmDatasetSourceConfigBase
    num_tokens: int
    cache_path: str = THIS_OUTPUT_PATH
    tokenizer: str = "stanford-crfm/marin_tokenizer"
    seed: int = 42

    def as_lm_dataset_source_config(
        self, actual_output_path: str | InputName | None, *, include_raw_paths=True
    ) -> LMDatasetSourceConfig:
        humanfriendly_tokens = humanfriendly.format_size(self.num_tokens)[0:-1].replace(" ", "").replace("byte", "")
        out = _patch_source_config(
            self.input_config, self.cache_path, extra_tags=["subsampled", f"subsampled-{humanfriendly_tokens}"]
        )

        return out  # type: ignore


def _do_slice_cache(
    cfg: SliceCacheConfig,
) -> LmDatasetSourceConfigBase:
    """
    Read a Levanter cache and produce a subsample of that cache.

    This only works for datasets with input ids right now. Luckily this is all the datasets we care about atm.
    """
    key = PRNGKey(cfg.seed)
    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer)
    split = "train"
    train_set = cfg.input_config.load_cache(split, tokenizer)

    # we want a random sample of docs (token seqs would probably be better but much more work)
    exemplar = train_set.as_sync_dataset()[0]

    assert "input_ids" in exemplar, "This only works for datasets with input ids right now"

    subsampled_source_spec = cfg.as_lm_dataset_source_config(cfg.cache_path)

    try:
        TreeCache.load(os.path.join(cfg.cache_path, split), exemplar)
        return subsampled_source_spec
    except FileNotFoundError:
        pass

    expected_input_ids = train_set.store.tree["input_ids"].data_size

    if expected_input_ids < cfg.num_tokens:
        raise ValueError(
            f"Cache does not seem to be big enough: {expected_input_ids} in cache but ${cfg.num_tokens} requested"
        )

    train_set_shuffled = train_set.shuffle(key).as_sync_dataset()
    num_docs = len(train_set_shuffled)

    with SerialCacheWriter(os.path.join(cfg.cache_path, split), exemplar) as output_writer:
        # TensorStore has high latency so we load biggish batches
        BS_TO_LOAD = 1024

        loaded_tokens = 0
        first_doc = 0
        while loaded_tokens < cfg.num_tokens and first_doc < num_docs:
            end_doc = min(first_doc + BS_TO_LOAD, num_docs)
            batch = train_set_shuffled.get_batch(range(first_doc, end_doc))
            first_doc = end_doc

            # decide how many docs to take from this batch
            batch_to_write = []
            for ex in batch:
                if loaded_tokens + len(ex["input_ids"]) > cfg.num_tokens:
                    break
                batch_to_write.append(ex)
                loaded_tokens += len(ex["input_ids"])

            output_writer.write_batch(batch_to_write)

    if loaded_tokens < cfg.num_tokens:
        raise ValueError("Provided cache doesn't have enough tokens")

    # These are usually uploaded to HF, so we write a README
    _create_readme(cfg.cache_path, cfg.input_config, loaded_tokens, cfg.tokenizer, cfg.seed)

    return subsampled_source_spec


def _create_readme(
    output_path: str, input_config: LmDatasetSourceConfigBase, num_tokens: int, tokenizer_spec: str, seed: int
):
    """
    Create a README file for the cache.
    """
    readme_path = f"{output_path}/README.md"
    with fsspec.open(readme_path, "w") as f:
        f.write("# Marin/Levanter Subsampled Pretokenized Dataset\n\n")
        f.write("## Dataset\n\n")
        f.write(_short_desc_from_lm_config(input_config))
        f.write("\n\n## Factsheet\n\n")
        f.write(f"* Original cache: {input_config.cache_dir}\n")
        f.write(f"* Tokenizer: [{tokenizer_spec}](https://huggingface.co/{tokenizer_spec})\n")
        f.write(f"* Seed {seed}\n")
        f.write(f"* Number of tokens: {humanfriendly.format_number(num_tokens)}\n")
        f.write("\n\n(This readme is automatically generated by Marin.)\n")


def _short_desc_from_lm_config(input_config: LmDatasetSourceConfigBase) -> str:
    """
    Get a short description of the dataset from the config.
    """
    if isinstance(input_config, HfDatasetSourceConfig):
        ds_id = input_config.id
        url = f"[{ds_id}](https://huggingface.co/datasets/{ds_id})"
        if input_config.name:
            url += f" (name: {input_config.name})"
        return url
    elif isinstance(input_config, UrlDatasetSourceConfig):
        out = ""
        if input_config.train_urls:
            out = "Train Urls: \n"
            for url in input_config.train_urls:
                out += f"- {url}\n"

        if input_config.validation_urls:
            out = "Validation Urls: \n"
            for url in input_config.validation_urls:
                out += f"- {url}\n"

        if not out:
            out = "{missing urls}"

        return out
    else:
        return ""


def _patch_source_config(
    input_config: LmDatasetSourceConfigBase, output_path: str, extra_tags: list[str]
) -> LmDatasetSourceConfigBase:
    """
    Patch the source config to point to the new cache.

    TODO: would be better to make this more explicit somehow...
    """
    return dataclasses.replace(input_config, cache_dir=output_path, tags=input_config.tags + extra_tags)


def slice_cache(
    input_config: LmDatasetSourceConfigBase,
    tokenizer_spec: str,
    output_path: str,
    num_tokens: int,
    seed: int = 42,
) -> ExecutorStep[SliceCacheConfig]:
    """High-level function to slice a Levanter cache.

    This is the main entry point for slicing a cache.

    Args:
        input_config: The input cache configuration.
        tokenizer_spec: The tokenizer specification.
        output_path: The output path for the sliced cache.
        num_tokens: The number of tokens to slice.
        seed: The random seed for shuffling the dataset.

    Returns:
        The configuration for the sliced cache
    """

    return ExecutorStep(
        fn=_do_slice_cache,
        config=SliceCacheConfig(
            input_config=input_config,
            num_tokens=num_tokens,
            seed=seed,
            cache_path=output_path,
            tokenizer=tokenizer_spec,
        ),
    )
