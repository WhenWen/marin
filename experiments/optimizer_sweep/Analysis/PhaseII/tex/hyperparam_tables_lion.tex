\subsection{Sweeping Results for Lion}% lion - 300m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Lion on 300m on 2x Chinchilla Data}
\label{tab:ablation_lion_300m_on_2x_chinchilla_data}
\begin{tabular}{cccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 0.001 & 1 & 0 & 128 & 2000 & 0.7 & 3.170 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-lion5aad2alr0.001-wd0.7-minlr0-warmup2000-b10.9-b-b039e3}{0} \\
\midrule
-- & 0.9 & -- & -- & -- & -- & -- & -- & 3.189 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-lionb554b6lr0.001-wd0.7-minlr0-warmup2000-b10.9-b-44d68f}{1} \\
-- & 0.95 & -- & -- & -- & -- & -- & -- & 3.175 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-lion1d519dlr0.001-wd0.7-minlr0-warmup2000-b10.9-b-2aa8be}{2} \\
-- & -- & 0.0005 & -- & -- & -- & -- & -- & 3.172 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-liond0e427lr0.0005-wd0.7-minlr0-warmup2000-b10.9--07e662}{3} \\
-- & -- & -- & -- & -- & 256 & -- & -- & 3.183 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-lion804c87lr0.001-wd0.7-minlr0-warmup2000-b10.9-b-d99925}{4} \\
\bottomrule
\end{tabular}
\end{table}

% lion - 300m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Lion on 300m on 4x Chinchilla Data}
\label{tab:ablation_lion_300m_on_4x_chinchilla_data}
\begin{tabular}{cccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 0.001 & 1 & 0 & 256 & 2000 & 0.7 & 3.100 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-lion5aad2alr0.001-wd0.7-minlr0-warmup2000-b10.9-b-edb7fa}{0} \\
\midrule
-- & 0.9 & -- & -- & -- & -- & -- & -- & 3.114 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-lionb554b6lr0.001-wd0.7-minlr0-warmup2000-b10.9-b-4d69db}{1} \\
-- & 0.95 & -- & -- & -- & -- & -- & -- & 3.103 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-lion1d519dlr0.001-wd0.7-minlr0-warmup2000-b10.9-b-a24dd3}{2} \\
-- & -- & 0.0005 & -- & -- & -- & -- & -- & 3.105 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-liond0e427lr0.0005-wd0.7-minlr0-warmup2000-b10.9--ab64aa}{3} \\
-- & -- & -- & -- & -- & 128 & -- & -- & 3.104 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-lioneb1a25lr0.001-wd0.7-minlr0-warmup2000-b10.9-b-8b79ae}{4} \\
\bottomrule
\end{tabular}
\end{table}

% lion - 300m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Lion on 300m on 8x Chinchilla Data}
\label{tab:ablation_lion_300m_on_8x_chinchilla_data}
\begin{tabular}{cccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 0.001 & 1 & 0 & 256 & 2000 & 0.7 & 3.046 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-lioneb1a25lr0.001-wd0.7-minlr0-warmup2000-b10.9-b-2006af}{0} \\
\midrule
-- & 0.9 & -- & -- & -- & -- & -- & -- & 3.058 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-lion2962dblr0.001-wd0.7-minlr0-warmup2000-b10.9-b-5ec1df}{1} \\
-- & 0.95 & -- & -- & -- & -- & -- & -- & 3.050 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-lion2842d8lr0.001-wd0.7-minlr0-warmup2000-b10.9-b-df0b7e}{2} \\
-- & -- & 0.0005 & -- & -- & -- & -- & -- & 3.043 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-lion698f3blr0.0005-wd0.7-minlr0-warmup2000-b10.9--6f8954}{3} \\
-- & -- & -- & -- & -- & 128 & -- & -- & 3.061 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-lion875c0alr0.001-wd0.7-minlr0-warmup2000-b10.9-b-a1a7a1}{4} \\
\bottomrule
\end{tabular}
\end{table}

% lion - 520m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Lion on 520m on 2x Chinchilla Data}
\label{tab:ablation_lion_520m_on_2x_chinchilla_data}
\begin{tabular}{cccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.95 & 0.001 & 1 & 0 & 128 & 2000 & 0.6 & 3.029 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-lion30535dlr0.001-wd0.6-minlr0-warmup2000-b10.9-b-c72b9c}{0} \\
\midrule
-- & 0.9 & -- & -- & -- & -- & -- & -- & 3.045 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-lion6d497blr0.001-wd0.6-minlr0-warmup2000-b10.9-b-deb100}{1} \\
-- & 0.98 & -- & -- & -- & -- & -- & -- & 7.779 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-lion70e78flr0.001-wd0.6-minlr0-warmup2000-b10.9-b-bd75ef}{2} \\
-- & -- & 0.0005 & -- & -- & -- & -- & -- & 3.028 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-lion9e353dlr0.0005-wd0.6-minlr0-warmup2000-b10.9--0e18f5}{3} \\
-- & -- & -- & -- & -- & 256 & -- & -- & 3.030 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-lion4bc061lr0.001-wd0.6-minlr0-warmup2000-b10.9-b-979e97}{4} \\
\bottomrule
\end{tabular}
\end{table}

% lion - 520m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Lion on 520m on 4x Chinchilla Data}
\label{tab:ablation_lion_520m_on_4x_chinchilla_data}
\begin{tabular}{cccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.95 & 0.001 & 1 & 0 & 256 & 2000 & 0.6 & 2.965 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-lion30535dlr0.001-wd0.6-minlr0-warmup2000-b10.9-b-b687e1}{0} \\
\midrule
-- & 0.9 & -- & -- & -- & -- & -- & -- & 2.971 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-lion6d497blr0.001-wd0.6-minlr0-warmup2000-b10.9-b-d19dd3}{1} \\
-- & 0.98 & -- & -- & -- & -- & -- & -- & 2.965 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-lion70e78flr0.001-wd0.6-minlr0-warmup2000-b10.9-b-fa40d0}{2} \\
-- & -- & 0.0005 & -- & -- & -- & -- & -- & 2.966 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-lion9e353dlr0.0005-wd0.6-minlr0-warmup2000-b10.9--e8f576}{3} \\
-- & -- & -- & -- & -- & 128 & -- & -- & 2.975 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-lion2b623elr0.001-wd0.6-minlr0-warmup2000-b10.9-b-f4b0ad}{4} \\
\bottomrule
\end{tabular}
\end{table}

% lion - 520m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Lion on 520m on 8x Chinchilla Data}
\label{tab:ablation_lion_520m_on_8x_chinchilla_data}
\begin{tabular}{cccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.95 & 0.0005 & 1 & 0 & 256 & 2000 & 0.6 & 2.915 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-lionf86a38lr0.0005-wd0.6-minlr0-warmup2000-b10.9--54ea21}{0} \\
\midrule
-- & 0.9 & -- & -- & -- & -- & -- & -- & 2.922 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-lion75027blr0.0005-wd0.6-minlr0-warmup2000-b10.9--094576}{1} \\
-- & 0.98 & -- & -- & -- & -- & -- & -- & 2.915 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-lion1c2a3blr0.0005-wd0.6-minlr0-warmup2000-b10.9--fdc97c}{2} \\
-- & -- & 0.001 & -- & -- & -- & -- & -- & 2.920 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-lion2b623elr0.001-wd0.6-minlr0-warmup2000-b10.9-b-a6a1ef}{3} \\
-- & -- & -- & -- & -- & 128 & -- & -- & 2.922 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-liond118d4lr0.0005-wd0.6-minlr0-warmup2000-b10.9--8f527b}{4} \\
\bottomrule
\end{tabular}
\end{table}

