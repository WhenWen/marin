\subsection{Sweeping Results for AdamW}% adamw - 300m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 2x Chinchilla Data}
\label{tab:ablation_adamw_300m_on_2x_chinchilla_data}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-15 & 0.008 & 1 & 0 & 128 & 2000 & 0.1 & 3.166 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwbd209alr0.008-wd0.1-minlr0.0-warmup2000-b10.-25750f}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & -- & -- & 3.167 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamw065712lr0.004-wd0.1-minlr0.0-warmup2000-b10.-593504}{1} \\
-- & -- & -- & -- & -- & -- & 256 & -- & -- & 3.170 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwdef996lr0.008-wd0.1-minlr0.0-warmup2000-b10.-75d2a3}{2} \\
-- & -- & -- & -- & -- & -- & -- & -- & 0.2 & 3.183 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwb1b055lr0.008-wd0.2-minlr0.0-warmup2000-b10.-63e273}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 4x Chinchilla Data}
\label{tab:ablation_adamw_300m_on_4x_chinchilla_data}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 2 & 0 & 256 & 2000 & 0.1 & 3.094 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw5dcabdlr0.008-wd0.1-minlr0-warmup2000-b10.9--5e75fb}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & -- & -- & 3.101 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamwae7cf4lr0.004-wd0.1-minlr0-warmup2000-b10.9--ee3fd2}{1} \\
-- & -- & -- & -- & -- & -- & 128 & -- & -- & 3.103 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw07fc6elr0.008-wd0.1-minlr0-warmup2000-b10.9--459481}{2} \\
-- & -- & -- & -- & -- & -- & -- & -- & 0.2 & 3.103 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw4476fflr0.008-wd0.2-minlr0-warmup2000-b10.9--1eec01}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 8x Chinchilla Data}
\label{tab:ablation_adamw_300m_on_8x_chinchilla_data}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 2 & 0 & 256 & 2000 & 0.1 & 3.043 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw07fc6elr0.008-wd0.1-minlr0-warmup2000-b10.9--b38246}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & -- & -- & 3.042 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw2537edlr0.004-wd0.1-minlr0-warmup2000-b10.9--a59fd5}{1} \\
-- & -- & -- & -- & -- & -- & 128 & -- & -- & 3.057 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw6a50aflr0.008-wd0.1-minlr0-warmup2000-b10.9--22e2a7}{2} \\
-- & -- & -- & -- & -- & -- & -- & -- & 0.2 & 3.059 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw7d9e60lr0.008-wd0.2-minlr0-warmup2000-b10.9--dfe5c9}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 2x Chinchilla Data}
\label{tab:ablation_adamw_520m_on_2x_chinchilla_data}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.2 & 3.023 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamws9d215dlr0.004-wd0.2-minlr0-warmup1000-b10.9-d16851}{0} \\
\midrule
-- & -- & -- & -- & -- & -- & 128 & -- & -- & 6.654 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamwsffc6bdlr0.004-wd0.2-minlr0-warmup1000-b10.9-646b5d}{1} \\
-- & -- & -- & -- & -- & -- & -- & -- & 0.1 & 3.025 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamws529f50lr0.004-wd0.1-minlr0-warmup1000-b10.9-c43fcb}{2} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 4x Chinchilla Data}
\label{tab:ablation_adamw_520m_on_4x_chinchilla_data}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.1 & 2.958 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwbc2b89lr0.004-wd0.1-minlr0-warmup1000-b10.9--50dbd9}{0} \\
\midrule
-- & -- & -- & 0.008 & -- & -- & -- & -- & -- & 7.075 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwf6bfb1lr0.008-wd0.1-minlr0-warmup1000-b10.9--c0b516}{1} \\
-- & -- & -- & -- & -- & -- & 128 & -- & -- & 7.139 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwf14f39lr0.004-wd0.1-minlr0-warmup1000-b10.9--48b1c7}{2} \\
-- & -- & -- & -- & -- & -- & -- & -- & 0.2 & 2.962 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwffc6bdlr0.004-wd0.2-minlr0-warmup1000-b10.9--69bc03}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 8x Chinchilla Data}
\label{tab:ablation_adamw_520m_on_8x_chinchilla_data}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.1 & 2.913 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamwf14f39lr0.004-wd0.1-minlr0-warmup1000-b10.9--84afa9}{0} \\
\midrule
-- & -- & -- & 0.008 & -- & -- & -- & -- & -- & 7.183 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamw166f50lr0.008-wd0.1-minlr0-warmup1000-b10.9--397bf5}{1} \\
-- & -- & -- & -- & -- & -- & 128 & -- & -- & 6.932 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamw2b7ec2lr0.004-wd0.1-minlr0-warmup1000-b10.9--01c1ad}{2} \\
-- & -- & -- & -- & -- & -- & -- & -- & 0.2 & 2.921 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamw61633blr0.004-wd0.2-minlr0-warmup1000-b10.9--1a26e3}{3} \\
\bottomrule
\end{tabular}
\end{table}

