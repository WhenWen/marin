\subsection{Sweeping Results for Soap}% soape - 130m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 130m on 1x Chinchilla Data}
\label{tab:ablation_soap_130m_1}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.98 & 256 & 1e-15 & 0.016 & 1.0 & 0 & True & 1 & 0.95 & 128 & 1000 & 0.1 & 3.483 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-soaplff7de8lr0.016-wd0.1-minlr0-warmup1000-b10.95--f08df8}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 130m on 16x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 130m on 16x Chinchilla Data}
\label{tab:ablation_soap_130m_16}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.98 & 256 & 1000 & 0.1 & 3.191 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-42B-soape1786aelr0.008-wd0.1-minlr0-warmup1000-b10.95-c390d2}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 130m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 130m on 8x Chinchilla Data}
\label{tab:ablation_soap_130m_8}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-15 & 0.008 & 1 & 0 & True & 10 & 0.98 & 256 & 1000 & 0.1 & 3.239 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-soape2479ablr0.008-wd0.1-minlr0-warmup1000-b10.95-a17efa}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 1.2b on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 1.2b on 1x Chinchilla Data}
\label{tab:ablation_soap_1.2b_1}
\begin{tabular}{cccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0.0 & 10 & 0.9 & 256 & 1000 & 0.1 & 2.940 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-soapebf16d04f58lr0.004-wd0.1-minlr0.0-warmup1000--82b8d3}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 1.2b on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 1.2b on 8x Chinchilla Data}
\label{tab:ablation_soap_1.2b_8}
\begin{tabular}{cccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0.0 & 10 & 0.9 & 256 & 1000 & 0.1 & 2.749 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-193B-soapeweightf32ebf088lr0.004-wd0.1-minlr0.0-warmu-029405}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 1.2b on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 1.2b on 2x Chinchilla Data}
\label{tab:ablation_soap_1.2b_2}
\begin{tabular}{cccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0.0 & 10 & 0.9 & 256 & 1000 & 0.1 & 2.829 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-48B-soapeweightf32ae127elr0.004-wd0.1-minlr0.0-warmup-000547}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 1.2b on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 1.2b on 4x Chinchilla Data}
\label{tab:ablation_soap_1.2b_4}
\begin{tabular}{cccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0.0 & 10 & 0.9 & 256 & 1000 & 0.1 & 2.783 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-96B-soapeweightf321bf579lr0.004-wd0.1-minlr0.0-warmup-7fd4b6}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 300m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 300m on 2x Chinchilla Data}
\label{tab:ablation_soap_300m_2}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.9 & 128 & 1000 & 0.1 & 3.147 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-soapefe4b166lr0.008-wd0.1-minlr0-warmup1000-b10.9-cebb81}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 300m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 300m on 8x Chinchilla Data}
\label{tab:ablation_soap_300m_8}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.9 & 256 & 1000 & 0.1 & 3.030 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-soapeae57080lr0.008-wd0.1-minlr0-warmup1000-b10.9-f53d36}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 300m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 300m on 1x Chinchilla Data}
\label{tab:ablation_soap_300m_1}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.9 & 128 & 1000 & 0.1 & 3.231 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-soape63ad3blr0.008-wd0.1-minlr0-warmup1000-b10.95--02f063}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 300m on 16x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 300m on 16x Chinchilla Data}
\label{tab:ablation_soap_300m_16}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0 & True & 10 & 0.9 & 256 & 1000 & 0.1 & 2.990 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-96B-soapef479bb0lr0.004-wd0.1-minlr0-warmup1000-b10.9-ef8190}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 300m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 300m on 4x Chinchilla Data}
\label{tab:ablation_soap_300m_4}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.9 & 256 & 1000 & 0.1 & 3.084 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-soapeie4b166lr0.008-wd0.1-minlr0-warmup1000-b10.9-9894a6}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 520m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 520m on 1x Chinchilla Data}
\label{tab:ablation_soap_520m_1}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.95 & 128 & 1000 & 0.1 & 3.079 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-soape9baa74lr0.008-wd0.1-minlr0-warmup1000-b10.95-8d30a7}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 520m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 520m on 2x Chinchilla Data}
\label{tab:ablation_soap_520m_2}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.008 & 1 & 0 & True & 10 & 0.95 & 256 & 1000 & 0.1 & 3.004 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-soapea9baa74lr0.008-wd0.1-minlr0-warmup1000-b10.9-7994bc}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 520m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 520m on 8x Chinchilla Data}
\label{tab:ablation_soap_520m_8}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0 & True & 10 & 0.95 & 256 & 1000 & 0.1 & 2.899 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-soapeaa7a19flr0.004-wd0.1-minlr0-warmup1000-b10.9-fe941f}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% soape - 520m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Soap on 520m on 4x Chinchilla Data}
\label{tab:ablation_soap_520m_4}
\begin{tabular}{ccccccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\mathrm{block size}$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{Blocking}$ & $f_{pc}$ & $\beta_{shampoo}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.95 & 0.99 & 512 & 1e-10 & 0.004 & 1 & 0 & True & 10 & 0.95 & 256 & 1000 & 0.1 & 2.944 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-soapepde58c5lr0.004-wd0.1-minlr0-warmup1000-b10.9-c834a4}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

