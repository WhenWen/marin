\subsection{Sweeping Results for AdamW}% adamw - 1.2b on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 1x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 256 & 2000 & 0.2 & 2.905 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwf115b7lr0.002-wd0.2-minlr0-warmup2000-b10.9--387e9f}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 2.905 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwf17f49lr0.002-wd0.2-minlr0-warmup2000-b10.95-444ce8}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 2.909 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwcce254flr0.002-wd0.2-minlr0-warmup2000-b10.9-744305}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 2.914 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwf7d5fdlr0.002-wd0.2-minlr0-warmup2000-b10.9--2217ed}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 2.909 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwa250dblr0.002-wd0.2-minlr0-warmup2000-b10.9--91a988}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 2.907 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwfb226elr0.002-wd0.2-minlr0-warmup2000-b10.9--56cdf6}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 2.907 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw269849lr0.002-wd0.2-minlr0-warmup2000-b10.9--f1a022}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 2.907 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwb9e3belr0.002-wd0.2-minlr0-warmup2000-b10.9--3d8fda}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 2.916 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw1edd23lr0.004-wd0.2-minlr0.0-warmup2000-b10.-92a750}{8} \\
-- & -- & -- & 0.008 & -- & -- & -- & 7.347 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw4476fflr0.008-wd0.2-minlr0-warmup2000-b10.9--5acee7}{9} \\
-- & -- & -- & -- & 128 & -- & -- & 2.904 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwc221ealr0.002-wd0.2-minlr0-warmup2000-b10.9--d14fe4}{10} \\
-- & -- & -- & -- & 512 & -- & -- & 2.928 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwb10b19lr0.002-wd0.2-minlr0-warmup2000-b10.9--2fcef1}{11} \\
-- & -- & -- & -- & 1024 & -- & -- & 2.985 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwf0dd70lr0.002-wd0.2-minlr0-warmup2000-b10.9--751ff8}{12} \\
-- & -- & -- & -- & -- & 500 & -- & 2.917 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw82e04dlr0.002-wd0.2-minlr0-warmup500-b10.9-b-4a7d30}{13} \\
-- & -- & -- & -- & -- & 1000 & -- & 2.910 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw613e35lr0.002-wd0.2-minlr0.0-warmup1000.0-b1-25ef51}{14} \\
-- & -- & -- & -- & -- & 4000 & -- & 2.912 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwf0120blr0.002-wd0.2-minlr0-warmup4000-b10.9--72e36e}{15} \\
-- & -- & -- & -- & -- & -- & 0 & 2.946 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw400796lr0.002-wd0-minlr0-warmup2000-b10.9-b2-78071b}{16} \\
-- & -- & -- & -- & -- & -- & 0.1 & 2.916 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamw8fd298lr0.002-wd0.1-minlr0-warmup2000-b10.9--2ccc79}{17} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 2x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 256 & 1000 & 0.2 & 2.836 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-48B-adamw7ebb9alr0.002-wd0.2-minlr0.0-warmup1000-b10.-994f23}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 4x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 256 & 1000 & 0.2 & 2.787 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-96B-adamw71c224lr0.002-wd0.2-minlr0.0-warmup1000-b10.-66682a}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 8x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 256 & 1000 & 0.2 & 2.752 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-193B-adamw1eeba1lr0.002-wd0.2-minlr0.0-warmup1000-b10-44a428}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 1x Chinchilla Data}
\label{tab:ablation_adamw_130m_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 128 & 2000 & 0.1 & 3.529 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw5098e9lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-bc5e36}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.539 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw6b5e10lr0.008-wd0.1-minlr0-warmup2000-b10.95--b1de93}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 3.882 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamwae7d4dlr0.008-wd0.1-minlr0-warmup2000-b10.98--df53b1}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.545 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw5a9f1blr0.008-wd0.1-minlr0-warmup2000-b10.9-b-691bb8}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.535 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamwb5ba64lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-77d771}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.529 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw0848aelr0.008-wd0.1-minlr0-warmup2000-b10.9-b-413f11}{5} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.531 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamwca195dlr0.008-wd0.1-minlr0-warmup2000-b10.9-b-1f0e34}{6} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.531 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamwb41b46lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-0cf15c}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.550 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw79cde5lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-019ae3}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 3.538 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw5f56aalr0.016-wd0.1-minlr0-warmup2000-b10.9-b-9972d1}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.781 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw73dd8elr0.032-wd0.1-minlr0-warmup2000-b10.9-b-d0f554}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.611 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw7fa927lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-8cb11e}{11} \\
-- & -- & -- & -- & -- & 500 & -- & 7.452 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw20884flr0.008-wd0.1-minlr0-warmup500-b10.9-b2-819ba9}{12} \\
-- & -- & -- & -- & -- & 1000 & -- & 3.532 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamwe4d6f6lr0.008-wd0.1-minlr0-warmup1000-b10.9-b-d51a10}{13} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.575 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw6afe2alr0.008-wd0.1-minlr0-warmup4000-b10.9-b-cff12f}{14} \\
-- & -- & -- & -- & -- & -- & 0 & 3.545 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamweb4f13lr0.008-wd0-minlr0-warmup2000-b10.9-b20-f7fd96}{15} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.536 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw2f8ed9lr0.008-wd0.2-minlr0-warmup2000-b10.9-b-f4d5cf}{16} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 2x Chinchilla Data}
\label{tab:ablation_adamw_130m_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 128 & 2000 & 0.1 & 3.409 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw90f5c1lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-67adb6}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.417 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw995582lr0.008-wd0.1-minlr0-warmup2000-b10.95--0fb321}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.557 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw220e11lr0.008-wd0.1-minlr0-warmup2000-b10.98--f8f851}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.423 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw757bfblr0.008-wd0.1-minlr0-warmup2000-b10.9-b-da391a}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.413 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamwc562dclr0.008-wd0.1-minlr0-warmup2000-b10.9-b-9752b5}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.409 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw0da527lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-9c6463}{5} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.409 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw4f03a8lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-c453b4}{6} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.410 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamwf4cc20lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-c52587}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.420 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw8cf61clr0.004-wd0.1-minlr0-warmup2000-b10.9-b-481a17}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 3.419 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw093957lr0.016-wd0.1-minlr0-warmup2000-b10.9-b-d1d1c5}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.840 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw0d117blr0.032-wd0.1-minlr0-warmup2000-b10.9-b-180de6}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.437 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw5098e9lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-b203b1}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.527 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw7fa927lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-23a9e4}{12} \\
-- & -- & -- & -- & -- & 500 & -- & 7.277 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamwb0eee1lr0.008-wd0.1-minlr0-warmup500-b10.9-b2-f92a87}{13} \\
-- & -- & -- & -- & -- & 1000 & -- & 3.413 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamwdb2f8blr0.008-wd0.1-minlr0-warmup1000-b10.9-b-a48171}{14} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.415 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw0cd0aalr0.008-wd0.1-minlr0-warmup4000-b10.9-b-26dd75}{15} \\
-- & -- & -- & -- & -- & -- & 0 & 3.436 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw7a3359lr0.008-wd0-minlr0-warmup2000-b10.9-b20-bf7e74}{16} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.415 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw1d5bddlr0.008-wd0.2-minlr0-warmup2000-b10.9-b-a991d6}{17} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 4x Chinchilla Data}
\label{tab:ablation_adamw_130m_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 128 & 2000 & 0.1 & 3.322 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw28a1ddlr0.008-wd0.1-minlr0-warmup2000-b10.9--a5a31c}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.330 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw3cefb4lr0.008-wd0.1-minlr0-warmup2000-b10.95-d2de07}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 3.416 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamwdec095lr0.008-wd0.1-minlr0-warmup2000-b10.98-5164f7}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.338 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw50c768lr0.008-wd0.1-minlr0-warmup2000-b10.9--374622}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.329 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw41da7flr0.008-wd0.1-minlr0-warmup2000-b10.9--e79638}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.322 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw11bb27lr0.008-wd0.1-minlr0-warmup2000-b10.9--112190}{5} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.323 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw2151b5lr0.008-wd0.1-minlr0-warmup2000-b10.9--8b96e9}{6} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.324 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamwc59aaalr0.008-wd0.1-minlr0-warmup2000-b10.9--64f3e7}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.329 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw05a8e1lr0.004-wd0.1-minlr0-warmup2000-b10.9--765ae0}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 3.337 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamwdc4f18lr0.016-wd0.1-minlr0-warmup2000-b10.9--5e227e}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.562 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamwa3cb44lr0.032-wd0.1-minlr0-warmup2000-b10.9--669760}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.331 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw90f5c1lr0.008-wd0.1-minlr0-warmup2000-b10.9--68bff2}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.373 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw5098e9lr0.008-wd0.1-minlr0-warmup2000-b10.9--3dd991}{12} \\
-- & -- & -- & -- & 1024 & -- & -- & 3.480 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw7fa927lr0.008-wd0.1-minlr0-warmup2000-b10.9--5e34ba}{13} \\
-- & -- & -- & -- & -- & 500 & -- & 7.262 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw479b6elr0.008-wd0.1-minlr0-warmup500-b10.9-b-029688}{14} \\
-- & -- & -- & -- & -- & 1000 & -- & 3.327 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamwc00703lr0.008-wd0.1-minlr0-warmup1000-b10.9--40ceef}{15} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.325 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamwb9e54alr0.008-wd0.1-minlr0-warmup4000-b10.9--eee94b}{16} \\
-- & -- & -- & -- & -- & -- & 0 & 3.359 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw76e938lr0.008-wd0-minlr0-warmup2000-b10.9-b2-6ac315}{17} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.335 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamweb3777lr0.008-wd0.2-minlr0-warmup2000-b10.9--c002ef}{18} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 8x Chinchilla Data}
\label{tab:ablation_adamw_130m_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 256 & 1000 & 0.1 & 3.262 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwc00703lr0.008-wd0.1-minlr0-warmup1000-b10.9--d73c2c}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.273 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwd5d639lr0.008-wd0.1-minlr0-warmup1000-b10.95-658eb8}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 3.430 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw0cb2d6lr0.008-wd0.1-minlr0-warmup1000-b10.98-7c952d}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.272 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwe479a6lr0.008-wd0.1-minlr0-warmup1000-b10.9--7570fe}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.266 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw63b609lr0.008-wd0.1-minlr0-warmup1000-b10.9--2da2b5}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.262 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwc0b69dlr0.008-wd0.1-minlr0-warmup1000-b10.9--2390cc}{5} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.263 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw9248c1lr0.008-wd0.1-minlr0-warmup1000-b10.9--7ae1a8}{6} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.261 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw357d4clr0.008-wd0.1-minlr0-warmup1000-b10.9--27bdc1}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.270 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwdb2f39lr0.004-wd0.1-minlr0-warmup1000-b10.9--cf39b6}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.435 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw06731clr0.016-wd0.1-minlr0-warmup1000-b10.9--2664a6}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.658 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamweac049lr0.032-wd0.1-minlr0-warmup1000-b10.9--cbb38e}{10} \\
-- & -- & -- & -- & 128 & -- & -- & 3.264 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw292ad2lr0.008-wd0.1-minlr0-warmup1000-b10.9--b88aa1}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.286 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwdb2f8blr0.008-wd0.1-minlr0-warmup1000-b10.9--849bed}{12} \\
-- & -- & -- & -- & 1024 & -- & -- & 3.328 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwe4d6f6lr0.008-wd0.1-minlr0-warmup1000-b10.9--f0487d}{13} \\
-- & -- & -- & -- & -- & 500 & -- & 3.278 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw479b6elr0.008-wd0.1-minlr0-warmup500-b10.9-b-5842d3}{14} \\
-- & -- & -- & -- & -- & 2000 & -- & 3.263 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw28a1ddlr0.008-wd0.1-minlr0-warmup2000-b10.9--b21aed}{15} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.262 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwb9e54alr0.008-wd0.1-minlr0-warmup4000-b10.9--74889b}{16} \\
-- & -- & -- & -- & -- & -- & 0 & 3.310 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamw549010lr0.008-wd0-minlr0-warmup1000-b10.9-b2-c08f03}{17} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.269 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwe72844lr0.008-wd0.2-minlr0-warmup1000-b10.9--2b5e04}{18} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 16x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 16x Chinchilla Data}
\label{tab:ablation_adamw_130m_16}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 256 & 1000 & 0.1 & 3.207 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-42B-adamwdf6bfb1lr0.008-wd0.1-minlr0-warmup1000-b10.9-c5d61d}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 1x Chinchilla Data}
\label{tab:ablation_adamw_300m_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 128 & 2000 & 0.1 & 3.264 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamwa7aafelr0.008-wd0.1-minlr0-warmup2000-b10.9-b-990eda}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.271 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw55fc1flr0.008-wd0.1-minlr0-warmup2000-b10.95--b7196b}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.351 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw3aa56elr0.008-wd0.1-minlr0-warmup2000-b10.98--36a5e7}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.280 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw72a6d2lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-dde1e5}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.269 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw901abclr0.008-wd0.1-minlr0-warmup2000-b10.9-b-de77a3}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.265 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamwfa0d44lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-89eba9}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.265 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw057b30lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-cff503}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.263 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw863e5alr0.008-wd0.1-minlr0-warmup2000-b10.9-b-8395a7}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.272 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw0c32aelr0.004-wd0.1-minlr0-warmup2000-b10.9-b-a044c7}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.760 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw2c8c47lr0.016-wd0.1-minlr0-warmup2000-b10.9-b-689737}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.784 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamwb6e480lr0.032-wd0.1-minlr0-warmup2000-b10.9-b-ff992d}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.282 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw5d409dlr0.008-wd0.1-minlr0-warmup2000-b10.9-b-632ec9}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.367 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw2a43e8lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-0d1c6f}{12} \\
-- & -- & -- & -- & -- & 500 & -- & 7.704 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw35a912lr0.008-wd0.1-minlr0-warmup500-b10.9-b2-a0e68a}{13} \\
-- & -- & -- & -- & -- & 1000 & -- & 7.759 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw0c358elr0.008-wd0.1-minlr0-warmup1000-b10.9-b-0be6f6}{14} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.270 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamwea4b29lr0.008-wd0.1-minlr0-warmup4000-b10.9-b-531c85}{15} \\
-- & -- & -- & -- & -- & -- & 0 & 3.303 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamw640457lr0.008-wd0-minlr0-warmup2000-b10.9-b20-12de29}{16} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.275 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamwd8feb6lr0.008-wd0.2-minlr0-warmup2000-b10.9-b-ea8bed}{17} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 2x Chinchilla Data}
\label{tab:ablation_adamw_300m_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 128 & 2000 & 0.1 & 3.166 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwo5dcabdlr0.008-wd0.1-minlr0-warmup2000-b10.9-3d4fae}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & 3.164 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwvae7cf4lr0.004-wd0.1-minlr0-warmup2000-b10.9-2dca32}{1} \\
-- & -- & -- & -- & 256 & -- & -- & 3.169 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwo6cb296lr0.008-wd0.1-minlr0-warmup2000-b10.9-3dedfd}{2} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.184 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwk4476fflr0.008-wd0.2-minlr0-warmup2000-b10.9-2177e1}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 4x Chinchilla Data}
\label{tab:ablation_adamw_300m_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 256 & 2000 & 0.1 & 3.094 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw5dcabdlr0.008-wd0.1-minlr0-warmup2000-b10.9--5e75fb}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & 3.101 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamwae7cf4lr0.004-wd0.1-minlr0-warmup2000-b10.9--ee3fd2}{1} \\
-- & -- & -- & -- & 128 & -- & -- & 3.103 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw07fc6elr0.008-wd0.1-minlr0-warmup2000-b10.9--459481}{2} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.103 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw4476fflr0.008-wd0.2-minlr0-warmup2000-b10.9--1eec01}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 8x Chinchilla Data}
\label{tab:ablation_adamw_300m_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 256 & 2000 & 0.1 & 3.043 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw07fc6elr0.008-wd0.1-minlr0-warmup2000-b10.9--b38246}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & 3.042 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw2537edlr0.004-wd0.1-minlr0-warmup2000-b10.9--a59fd5}{1} \\
-- & -- & -- & -- & 128 & -- & -- & 3.057 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw6a50aflr0.008-wd0.1-minlr0-warmup2000-b10.9--22e2a7}{2} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.059 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw7d9e60lr0.008-wd0.2-minlr0-warmup2000-b10.9--dfe5c9}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 16x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 16x Chinchilla Data}
\label{tab:ablation_adamw_300m_16}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 256 & 2000 & 0.1 & 3.001 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-96B-adamwfcee97lr0.004-wd0.1-minlr0-warmup2000-b10.9--18b705}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 1x Chinchilla Data}
\label{tab:ablation_adamw_520m_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 256 & 1000 & 0.2 & 3.110 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwf7a9c6lr0.004-wd0.2-minlr0-warmup1000-b10.9--0a09ef}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.112 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw92fab3lr0.004-wd0.2-minlr0-warmup1000-b10.95-b52d40}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 3.229 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw757dfblr0.004-wd0.2-minlr0-warmup1000-b10.98-82eb00}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.116 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw2399bdlr0.004-wd0.2-minlr0-warmup1000-b10.9--235c9f}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.111 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw49b619lr0.004-wd0.2-minlr0-warmup1000-b10.9--0339de}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.116 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwe08492lr0.004-wd0.2-minlr0-warmup1000-b10.9--550b0a}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.116 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw920b54lr0.004-wd0.2-minlr0-warmup1000-b10.9--29bc56}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.115 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwc8444flr0.004-wd0.2-minlr0-warmup1000-b10.9--ad8761}{7} \\
-- & -- & -- & 0.008 & -- & -- & -- & 7.837 & N/A \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.756 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwd13956lr0.016-wd0.2-minlr0-warmup1000-b10.9--db0248}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.680 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwd5cf15lr0.032-wd0.2-minlr0-warmup1000-b10.9--ef01e7}{10} \\
-- & -- & -- & -- & 128 & -- & -- & 7.630 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw9d215dlr0.004-wd0.2-minlr0-warmup1000-b10.9--0e6642}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.169 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwa3b31flr0.004-wd0.2-minlr0-warmup1000-b10.9--3d3c5b}{12} \\
-- & -- & -- & -- & 1024 & -- & -- & 3.302 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwa78593lr0.004-wd0.2-minlr0-warmup1000-b10.9--f6c234}{13} \\
-- & -- & -- & -- & -- & 500 & -- & 3.165 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwfa4107lr0.004-wd0.2-minlr0-warmup500-b10.9-b-da6d10}{14} \\
-- & -- & -- & -- & -- & 2000 & -- & 3.113 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw78783dlr0.004-wd0.2-minlr0-warmup2000-b10.9--07b41c}{15} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.126 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw27eccflr0.004-wd0.2-minlr0-warmup4000-b10.9--46c4f0}{16} \\
-- & -- & -- & -- & -- & -- & 0 & 7.270 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw25a778lr0.004-wd0-minlr0-warmup1000-b10.9-b2-f5ae68}{17} \\
-- & -- & -- & -- & -- & -- & 0.1 & 3.135 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamw1872fclr0.004-wd0.1-minlr0-warmup1000-b10.9--34d388}{18} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 2x Chinchilla Data}
\label{tab:ablation_adamw_520m_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 256 & 1000 & 0.2 & 3.023 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamws9d215dlr0.004-wd0.2-minlr0-warmup1000-b10.9-d16851}{0} \\
\midrule
-- & -- & -- & -- & 128 & -- & -- & 6.654 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamwsffc6bdlr0.004-wd0.2-minlr0-warmup1000-b10.9-646b5d}{1} \\
-- & -- & -- & -- & -- & -- & 0.1 & 3.025 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamws529f50lr0.004-wd0.1-minlr0-warmup1000-b10.9-c43fcb}{2} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 4x Chinchilla Data}
\label{tab:ablation_adamw_520m_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 256 & 1000 & 0.1 & 2.958 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwbc2b89lr0.004-wd0.1-minlr0-warmup1000-b10.9--50dbd9}{0} \\
\midrule
-- & -- & -- & 0.008 & -- & -- & -- & 7.075 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwf6bfb1lr0.008-wd0.1-minlr0-warmup1000-b10.9--c0b516}{1} \\
-- & -- & -- & -- & 128 & -- & -- & 7.139 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwf14f39lr0.004-wd0.1-minlr0-warmup1000-b10.9--48b1c7}{2} \\
-- & -- & -- & -- & -- & -- & 0.2 & 2.962 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwffc6bdlr0.004-wd0.2-minlr0-warmup1000-b10.9--69bc03}{3} \\
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 8x Chinchilla Data}
\label{tab:ablation_adamw_520m_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 256 & 1000 & 0.1 & 2.913 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamwf14f39lr0.004-wd0.1-minlr0-warmup1000-b10.9--84afa9}{0} \\
\midrule
-- & -- & -- & 0.008 & -- & -- & -- & 7.183 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamw166f50lr0.008-wd0.1-minlr0-warmup1000-b10.9--397bf5}{1} \\
-- & -- & -- & -- & 128 & -- & -- & 6.932 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamw2b7ec2lr0.004-wd0.1-minlr0-warmup1000-b10.9--01c1ad}{2} \\
-- & -- & -- & -- & -- & -- & 0.2 & 2.921 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamw61633blr0.004-wd0.2-minlr0-warmup1000-b10.9--1a26e3}{3} \\
\bottomrule
\end{tabular}
\end{table}

