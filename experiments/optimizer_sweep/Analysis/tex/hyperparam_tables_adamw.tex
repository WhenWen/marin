\subsection{Sweeping Results for AdamW}% adamw - 300m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 1x Chinchilla Data}
\label{tab:ablation_adamw_300m_1}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 1 & 0 & 128 & 2000 & 0.1 & 3.264 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-adamwa7aafelr0.008-wd0.1-minlr0-warmup2000-b10.9-b-990eda}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 16x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 16x Chinchilla Data}
\label{tab:ablation_adamw_300m_16}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 2 & 0 & 256 & 2000 & 0.1 & 3.001 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-96B-adamwfcee97lr0.004-wd0.1-minlr0-warmup2000-b10.9--18b705}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 2x Chinchilla Data}
\label{tab:ablation_adamw_300m_2}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 2 & 0 & 128 & 2000 & 0.1 & 3.166 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-adamwo5dcabdlr0.008-wd0.1-minlr0-warmup2000-b10.9-3d4fae}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 8x Chinchilla Data}
\label{tab:ablation_adamw_300m_8}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 2 & 0 & 256 & 2000 & 0.1 & 3.043 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-adamw07fc6elr0.008-wd0.1-minlr0-warmup2000-b10.9--b38246}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 300m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 300m on 4x Chinchilla Data}
\label{tab:ablation_adamw_300m_4}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 2 & 0 & 256 & 2000 & 0.1 & 3.094 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-adamw5dcabdlr0.008-wd0.1-minlr0-warmup2000-b10.9--5e75fb}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 4x Chinchilla Data}
\label{tab:ablation_adamw_520m_4}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.1 & 2.958 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-adamwbc2b89lr0.004-wd0.1-minlr0-warmup1000-b10.9--50dbd9}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 1x Chinchilla Data}
\label{tab:ablation_adamw_520m_1}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.2 & 3.110 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-adamwf7a9c6lr0.004-wd0.2-minlr0-warmup1000-b10.9--0a09ef}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 2x Chinchilla Data}
\label{tab:ablation_adamw_520m_2}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.2 & 3.023 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-adamws9d215dlr0.004-wd0.2-minlr0-warmup1000-b10.9-d16851}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 520m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 520m on 8x Chinchilla Data}
\label{tab:ablation_adamw_520m_8}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 1 & 0 & 256 & 1000 & 0.1 & 2.913 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-adamwf14f39lr0.004-wd0.1-minlr0-warmup1000-b10.9--84afa9}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 4x Chinchilla Data}
\label{tab:ablation_adamw_130m_4}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 1 & 0 & 128 & 2000 & 0.1 & 3.322 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-adamw28a1ddlr0.008-wd0.1-minlr0-warmup2000-b10.9--a5a31c}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 1x Chinchilla Data}
\label{tab:ablation_adamw_130m_1}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 1 & 0 & 128 & 2000 & 0.1 & 3.529 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-adamw5098e9lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-bc5e36}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 2x Chinchilla Data}
\label{tab:ablation_adamw_130m_2}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 1 & 0 & 128 & 2000 & 0.1 & 3.409 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-adamw90f5c1lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-67adb6}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 8x Chinchilla Data}
\label{tab:ablation_adamw_130m_8}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 1 & 0 & 256 & 1000 & 0.1 & 3.262 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-adamwc00703lr0.008-wd0.1-minlr0-warmup1000-b10.9--d73c2c}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 130m on 16x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 130m on 16x Chinchilla Data}
\label{tab:ablation_adamw_130m_16}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 1 & 0 & 256 & 1000 & 0.1 & 3.207 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-42B-adamwdf6bfb1lr0.008-wd0.1-minlr0-warmup1000-b10.9-c5d61d}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 4x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_4}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 1 & 0.0 & 256 & 1000 & 0.2 & 2.787 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-96B-adamw71c224lr0.002-wd0.2-minlr0.0-warmup1000-b10.-66682a}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 1x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_1}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 2 & 0.0 & 256 & 2000 & 0.2 & 2.905 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-24B-adamwf115b7lr0.002-wd0.2-minlr0-warmup2000-b10.9--387e9f}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 2x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_2}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 1 & 0.0 & 256 & 1000 & 0.2 & 2.836 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-48B-adamw7ebb9alr0.002-wd0.2-minlr0.0-warmup1000-b10.-994f23}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

% adamw - 1.2b on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for AdamW on 1.2b on 8x Chinchilla Data}
\label{tab:ablation_adamw_1.2b_8}
\begin{tabular}{ccccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\gradnorm$ & $\eta_{min}$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.002 & 1 & 0.0 & 256 & 1000 & 0.2 & 2.752 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-1.2b-193B-adamw1eeba1lr0.002-wd0.2-minlr0.0-warmup1000-b10-44a428}{0} \\
\midrule
\bottomrule
\end{tabular}
\end{table}

