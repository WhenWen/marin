\subsection{Sweeping Results for Adam-Mini}% mini - 130m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 130m on 1x Chinchilla Data}
\label{tab:ablation_adam-mini_130m_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-15 & 0.008 & 128 & 2000 & 0.1 & 3.542 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-minif4e66flr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-4d3c05}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.560 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-minia4c40alr0.008-wd0.1-minlr0-warmup2000-b10.95-b-73290f}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.733 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini59cf9alr0.008-wd0.1-minlr0-warmup2000-b10.98-b-df073b}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.554 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini84080dlr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-d74517}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.545 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-minic9835clr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-d57aa7}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.546 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini510347lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-9a27cf}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.546 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini6d8617lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-7a2d5e}{6} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.548 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini845331lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-fadab9}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.558 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini08c39flr0.004-wd0.1-minlr0-warmup2000-b10.9-b2-1b064a}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.800 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini22b748lr0.016-wd0.1-minlr0-warmup2000-b10.9-b2-6d20c4}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.825 & N/A \\
-- & -- & -- & -- & 256 & -- & -- & 3.785 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-minidd6b61lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-b988c3}{11} \\
-- & -- & -- & -- & -- & 500 & -- & 7.725 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini0c2c00lr0.008-wd0.1-minlr0-warmup500-b10.9-b20-8094ab}{12} \\
-- & -- & -- & -- & -- & 1000 & -- & 7.729 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-minibd49d8lr0.008-wd0.1-minlr0-warmup1000-b10.9-b2-28a98b}{13} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.587 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-minif55f2dlr0.008-wd0.1-minlr0-warmup4000-b10.9-b2-b00ef5}{14} \\
-- & -- & -- & -- & -- & -- & 0 & 3.566 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini73fab0lr0.008-wd0-minlr0-warmup2000-b10.9-b20.-aac58f}{15} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.589 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-2B-mini9fac99lr0.008-wd0.2-minlr0-warmup2000-b10.9-b2-86f1dc}{16} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 130m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 130m on 2x Chinchilla Data}
\label{tab:ablation_adam-mini_130m_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-20 & 0.008 & 128 & 2000 & 0.1 & 3.416 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini609ad2lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-b557c0}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.429 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini20b8b1lr0.008-wd0.1-minlr0-warmup2000-b10.95-b-b46cb8}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.520 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-minideec58lr0.008-wd0.1-minlr0-warmup2000-b10.98-b-62bc48}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.422 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-minib27e6blr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-04f0b5}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.419 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini51a865lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-856cfa}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.416 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini7d5c8flr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-dd8629}{5} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.416 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini8f577dlr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-361bf6}{6} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.415 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-minidbb1e7lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-9d6767}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.425 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-miniab7841lr0.004-wd0.1-minlr0-warmup2000-b10.9-b2-0d5f4f}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.796 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini3fbff3lr0.016-wd0.1-minlr0-warmup2000-b10.9-b2-e1ec05}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.721 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-minif2196dlr0.032-wd0.1-minlr0-warmup2000-b10.9-b2-bbaaf6}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.487 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini4ff858lr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-e5ff00}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.758 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini9232dclr0.008-wd0.1-minlr0-warmup2000-b10.9-b2-f9c99a}{12} \\
-- & -- & -- & -- & -- & 500 & -- & 7.617 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini049b41lr0.008-wd0.1-minlr0-warmup500-b10.9-b20-54b29d}{13} \\
-- & -- & -- & -- & -- & 1000 & -- & 7.445 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini0c45d6lr0.008-wd0.1-minlr0-warmup1000-b10.9-b2-a07cd7}{14} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.424 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-mini05a729lr0.008-wd0.1-minlr0-warmup4000-b10.9-b2-68948f}{15} \\
-- & -- & -- & -- & -- & -- & 0 & 3.447 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-minicf80e8lr0.008-wd0-minlr0-warmup2000-b10.9-b20.-e3acc2}{16} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.426 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-5B-minimc35720lr0.008-wd0.2-minlr0-warmup2000-b10.9-b-54c1f6}{17} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 130m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 130m on 4x Chinchilla Data}
\label{tab:ablation_adam-mini_130m_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 128 & 2000 & 0.1 & 3.328 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini8e0689lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-1de787}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.360 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini5655falr0.008-wd0.1-minlr0-warmup2000-b10.95--7977a5}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.771 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini608e8clr0.008-wd0.1-minlr0-warmup2000-b10.98--851955}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.337 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-minic61cb4lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-3daee1}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.331 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-minica3b57lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-c56f5e}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.331 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini98f892lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-b90941}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.331 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini3af704lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-522688}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.334 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini99be6blr0.008-wd0.1-minlr0-warmup2000-b10.9-b-c9b783}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.334 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini593d31lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-17e1ff}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.717 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini76c7dflr0.016-wd0.1-minlr0-warmup2000-b10.9-b-42ab2c}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.652 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini7491aelr0.032-wd0.1-minlr0-warmup2000-b10.9-b-eec75e}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.363 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini5a8324lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-b209a4}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.447 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini845331lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-d5fe03}{12} \\
-- & -- & -- & -- & 1024 & -- & -- & 3.784 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini3fca05lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-a825c0}{13} \\
-- & -- & -- & -- & -- & 500 & -- & 7.855 & N/A \\
-- & -- & -- & -- & -- & 1000 & -- & 7.411 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini684316lr0.008-wd0.1-minlr0-warmup1000-b10.9-b-419ffe}{15} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.331 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini48c902lr0.008-wd0.1-minlr0-warmup4000-b10.9-b-c9d5f8}{16} \\
-- & -- & -- & -- & -- & -- & 0 & 3.364 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-mini774178lr0.008-wd0-minlr0-warmup2000-b10.9-b20-1ab0b9}{17} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.365 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-10B-minic5c69blr0.008-wd0.2-minlr0-warmup2000-b10.9-b-0638b6}{18} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 130m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 130m on 8x Chinchilla Data}
\label{tab:ablation_adam-mini_130m_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.008 & 128 & 2000 & 0.1 & 3.266 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-miniac2e1elr0.008-wd0.1-minlr0-warmup2000-b10.9-b-d489dc}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.290 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini117b84lr0.008-wd0.1-minlr0-warmup2000-b10.95--817e0e}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.552 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini9179b1lr0.008-wd0.1-minlr0-warmup2000-b10.98--93bc9f}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.281 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-minic5318dlr0.008-wd0.1-minlr0-warmup2000-b10.9-b-6f8a67}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.267 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-minic96875lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-fa1dd8}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.267 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-minidd3b3clr0.008-wd0.1-minlr0-warmup2000-b10.9-b-1d94ce}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.267 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini88561dlr0.008-wd0.1-minlr0-warmup2000-b10.9-b-f66e88}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.266 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini3c1780lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-9a05a7}{7} \\
-- & -- & -- & 0.004 & -- & -- & -- & 3.264 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini5c5f85lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-eaf3db}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.614 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-minidb82b6lr0.016-wd0.1-minlr0-warmup2000-b10.9-b-765b78}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.773 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-minid9a283lr0.032-wd0.1-minlr0-warmup2000-b10.9-b-3d6c8c}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.281 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini8e0689lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-f8ffc0}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.324 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini5a8324lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-82522f}{12} \\
-- & -- & -- & -- & 1024 & -- & -- & 3.426 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini845331lr0.008-wd0.1-minlr0-warmup2000-b10.9-b-05faa2}{13} \\
-- & -- & -- & -- & -- & 500 & -- & 7.868 & N/A \\
-- & -- & -- & -- & -- & 1000 & -- & 7.022 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini056a5flr0.008-wd0.1-minlr0-warmup1000-b10.9-b-d7cc88}{15} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.266 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini9445d9lr0.008-wd0.1-minlr0-warmup4000-b10.9-b-ae303f}{16} \\
-- & -- & -- & -- & -- & -- & 0 & 3.304 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini2e27bblr0.008-wd0-minlr0-warmup2000-b10.9-b20-c75282}{17} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.291 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-130m-21B-mini593df5lr0.008-wd0.2-minlr0-warmup2000-b10.9-b-27dd40}{18} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 300m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 300m on 1x Chinchilla Data}
\label{tab:ablation_adam-mini_300m_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-25 & 0.004 & 128 & 2000 & 0.2 & 3.272 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minimf93988lr0.004-wd0.2-minlr0-warmup2000-b10.9-b-1a622b}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.276 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minic68fa6lr0.004-wd0.2-minlr0-warmup2000-b10.95-b-61c14d}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 8.024 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minie808f4lr0.004-wd0.2-minlr0-warmup2000-b10.98-b-2ad92e}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.279 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim7e5831lr0.004-wd0.2-minlr0-warmup2000-b10.9-b-3f89ca}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.277 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-mini3022d1lr0.004-wd0.2-minlr0-warmup2000-b10.9-b2-0db993}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.272 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minimf93988lr0.004-wd0.2-minlr0-warmup2000-b10.9-b-1a622b}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.272 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-mini3ff087lr0.004-wd0.2-minlr0-warmup2000-b10.9-b2-e0d948}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.273 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim4cb19elr0.004-wd0.2-minlr0-warmup2000-b10.9-b-4a59e2}{7} \\
-- & -- & 1e-10 & -- & -- & -- & -- & 3.273 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim8dac9blr0.004-wd0.2-minlr0-warmup2000-b10.9-b-2ed5f7}{8} \\
-- & -- & -- & 0.008 & -- & -- & -- & 7.859 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim70e785lr0.008-wd0.2-minlr0-warmup2000-b10.9-b-8588ff}{9} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.990 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-mini7b7df9lr0.016-wd0.2-minlr0-warmup2000-b10.9-b2-7d82fa}{10} \\
-- & -- & -- & 0.032 & -- & -- & -- & 8.581 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-mini1247b4lr0.032-wd0.2-minlr0-warmup2000-b10.9-b2-b5154c}{11} \\
-- & -- & -- & -- & 256 & -- & -- & 5.691 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim043b8elr0.004-wd0.2-minlr0-warmup2000-b10.9-b-687e85}{12} \\
-- & -- & -- & -- & 512 & -- & -- & 3.629 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim140e6dlr0.004-wd0.2-minlr0-warmup2000-b10.9-b-2bda97}{13} \\
-- & -- & -- & -- & -- & 500 & -- & 7.020 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim159817lr0.004-wd0.2-minlr0-warmup500-b10.9-b2-8ca381}{14} \\
-- & -- & -- & -- & -- & 1000 & -- & 7.042 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-mini1778f2lr0.004-wd0.2-minlr0-warmup1000-b10.9-b2-55a012}{15} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.280 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minim6fb244lr0.004-wd0.2-minlr0-warmup4000-b10.9-b-4c5f20}{16} \\
-- & -- & -- & -- & -- & -- & 0 & 3.336 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-mini59d801lr0.004-wd0-minlr0-warmup2000-b10.9-b20.-559ac1}{17} \\
-- & -- & -- & -- & -- & -- & 0.1 & 3.280 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-6B-minimc4311dlr0.004-wd0.1-minlr0-warmup2000-b10.9-b-d451be}{18} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 300m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 300m on 2x Chinchilla Data}
\label{tab:ablation_adam-mini_300m_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-25 & 0.004 & 128 & 2000 & 0.2 & 3.178 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-mini9676c0lr0.004-wd0.2-minlr0-warmup2000-b10.9-b-80350d}{0} \\
\midrule
-- & -- & -- & 0.002 & -- & -- & -- & 3.180 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-mini4cf513lr0.002-wd0.2-minlr0-warmup2000-b10.9-b-df6367}{1} \\
-- & -- & -- & -- & 256 & -- & -- & 6.960 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-mini65d3bflr0.004-wd0.2-minlr0-warmup2000-b10.9-b-79c803}{2} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.183 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-minic35a69lr0.004-wd0.2-minlr0-warmup4000-b10.9-b-05ae30}{3} \\
-- & -- & -- & -- & -- & -- & 0.1 & 3.179 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-12B-minif8c6e5lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-aa6c53}{4} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 300m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 300m on 4x Chinchilla Data}
\label{tab:ablation_adam-mini_300m_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-25 & 0.004 & 128 & 2000 & 0.1 & 3.103 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-mini09d394lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-4e8de7}{0} \\
\midrule
-- & -- & -- & 0.002 & -- & -- & -- & 3.111 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-mini5d38a2lr0.002-wd0.1-minlr0-warmup2000-b10.9-b-951b01}{1} \\
-- & -- & -- & -- & 256 & -- & -- & 3.109 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-minid571a7lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-e475d2}{2} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.104 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-mini0831d8lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-41a654}{3} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.111 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-24B-mini891741lr0.004-wd0.2-minlr0-warmup2000-b10.9-b-a97787}{4} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 300m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 300m on 8x Chinchilla Data}
\label{tab:ablation_adam-mini_300m_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-25 & 0.002 & 128 & 2000 & 0.2 & 3.049 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-minic8695flr0.002-wd0.2-minlr0-warmup2000-b10.9-b-952cc4}{0} \\
\midrule
-- & -- & -- & 0.004 & -- & -- & -- & 3.064 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-minid13807lr0.004-wd0.2-minlr0-warmup2000-b10.9-b-c8b391}{1} \\
-- & -- & -- & -- & 256 & -- & -- & 3.052 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-mini54b665lr0.002-wd0.2-minlr0-warmup2000-b10.9-b-bb57bb}{2} \\
-- & -- & -- & -- & -- & 4000 & -- & 3.050 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-mini8ec643lr0.002-wd0.2-minlr0-warmup4000-b10.9-b-efe5ba}{3} \\
-- & -- & -- & -- & -- & -- & 0.1 & 3.051 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-300m-48B-mini293878lr0.002-wd0.1-minlr0-warmup2000-b10.9-b-f38163}{4} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 520m on 1x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 520m on 1x Chinchilla Data}
\label{tab:ablation_adam-mini_520m_1}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 128 & 4000 & 0.1 & 3.112 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini9187d7lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-398e5f}{0} \\
\midrule
0.95 & -- & -- & -- & -- & -- & -- & 3.113 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-minic83515lr0.004-wd0.1-minlr0-warmup4000-b10.95--2b99be}{1} \\
0.98 & -- & -- & -- & -- & -- & -- & 7.459 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini61f7e6lr0.004-wd0.1-minlr0-warmup4000-b10.98--c8aa3c}{2} \\
-- & 0.9 & -- & -- & -- & -- & -- & 3.120 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-minib059b0lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-ef048a}{3} \\
-- & 0.95 & -- & -- & -- & -- & -- & 3.115 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-minid80005lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-4d59a5}{4} \\
-- & -- & 1e-25 & -- & -- & -- & -- & 3.115 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini030aa2lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-e031a9}{5} \\
-- & -- & 1e-20 & -- & -- & -- & -- & 3.115 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-miniad4882lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-375106}{6} \\
-- & -- & 1e-15 & -- & -- & -- & -- & 3.112 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-minica5e495lr0.004-wd0.1-minlr0-warmup4000-b10.9--791457}{7} \\
-- & -- & -- & 0.008 & -- & -- & -- & 7.771 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini8213c6lr0.008-wd0.1-minlr0-warmup4000-b10.9-b-cea98b}{8} \\
-- & -- & -- & 0.016 & -- & -- & -- & 7.746 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini743375lr0.016-wd0.1-minlr0-warmup4000-b10.9-b-71d93a}{9} \\
-- & -- & -- & 0.032 & -- & -- & -- & 7.778 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini616d96lr0.032-wd0.1-minlr0-warmup4000-b10.9-b-6809c5}{10} \\
-- & -- & -- & -- & 256 & -- & -- & 3.133 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-minib57861lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-a05b39}{11} \\
-- & -- & -- & -- & 512 & -- & -- & 3.200 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini05d195lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-f943f3}{12} \\
-- & -- & -- & -- & -- & 500 & -- & 7.457 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini7194dalr0.004-wd0.1-minlr0-warmup500-b10.9-b2-4a47f7}{13} \\
-- & -- & -- & -- & -- & 1000 & -- & 7.274 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini358c33lr0.004-wd0.1-minlr0-warmup1000-b10.9-b-ac8e20}{14} \\
-- & -- & -- & -- & -- & 2000 & -- & 7.289 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini5326c8lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-df18d8}{15} \\
-- & -- & -- & -- & -- & -- & 0 & 7.792 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-minia9054blr0.004-wd0-minlr0-warmup4000-b10.9-b20-1ef9ff}{16} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.115 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-10B-mini52759flr0.004-wd0.2-minlr0-warmup4000-b10.9-b-8078e5}{17} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 520m on 2x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 520m on 2x Chinchilla Data}
\label{tab:ablation_adam-mini_520m_2}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 128 & 4000 & 0.1 & 3.027 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-mini4e584elr0.004-wd0.1-minlr0-warmup4000-b10.9-b-eceb71}{0} \\
\midrule
-- & -- & -- & 0.002 & -- & -- & -- & 3.031 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-mini3dba17lr0.002-wd0.1-minlr0-warmup4000-b10.9-b-557d40}{1} \\
-- & -- & -- & -- & 256 & -- & -- & 3.032 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-mini89512clr0.004-wd0.1-minlr0-warmup4000-b10.9-b-66cd27}{2} \\
-- & -- & -- & -- & -- & 2000 & -- & 7.359 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-mini5c5f85lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-9ff993}{3} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.037 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-21B-miniqbb0854lr0.004-wd0.2-minlr0-warmup4000-b10.9--8a1042}{4} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 520m on 4x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 520m on 4x Chinchilla Data}
\label{tab:ablation_adam-mini_520m_4}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 128 & 4000 & 0.1 & 2.966 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-minikdfe5aclr0.004-wd0.1-minlr0-warmup4000-b10.9--5de7af}{0} \\
\midrule
-- & -- & -- & 0.002 & -- & -- & -- & 2.963 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-minik6bf656lr0.002-wd0.1-minlr0-warmup4000-b10.9--a9507c}{1} \\
-- & -- & -- & -- & 256 & -- & -- & 2.963 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-minik4e584elr0.004-wd0.1-minlr0-warmup4000-b10.9--3eca5c}{2} \\
-- & -- & -- & -- & -- & 2000 & -- & 7.529 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-minikb765f6lr0.004-wd0.1-minlr0-warmup2000-b10.9--520ca1}{3} \\
-- & -- & -- & -- & -- & -- & 0.2 & 2.981 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-42B-miniba6697lr0.004-wd0.2-minlr0-warmup4000-b10.9-b-2db890}{4} \\
\bottomrule
\end{tabular}
\end{table}

% mini - 520m on 8x Chinchilla Data
\begin{table}[H]
\centering
\caption{Hyperparameter ablation for Adam-Mini on 520m on 8x Chinchilla Data}
\label{tab:ablation_adam-mini_520m_8}
\begin{tabular}{ccccccccc}
\toprule
$\beta_1$ & $\beta_2$ & $\epsilon$ & $\eta$ & $\mathrm{BSZ}$ & $\mathrm{warmup}$ & $\lambda$ & Loss & Link \\
\midrule
0.9 & 0.98 & 1e-10 & 0.004 & 256 & 4000 & 0.1 & 2.912 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-minidfe5aclr0.004-wd0.1-minlr0-warmup4000-b10.9-b-a76572}{0} \\
\midrule
-- & -- & -- & 0.002 & -- & -- & -- & 2.918 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-mini6bf656lr0.002-wd0.1-minlr0-warmup4000-b10.9-b-aa645d}{1} \\
-- & -- & -- & -- & 128 & -- & -- & 2.921 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-mini6698b4lr0.004-wd0.1-minlr0-warmup4000-b10.9-b-0f2989}{2} \\
-- & -- & -- & -- & -- & 2000 & -- & 7.449 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-minib765f6lr0.004-wd0.1-minlr0-warmup2000-b10.9-b-e401ae}{3} \\
-- & -- & -- & -- & -- & -- & 0.2 & 3.025 & \href{https://wandb.ai/stanford-mercury/optimizer-scaling/runs/sweep-520m-85B-miniba6697lr0.004-wd0.2-minlr0-warmup4000-b10.9-b-137303}{4} \\
\bottomrule
\end{tabular}
\end{table}

