InputName(step=ExecutorStep(name='checkpoints/suhas/debug-resume-optimizer-v3-stage1', fn=<ray.remote_function.RemoteFunction object at 0x705e5a179350>, config=TrainLmOnPodConfig(data=LMMixtureDatasetConfig(tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), vocab_size=None, cache_dir=None, cache_options=CacheOptions(num_shard_groups=128, target_size_per_flush='512MB', batch_size=128), enforce_eos=True, ignore_token_id=None, shuffle=True, configs={'SlimPajama-6B': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=[InputName(step=ExecutorStep(name='raw/SlimPajama-6B', fn=<function download at 0x705e34d40040>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='DKYoon/SlimPajama-6B', revision='b5f90f4', hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/SlimPajama-6B-be35b7', pip_dependency_groups=None), name='b5f90f4/huggingface.co/datasets/DKYoon/SlimPajama-6B/resolve/b5f90f4')], validation_urls=[], cache_dir=InputName(step=ExecutorStep(name='tokenized/SlimPajama-6B', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=[InputName(step=ExecutorStep(name='raw/SlimPajama-6B', fn=<function download at 0x705e34d40040>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='DKYoon/SlimPajama-6B', revision='b5f90f4', hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/SlimPajama-6B-be35b7', pip_dependency_groups=None), name='b5f90f4/huggingface.co/datasets/DKYoon/SlimPajama-6B/resolve/b5f90f4')], validation_paths=[], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description='Tokenize raw text using the meta-llama/Meta-Llama-3.1-8B tokenizer.', override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/4chan': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/4chan_meta_sep/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/4chan', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/4chan_meta_sep/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/c4_100_domains': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/c4_100_domains/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/c4_100_domains', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/c4_100_domains/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/c4_en': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/c4_en/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/c4_en', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/c4_en/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/dolma-v1_5': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/dolma-v1_5/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/dolma-v1_5', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/dolma-v1_5/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/dolma_100_programing_languages': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/dolma_100_programing_languages', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/dolma_100_subreddits': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/dolma_100_subreddits', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/falcon-refinedweb': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/falcon-refinedweb/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/falcon-refinedweb', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/falcon-refinedweb/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/gab': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/gab/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/gab', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/gab/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/m2d2_s2orc_unsplit': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/m2d2_s2orc_unsplit', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/m2d2_wikipedia_unsplit': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/m2d2_wikipedia_unsplit', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/manosphere_meta_sep': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/manosphere_meta_sep', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/mc4': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/mc4/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/mc4', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/mc4/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/ptb': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/ptb/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/ptb', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/ptb/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/redpajama': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/redpajama/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/redpajama', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/redpajama/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/twitterAAE_HELM_fixed': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/twitterAAE_HELM_fixed', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None)), 'paloma/wikitext_103': LMDatasetSourceConfig(tags=[], id=None, name=None, plaintext=False, stream=True, text_key='text', train_urls=VersionedValue(value=[]), validation_urls=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/wikitext_103/val/val*.jsonl.gz')], cache_dir=InputName(step=ExecutorStep(name='tokenized/paloma/wikitext_103', fn=<function tokenize at 0x705e5a0c04a0>, config=TokenizeConfig(train_paths=VersionedValue(value=[]), validation_paths=[InputName(step=ExecutorStep(name='raw/paloma', fn=<function download_and_upload_to_store at 0x705e34d400e0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id=VersionedValue(value='allenai/paloma'), revision=VersionedValue(value='65cd6fc'), hf_urls_glob=[], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path=None, pip_dependency_groups=None), name='65cd6fc/wikitext_103/val/val*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='', output_field='', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None))}, train_weights={'SlimPajama-6B': 1.0, 'paloma/4chan': 0.0, 'paloma/c4_100_domains': 0.0, 'paloma/c4_en': 0.0, 'paloma/dolma-v1_5': 0.0, 'paloma/dolma_100_programing_languages': 0.0, 'paloma/dolma_100_subreddits': 0.0, 'paloma/falcon-refinedweb': 0.0, 'paloma/gab': 0.0, 'paloma/m2d2_s2orc_unsplit': 0.0, 'paloma/m2d2_wikipedia_unsplit': 0.0, 'paloma/manosphere_meta_sep': 0.0, 'paloma/mc4': 0.0, 'paloma/ptb': 0.0, 'paloma/redpajama': 0.0, 'paloma/twitterAAE_HELM_fixed': 0.0, 'paloma/wikitext_103': 0.0}, stop_strategy='restart', mixture_block_size=2048), supervised_data={'mmlu': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/mmlu', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/mmlu-eval-aux', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='cais/mmlu', subsets=['all'], splits=['auxiliary_train'], input_path=InputName(step=ExecutorStep(name='raw/cais/mmlu', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='cais/mmlu', revision=VersionedValue(value='c30699e'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/cais/mmluhf', pip_dependency_groups=None), name='c30699e'), hf_path='cais/mmlu', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='answer', answer_label_key='', answer_text_ignore=False, options_key='choices', options_keys=[], answer_labels=['A', 'B', 'C', 'D'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='cais/*.jsonl.gz'), InputName(step=ExecutorStep(name='evaluation/mmlu-eval-subject', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='cais/mmlu', subsets=['*'], splits=['dev', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/cais/mmlu', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='cais/mmlu', revision=VersionedValue(value='c30699e'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/cais/mmluhf', pip_dependency_groups=None), name='c30699e'), hf_path='cais/mmlu', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='answer', answer_label_key='', answer_text_ignore=False, options_key='choices', options_keys=[], answer_labels=['A', 'B', 'C', 'D'], answer_labels_key='', exclude_subsets=['all', 'auxiliary_train'], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='cais/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/mmlu-eval-aux', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='cais/mmlu', subsets=['all'], splits=['auxiliary_train'], input_path=InputName(step=ExecutorStep(name='raw/cais/mmlu', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='cais/mmlu', revision=VersionedValue(value='c30699e'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/cais/mmluhf', pip_dependency_groups=None), name='c30699e'), hf_path='cais/mmlu', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='answer', answer_label_key='', answer_text_ignore=False, options_key='choices', options_keys=[], answer_labels=['A', 'B', 'C', 'D'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='cais/*.jsonl.gz'), InputName(step=ExecutorStep(name='evaluation/mmlu-eval-subject', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='cais/mmlu', subsets=['*'], splits=['dev', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/cais/mmlu', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='cais/mmlu', revision=VersionedValue(value='c30699e'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/cais/mmluhf', pip_dependency_groups=None), name='c30699e'), hf_path='cais/mmlu', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='answer', answer_label_key='', answer_text_ignore=False, options_key='choices', options_keys=[], answer_labels=['A', 'B', 'C', 'D'], answer_labels_key='', exclude_subsets=['all', 'auxiliary_train'], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='cais/*.jsonl.gz')], input_field='prompt', output_field='response', tags=[]), 'boolq': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/boolq', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/boolq-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='google/boolq', subsets=['*'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/google/boolq', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='google/boolq', revision=VersionedValue(value='35b264d'), hf_urls_glob=['**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/google/boolqhf', pip_dependency_groups=None), name='35b264d'), hf_path='google/boolq', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='', answer_label_key='answer', answer_text_ignore=True, options_key='', options_keys=[], answer_labels=[True, False], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='google/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/boolq-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='google/boolq', subsets=['*'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/google/boolq', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='google/boolq', revision=VersionedValue(value='35b264d'), hf_urls_glob=['**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/google/boolqhf', pip_dependency_groups=None), name='35b264d'), hf_path='google/boolq', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='', answer_label_key='answer', answer_text_ignore=True, options_key='', options_keys=[], answer_labels=[True, False], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='google/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core']), 'hellaswag': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/hellaswag', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/hellaswag-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='Rowan/hellaswag', subsets=['*'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/Rowan/hellaswag', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='Rowan/hellaswag', revision=VersionedValue(value='50441ce'), hf_urls_glob=['**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/Rowan/hellaswaghf', pip_dependency_groups=None), name='50441ce'), hf_path='Rowan/hellaswag', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='ctx', answer_text_key='', answer_idx_key='label', answer_label_key='', answer_text_ignore=False, options_key='endings', options_keys=[], answer_labels=['A', 'B', 'C', 'D'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='Rowan/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/hellaswag-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='Rowan/hellaswag', subsets=['*'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/Rowan/hellaswag', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='Rowan/hellaswag', revision=VersionedValue(value='50441ce'), hf_urls_glob=['**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/Rowan/hellaswaghf', pip_dependency_groups=None), name='50441ce'), hf_path='Rowan/hellaswag', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='ctx', answer_text_key='', answer_idx_key='label', answer_label_key='', answer_text_ignore=False, options_key='endings', options_keys=[], answer_labels=['A', 'B', 'C', 'D'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='Rowan/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core']), 'piqa': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/piqa', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/piqa', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='ybisk/piqa', subsets=['*'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/ybisk/piqa', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='ybisk/piqa', revision=VersionedValue(value='142c512'), hf_urls_glob=['**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/ybisk/piqahf', pip_dependency_groups=None), name='142c512'), hf_path='ybisk/piqa', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='goal', answer_text_key='', answer_idx_key='label', answer_label_key='', answer_text_ignore=False, options_key='', options_keys=['sol1', 'sol2'], answer_labels=['1', '2'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='ybisk/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/piqa', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='ybisk/piqa', subsets=['*'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/ybisk/piqa', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='ybisk/piqa', revision=VersionedValue(value='142c512'), hf_urls_glob=['**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/ybisk/piqahf', pip_dependency_groups=None), name='142c512'), hf_path='ybisk/piqa', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='goal', answer_text_key='', answer_idx_key='label', answer_label_key='', answer_text_ignore=False, options_key='', options_keys=['sol1', 'sol2'], answer_labels=['1', '2'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='ybisk/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core']), 'winogrande': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/winogrande', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/winogrande', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/winogrande', subsets=['default'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/winogrande', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/winogrande', revision=VersionedValue(value='ebf71e3'), hf_urls_glob=['winogrande_xl/**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/winograndehf', pip_dependency_groups=None), name='ebf71e3'), hf_path='allenai/winogrande', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='sentence', answer_text_key='', answer_idx_key='', answer_label_key='answer', answer_text_ignore=False, options_key='', options_keys=['option1', 'option2'], answer_labels=['1', '2'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/winogrande', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/winogrande', subsets=['default'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/winogrande', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/winogrande', revision=VersionedValue(value='ebf71e3'), hf_urls_glob=['winogrande_xl/**/*.parquet'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/winograndehf', pip_dependency_groups=None), name='ebf71e3'), hf_path='allenai/winogrande', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='sentence', answer_text_key='', answer_idx_key='', answer_label_key='answer', answer_text_ignore=False, options_key='', options_keys=['option1', 'option2'], answer_labels=['1', '2'], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core']), 'ai2_arc_easy': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/ai2_arc_easy', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/arc-easy', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/ai2_arc', subsets=['ARC-Easy'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/ai2_arc', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/ai2_arc', revision=VersionedValue(value='210d026'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/ai2_archf', pip_dependency_groups=None), name='210d026'), hf_path='allenai/ai2_arc', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='', answer_label_key='answerKey', answer_text_ignore=False, options_key='choices.text', options_keys=[], answer_labels=[], answer_labels_key='choices.label', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core', 'arc'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/arc-easy', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/ai2_arc', subsets=['ARC-Easy'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/ai2_arc', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/ai2_arc', revision=VersionedValue(value='210d026'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/ai2_archf', pip_dependency_groups=None), name='210d026'), hf_path='allenai/ai2_arc', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='', answer_label_key='answerKey', answer_text_ignore=False, options_key='choices.text', options_keys=[], answer_labels=[], answer_labels_key='choices.label', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core', 'arc']), 'ai2_arc_challenge': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/ai2_arc_challenge', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/arc-challenge', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/ai2_arc', subsets=['ARC-Challenge'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/ai2_arc', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/ai2_arc', revision=VersionedValue(value='210d026'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/ai2_archf', pip_dependency_groups=None), name='210d026'), hf_path='allenai/ai2_arc', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='', answer_label_key='answerKey', answer_text_ignore=False, options_key='choices.text', options_keys=[], answer_labels=[], answer_labels_key='choices.label', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core', 'arc'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/arc-challenge', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/ai2_arc', subsets=['ARC-Challenge'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/ai2_arc', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/ai2_arc', revision=VersionedValue(value='210d026'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/ai2_archf', pip_dependency_groups=None), name='210d026'), hf_path='allenai/ai2_arc', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question', answer_text_key='', answer_idx_key='', answer_label_key='answerKey', answer_text_ignore=False, options_key='choices.text', options_keys=[], answer_labels=[], answer_labels_key='choices.label', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core', 'arc']), 'openbookqa': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/openbookqa', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/openbookqa-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/openbookqa', subsets=['main'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/openbookqa', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/openbookqa', revision=VersionedValue(value='388097e'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/openbookqahf', pip_dependency_groups=None), name='388097e'), hf_path='allenai/openbookqa', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question_stem', answer_text_key='', answer_idx_key='', answer_label_key='answerKey', answer_text_ignore=False, options_key='choices.text', options_keys=[], answer_labels=[], answer_labels_key='choices.label', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=['core'], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/openbookqa-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='allenai/openbookqa', subsets=['main'], splits=['train', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/allenai/openbookqa', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='allenai/openbookqa', revision=VersionedValue(value='388097e'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/allenai/openbookqahf', pip_dependency_groups=None), name='388097e'), hf_path='allenai/openbookqa', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='question_stem', answer_text_key='', answer_idx_key='', answer_label_key='answerKey', answer_text_ignore=False, options_key='choices.text', options_keys=[], answer_labels=[], answer_labels_key='choices.label', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='allenai/*.jsonl.gz')], input_field='prompt', output_field='response', tags=['core']), 'openai_humaneval': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/openai_humaneval', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/humaneval-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='openai/openai_humaneval', subsets=['*'], splits=['test'], input_path=InputName(step=ExecutorStep(name='raw/openai/openai_humaneval', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='openai/openai_humaneval', revision=VersionedValue(value='7dce605'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='gs://marin-us-central2/raw/openai/openai_humanevalhf', pip_dependency_groups=None), name='7dce605'), hf_path='openai/openai_humaneval', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='prompt', answer_text_key='canonical_solution', answer_idx_key='', answer_label_key='', answer_text_ignore=False, options_key='', options_keys=[], answer_labels=[], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='openai/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/humaneval-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='openai/openai_humaneval', subsets=['*'], splits=['test'], input_path=InputName(step=ExecutorStep(name='raw/openai/openai_humaneval', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='openai/openai_humaneval', revision=VersionedValue(value='7dce605'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='gs://marin-us-central2/raw/openai/openai_humanevalhf', pip_dependency_groups=None), name='7dce605'), hf_path='openai/openai_humaneval', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='prompt', answer_text_key='canonical_solution', answer_idx_key='', answer_label_key='', answer_text_ignore=False, options_key='', options_keys=[], answer_labels=[], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='openai/*.jsonl.gz')], input_field='prompt', output_field='response', tags=[]), 'mbpp': SupervisedUrlSourceConfig(cache_dir=InputName(step=ExecutorStep(name='tokenized/evaluation/mbpp', fn=<ray.remote_function.RemoteFunction object at 0x705e34ca03d0>, config=TokenizeConfig(train_paths=[], validation_paths=[InputName(step=ExecutorStep(name='evaluation/mbpp-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='google-research-datasets/mbpp', subsets=['*'], splits=['train', 'test', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/google-research-datasets/mbpp', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='google-research-datasets/mbpp', revision=VersionedValue(value='4bb6404'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/google-research-datasets/mbpphf', pip_dependency_groups=None), name='4bb6404/full'), hf_path='google-research-datasets/mbpp', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='text', answer_text_key='code', answer_idx_key='', answer_label_key='', answer_text_ignore=False, options_key='', options_keys=[], answer_labels=[], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='google-research-datasets/*.jsonl.gz')], cache_path=OutputName(name=None), tokenizer=VersionedValue(value='meta-llama/Meta-Llama-3.1-8B'), tags=[], cache_options=CacheOptions(num_shard_groups=1024, target_size_per_flush='512MB', batch_size=128), input_field='prompt', output_field='response', text_key='text'), description=None, override_output_path=None, pip_dependency_groups=None), name=None), train_urls=[], validation_urls=[InputName(step=ExecutorStep(name='evaluation/mbpp-eval', fn=<function raw2json at 0x705e34d422a0>, config=DatasetConversionConfig(dataset_name='google-research-datasets/mbpp', subsets=['*'], splits=['train', 'test', 'validation'], input_path=InputName(step=ExecutorStep(name='raw/google-research-datasets/mbpp', fn=<function download_hf at 0x705e34d411c0>, config=DownloadConfig(gcs_output_path=OutputName(name=None), hf_dataset_id='google-research-datasets/mbpp', revision=VersionedValue(value='4bb6404'), hf_urls_glob=['**/*.parquet', '*.md'], public_gcs_path='gs://hf_dataset_transfer_bucket', wait_for_completion=True, timeout=1800, poll_interval=10), description=None, override_output_path='raw/google-research-datasets/mbpphf', pip_dependency_groups=None), name='4bb6404/full'), hf_path='google-research-datasets/mbpp', output_path=OutputName(name=None), output_format=<OutputFormatOptions.evaluation: 'evaluation'>, prompt_key='text', answer_text_key='code', answer_idx_key='', answer_label_key='', answer_text_ignore=False, options_key='', options_keys=[], answer_labels=[], answer_labels_key='', exclude_subsets=[], revision=None, token=False, trust_remote_code=False), description=None, override_output_path=None, pip_dependency_groups=None), name='google-research-datasets/*.jsonl.gz')], input_field='prompt', output_field='response', tags=[])}, trainer=TrainerConfig(seed=0, mp=Policy(param_dtype=<class 'jax.numpy.float32'>, compute_dtype=<class 'jax.numpy.bfloat16'>, output_dtype=<class 'jax.numpy.bfloat16'>), fp8=None, wandb=None, log_dir=PosixPath('logs'), id=None, tracker=WandbConfig(entity=None, project='suhas-curriculum', name=None, tags=['debug-resume-optimizer-v3-stage1', '702_targeted_curriculum'], id=None, group=None, mode=None, resume='allow', save_code=True, save_xla_dumps=False), profiler=False, profiler_start_step=5, profiler_num_steps=100, profiler_perfetto_link=False, batch_axis='batch', fsdp_axis='embed', tensor_parallel_axes=None, axis_resources={}, parameter_axis_resources={}, replica_ici_axis_size=1, model_axis_size=1, replica_dcn_axis_size=-1, train_batch_size=1024, per_device_parallelism=-1, per_device_eval_parallelism=-1, num_train_steps=100, steps_per_eval=50, max_eval_batches=None, checkpointer=CheckpointerConfig(base_path='checkpoints/', save_interval=datetime.timedelta(seconds=600), keep=[{'every': 100}], append_run_id_to_base_path=True), load_checkpoint=None, load_checkpoint_path=None, initialize_from=None, jax_config={'jax_threefry_partitionable': True, 'jax_softmax_custom_jvp': True}, distributed=DistributedConfig(coordinator_address=None, num_processes=None, process_id=None, local_device_ids=None), ray=RayConfig(address=None, start_workers=True, auto_start_cluster=True), require_accelerator=None, shutdown_at_exit=False), model=LlamaConfig(cross_entropy_block_size=None, seq_len=1024, hidden_dim=512, intermediate_dim=1792, num_layers=6, num_heads=8, num_kv_heads=8, activation_function='silu', initializer_range=0.02, layer_norm_epsilon=1e-05, tie_word_embeddings=False, upcast_attn=False, use_flash_attention=True, attn_backend=None, flash_attention_block_size=None, gradient_checkpointing=True, gradient_checkpointing_block_size=5, scan_layers=True, use_bias=False, use_layer_norm_weight=True, rope=DefaultRotaryEmbeddingsConfig(theta=10000, factor=1.0), reference_checkpoint='meta-llama/Llama-2-7b-hf', tokenizer=None), optimizer=AdamConfig(learning_rate=0.003, weight_decay=0.1, min_lr_ratio=0.1, warmup=2.0, decay=198.0, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=[200], lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.9, beta2=0.95, epsilon=1e-08, max_grad_norm=1.0), initialize_from_hf=False, use_hf_model_config=False, fcm_prob=0.0, z_loss_weight=None, hf_save_path=None, hf_upload=None, hf_save_steps=100, data_seed=42, initialize_from_checkpoint_path=None, epoch=0, eval_harness=None, eval_harness_steps=None, output_path=OutputName(name=None), tpu_type='v4-128', env={}, bypass_path_checks=False, impute_run_id_from_output_path=True, node_count=1), description='Train a llama_300m model for 100 (steps) * 1024 (batch_size) * 1024 (seq_len) = 104,857,600 tokens.', override_output_path='checkpoints/suhas/debug-resume-optimizer-v3-stage1', pip_dependency_groups=None), name=None)