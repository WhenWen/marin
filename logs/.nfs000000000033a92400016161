2025-08-07 07:22:28,924	INFO ray_run.py:64 -- Dependencies extracted: ['draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:22:29,530	INFO ray_run.py:131 -- Submitting job with entrypoint:  python experiments/optimizer_sweep/PhaseI/exp725_mudamsweep_130M_8.py --force_run_failed True
2025-08-07 07:22:29,530	INFO ray_run.py:132 -- Dependencies: [
    "draccus>=0.11.5",
    "google-api-python-client>=2.175.0",
    "gcsfs",
    "google-cloud-storage",
    "google-cloud-storage-transfer",
    "cryptography>=45",
    "s3fs>=2024",
    "datasets==3.1.0",
    "regex",
    "requests",
    "numpy",
    "torch",
    "braceexpand",
    "deepdiff",
    "tqdm",
    "tqdm-loggable",
    "toml",
    "pandas",
    "pyarrow",
    "multiprocess==0.70.16",
    "levanter>=1.2.dev1359",
    "haliax>=1.4.dev348",
    "sentencepiece",
    "lz4",
    "wandb<=0.19.9",
    "openai"
]
2025-08-07 07:22:29,530	INFO ray_run.py:133 -- env_vars: {
    "PYTHONPATH": "submodules/levanter:submodules/levanter/src:./submodules/levanter/src:${PYTHONPATH}",
    "LIBTPU_INIT_ARGS": "--xla_tpu_scoped_vmem_limit_kib=81920 --xla_enable_async_all_gather=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_data_parallel_all_reduce_opt=true --xla_tpu_data_parallel_opt_different_sized_ops=true --xla_tpu_megacore_fusion_allow_ags=false --xla_enable_async_collective_permute=true --xla_tpu_enable_ag_backward_pipelining=true",
    "WANDB_API_KEY": "c10de4d92e8d3857f5ffe660fbcf7c4ae067b062"
}
2025-08-07 07:22:29,530	INFO ray_run.py:135 -- Terminal command: 
ray job submit --runtime-env-json '{"pip": ["draccus>=0.11.5", "google-api-python-client>=2.175.0", "gcsfs", "google-cloud-storage", "google-cloud-storage-transfer", "cryptography>=45", "s3fs>=2024", "datasets==3.1.0", "regex", "requests", "numpy", "torch", "braceexpand", "deepdiff", "tqdm", "tqdm-loggable", "toml", "pandas", "pyarrow", "multiprocess==0.70.16", "levanter>=1.2.dev1359", "haliax>=1.4.dev348", "sentencepiece", "lz4", "wandb<=0.19.9", "openai"], "working_dir": "/tiger/u/kaiyue/workspace/TPU/Marins/marin-release", "env_vars": {"PYTHONPATH": "submodules/levanter:submodules/levanter/src:./submodules/levanter/src:${PYTHONPATH}", "LIBTPU_INIT_ARGS": "--xla_tpu_scoped_vmem_limit_kib=81920 --xla_enable_async_all_gather=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_data_parallel_all_reduce_opt=true --xla_tpu_data_parallel_opt_different_sized_ops=true --xla_tpu_megacore_fusion_allow_ags=false --xla_enable_async_collective_permute=true --xla_tpu_enable_ag_backward_pipelining=true", "WANDB_API_KEY": "c10de4d92e8d3857f5ffe660fbcf7c4ae067b062"}, "config": {"setup_timeout_seconds": 1800}, "excludes": ["logs/exp725_soapsweep_1.2B_8.txt", "logs/exp725_mudamsweep_300m_1.txt", "logs/exp725_muonsweep_1.2B_8.txt", "logs/exp725_mudamsweep_130M_4.txt", "logs/exp725_mudamsweep_520M_1.txt", "logs/exp725_sophiasweep_520M_1.txt", "logs/exp725_mudamsweep_130M_2.txt", "logs/exp725_mudamsweep_130M_1.txt", "logs/exp725_muonsweep_1.2B_4.txt", "logs/exp725_mudamsweep_130M_8.txt", "logs/exp725_mudamsweep_300M_1.txt", "logs/exp725_mudamsweep_520m_1.txt", "logs/exp725_muonsweep_1.2B_2.txt", "logs/exp725_sophiasweep_130M_8.txt", "logs/exp725_muonsweep_130M_16.txt", ".git/ORIG_HEAD", ".git/COMMIT_EDITMSG", ".git/HEAD", ".git/objects", ".git/index", ".git/logs", ".git/config", ".git/info", ".git/packed-refs", ".git/branches", ".git/FETCH_HEAD", ".git/hooks", ".git/description", ".git/refs"]}' --  python experiments/optimizer_sweep/PhaseI/exp725_mudamsweep_130M_8.py --force_run_failed True
2025-08-07 07:22:33,269	INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_59b88c775c4e0044.zip.
2025-08-07 07:22:33,270	INFO packaging.py:576 -- Creating a file package for local module '/tiger/u/kaiyue/workspace/TPU/Marins/marin-release'.
2025-08-07 07:22:51,741	INFO ray_run.py:147 -- Job submitted with ID: raysubmit_rjhKS7e4ryTuQEUn
2025-08-07 07:22:51,741	INFO ray_run.py:148 -- Job URL: http://localhost:8265/#/jobs/raysubmit_rjhKS7e4ryTuQEUn
['logs/exp725_soapsweep_1.2B_8.txt', 'logs/exp725_mudamsweep_300m_1.txt', 'logs/exp725_muonsweep_1.2B_8.txt', 'logs/exp725_mudamsweep_130M_4.txt', 'logs/exp725_mudamsweep_520M_1.txt', 'logs/exp725_sophiasweep_520M_1.txt', 'logs/exp725_mudamsweep_130M_2.txt', 'logs/exp725_mudamsweep_130M_1.txt', 'logs/exp725_muonsweep_1.2B_4.txt', 'logs/exp725_mudamsweep_130M_8.txt', 'logs/exp725_mudamsweep_300M_1.txt', 'logs/exp725_mudamsweep_520m_1.txt', 'logs/exp725_muonsweep_1.2B_2.txt', 'logs/exp725_sophiasweep_130M_8.txt', 'logs/exp725_muonsweep_130M_16.txt', '.git/ORIG_HEAD', '.git/COMMIT_EDITMSG', '.git/HEAD', '.git/objects', '.git/index', '.git/logs', '.git/config', '.git/info', '.git/packed-refs', '.git/branches', '.git/FETCH_HEAD', '.git/hooks', '.git/description', '.git/refs']
2025-08-07 07:22:51,657	INFO job_manager.py:531 -- Runtime env is setting up.
('130m', '21B', 'mudam')
3.239591598510742
Current best config: {'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.1, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.95, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}
Current Approximate best config: [{'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.2, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.9, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}, {'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.2, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.99, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}, {'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.1, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.95, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}, {'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.2, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.95, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}, {'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.2, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.98, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}]
{'learning_rate': [0.002, 0.004, 0.008], 'weight_decay': [0, 0.1, 0.2, 0.3], 'beta1': [0.9, 0.95, 0.98, 0.99], 'beta2': [0.95, 0.95, 0.98, 0.99], 'shampoo_beta': [0.95, 0.98, 0.99], 'momentum': [0.9, 0.95, 0.98], 'adam_lr': [0.002, 0.004, 0.008, 0.016], 'warmup': [0, 500, 1000], 'normalization': ['muon', None], 'train_batch_size': [128, 256], 'min_lr_ratio': [0, 0.05, 0.1]}
{'learning_rate': [0.002, 0.004, 0.008], 'weight_decay': [0, 0.1, 0.2, 0.3], 'beta1': [0.9, 0.95, 0.98, 0.99], 'beta2': [0.95, 0.95, 0.98, 0.99], 'shampoo_beta': [0.95, 0.98, 0.99], 'momentum': [0.9, 0.95, 0.98], 'adam_lr': [0.002, 0.004, 0.008, 0.016], 'warmup': [0, 500, 1000], 'normalization': ['muon', None], 'train_batch_size': [128, 256], 'min_lr_ratio': [0, 0.05, 0.1]}
{'learning_rate': [0.002, 0.004, 0.008], 'weight_decay': [0, 0.1, 0.2, 0.3], 'beta1': [0.9, 0.95, 0.98, 0.99], 'beta2': [0.95, 0.95, 0.98, 0.99], 'shampoo_beta': [0.95, 0.98, 0.99], 'momentum': [0.9, 0.95, 0.98], 'adam_lr': [0.002, 0.004, 0.008, 0.016], 'warmup': [0, 500, 1000], 'normalization': ['muon', None], 'train_batch_size': [128, 256], 'min_lr_ratio': [0, 0.05, 0.1]}
{'learning_rate': [0.002, 0.004, 0.008], 'weight_decay': [0, 0.1, 0.2, 0.3], 'beta1': [0.9, 0.95, 0.98, 0.99], 'beta2': [0.95, 0.95, 0.98, 0.99], 'shampoo_beta': [0.95, 0.98, 0.99], 'momentum': [0.9, 0.95, 0.98], 'adam_lr': [0.002, 0.004, 0.008, 0.016], 'warmup': [0, 500, 1000], 'normalization': ['muon', None], 'train_batch_size': [128, 256], 'min_lr_ratio': [0, 0.05, 0.1]}
{'learning_rate': [0.002, 0.004, 0.008], 'weight_decay': [0, 0.1, 0.2, 0.3], 'beta1': [0.9, 0.95, 0.98, 0.99], 'beta2': [0.95, 0.95, 0.98, 0.99], 'shampoo_beta': [0.95, 0.98, 0.99], 'momentum': [0.9, 0.95, 0.98], 'adam_lr': [0.002, 0.004, 0.008, 0.016], 'warmup': [0, 500, 1000], 'normalization': ['muon', None], 'train_batch_size': [128, 256], 'min_lr_ratio': [0, 0.05, 0.1]}
{'learning_rate': [0.002, 0.004, 0.008], 'weight_decay': [0, 0.1, 0.2, 0.3], 'beta1': [0.9, 0.95, 0.98, 0.99], 'beta2': [0.95, 0.95, 0.98, 0.99], 'shampoo_beta': [0.95, 0.98, 0.99], 'momentum': [0.9, 0.95, 0.98], 'adam_lr': [0.002, 0.004, 0.008, 0.016], 'warmup': [0, 500, 1000], 'normalization': ['muon', None], 'train_batch_size': [128, 256], 'min_lr_ratio': [0, 0.05, 0.1]}
All configs are nearly equally close to finish
Choose: {'learning_rate': 0.004, 'adam_lr': 0.004, 'weight_decay': 0.1, 'min_lr_ratio': 0, 'warmup': 500, 'beta1': 0.95, 'momentum': 0.95, 'beta2': 0.99, 'shampoo_beta': 0.95, 'epsilon': 1e-15, 'max_grad_norm': 1, 'lr_schedule': 'cosine', 'train_batch_size': 128, 'normalization': 'muon'}
Need to run: [SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.002, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.008, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.9, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.98, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.99, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.95, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.95, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.98, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.98, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.99, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.9, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.002, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.008, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.016, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=0, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=1000, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization=None, another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=256, num_train_steps=20480, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0.05, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10)), SimpleTrainConfig(resources=TpuPodConfig(tpu_type=VersionedValue(value='v5litepod-128'), slice_count=1, runtime_env={}, device_flops_override=None), train_batch_size=128, num_train_steps=40960, learning_rate=None, data_seed=None, weight_decay=None, beta1=None, beta2=None, epsilon=None, max_grad_norm=None, warmup=None, decay=None, rewarmup=None, lr_schedule=None, stable_lr_schedule=None, min_lr_ratio=None, cycle_length=None, z_loss_weight=None, ema_beta=None, nesterov=None, precision='p=f32,c=bfloat16', id=None, ckpt_path=None, steps_per_eval=1000, steps_per_export=10000, steps_per_task_eval=None, steps_per_hf_export=None, per_device_eval_parallelism=None, max_eval_batches=None, initialize_from_checkpoint_path=None, initialize_from_hf=None, reset_data_loader_on_init=True, allow_partial_checkpoint=False, int8=False, optimizer_config=MudamConfig(learning_rate=0.004, weight_decay=0.1, min_lr_ratio=0.1, warmup=500, decay=None, rewarmup=0.0, cooldown=None, cycle_length=None, cycles=None, lr_schedule='cosine', haps=None, weight_decay_modules=None, default_weight_decay_mask=None, beta1=0.95, momentum=0.95, shampoo_beta=0.95, beta2=0.99, epsilon=1e-15, max_grad_norm=1, schedule_list=None, max_precond_dim=10000, merge_small_dims=True, target_merged_dim_size=2048, mu_dtype=None, precond_dtype=None, partition_grads_into_blocks=False, adam_lr=0.004, block_size=256, steps=5, force_full_rank=False, use_mudam_for_embedding=False, use_nesterov_in_second_moment=False, use_momentum_in_second_moment=False, reduce_to_muon=False, prefer_embedding_side=False, prefer_input_side=True, use_nesterov_in_first_moment=True, nesterov_adam=True, normalization='muon', another_muon=None), watch=WatchConfig(watch_targets=['grads', 'params'], include_norms=True, include_per_parameter_norms=True, include_histograms=False, split_scan_layers=True, interval=10))]
Closest to finish: 20
2025-08-07 07:23:13,734	INFO worker.py:1554 -- Using address 10.164.0.67:6379 set in the environment variable RAY_ADDRESS
2025-08-07 07:23:13,735	INFO worker.py:1694 -- Connecting to existing Ray cluster at address: 10.164.0.67:6379...
2025-08-07 07:23:13,746	INFO worker.py:1879 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://10.164.0.67:8265 [39m[22m
2025-08-07 07:23:13,767	INFO executor.py:1095 -- Ray init took 0.03s
2025-08-07 07:23:13,865	INFO executor.py:554 -- ### Inspecting the 10 provided steps ###
2025-08-07 07:23:13,866	WARNING executor.py:798 -- Output path gs://marin-eu-west4/raw/dclm-baseline-1.0-ba19eb doesn't match given override raw/dclm, using the latter.
2025-08-07 07:23:13,866	WARNING executor.py:798 -- Output path gs://marin-eu-west4/tokenized/dclm_baseline-951f3f doesn't match given override tokenized/dclm_baseline-0206f1/, using the latter.
2025-08-07 07:23:13,866	WARNING executor.py:798 -- Output path gs://marin-eu-west4/raw/starcoderdata-29d91d doesn't match given override raw/starcoderdata-720c8c, using the latter.
2025-08-07 07:23:13,867	WARNING executor.py:798 -- Output path gs://marin-eu-west4/tokenized/starcoderdata-377a39 doesn't match given override tokenized/starcoderdata-12f018/, using the latter.
2025-08-07 07:23:13,867	WARNING executor.py:798 -- Output path gs://marin-eu-west4/raw/proof-pile-2-67a30b doesn't match given override raw/proof-pile-2-f1b1d8, using the latter.
2025-08-07 07:23:13,867	WARNING executor.py:798 -- Output path gs://marin-eu-west4/tokenized/proofpile_2-4a35c7 doesn't match given override tokenized/proofpile_2-4a35c7/, using the latter.
2025-08-07 07:23:13,876	WARNING executor.py:813 -- Multiple `ExecutorStep`s (named checkpoints/sweep-130m-21B-mudamv4d53eelr0.004-alr0.004-wd0.1-minlr0-warmup5) have the same version; try to instantiate only once.
2025-08-07 07:23:13,884	INFO executor.py:562 -- ### Reading 32 statuses ###
2025-08-07 07:23:13,884	INFO executor.py:572 -- ### Launching 32 steps ###
2025-08-07 07:23:13,885	INFO executor.py:647 -- raw/paloma: marin.download.huggingface.download_gated_manual.download_and_upload_to_store
2025-08-07 07:23:13,885	INFO executor.py:648 --   output_path = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:13,885	INFO executor.py:649 --   config = {"hf_dataset_id": "allenai/paloma", "revision": "65cd6fc"}
2025-08-07 07:23:13,885	INFO executor.py:658 --   pip_dependencies = None
2025-08-07 07:23:13,885	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:14,073	INFO executor.py:1004 -- Status raw/paloma: marin.download.huggingface.download_gated_manual.download_and_upload_to_store : SUCCESS.
2025-08-07 07:23:14,075	INFO executor.py:1028 -- Step raw/paloma: marin.download.huggingface.download_gated_manual.download_and_upload_to_store has already succeeded. Status: SUCCESS
2025-08-07 07:23:14,331	INFO executor.py:647 -- raw/proof-pile-2: marin.download.huggingface.download.download
2025-08-07 07:23:14,331	INFO executor.py:648 --   output_path = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:14,331	INFO executor.py:649 --   config = {}
2025-08-07 07:23:14,331	INFO executor.py:658 --   pip_dependencies = None
2025-08-07 07:23:14,331	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:14,446	INFO executor.py:1004 -- Status raw/proof-pile-2: marin.download.huggingface.download.download : SUCCESS.
2025-08-07 07:23:14,448	INFO executor.py:1028 -- Step raw/proof-pile-2: marin.download.huggingface.download.download has already succeeded. Status: SUCCESS
2025-08-07 07:23:14,696	INFO executor.py:647 -- raw/starcoderdata: marin.download.huggingface.download_hf.download_hf
2025-08-07 07:23:14,696	INFO executor.py:648 --   output_path = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:14,697	INFO executor.py:649 --   config = {}
2025-08-07 07:23:14,697	INFO executor.py:658 --   pip_dependencies = None
2025-08-07 07:23:14,697	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:14,813	INFO executor.py:1004 -- Status raw/starcoderdata: marin.download.huggingface.download_hf.download_hf : SUCCESS.
2025-08-07 07:23:14,814	INFO executor.py:1028 -- Step raw/starcoderdata: marin.download.huggingface.download_hf.download_hf has already succeeded. Status: SUCCESS
2025-08-07 07:23:15,028	INFO executor.py:647 -- raw/dclm-baseline-1.0: marin.download.huggingface.download.download
2025-08-07 07:23:15,028	INFO executor.py:648 --   output_path = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:15,028	INFO executor.py:649 --   config = {}
2025-08-07 07:23:15,028	INFO executor.py:658 --   pip_dependencies = None
2025-08-07 07:23:15,028	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:15,159	INFO executor.py:1004 -- Status raw/dclm-baseline-1.0: marin.download.huggingface.download.download : SUCCESS.
2025-08-07 07:23:15,161	INFO executor.py:1028 -- Step raw/dclm-baseline-1.0: marin.download.huggingface.download.download has already succeeded. Status: SUCCESS
2025-08-07 07:23:15,409	INFO executor.py:647 -- tokenized/paloma/wikitext_103: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:15,410	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:15,410	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:15,410	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:15,413	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:15,413	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:15,529	INFO executor.py:1004 -- Status tokenized/paloma/wikitext_103: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:15,531	INFO executor.py:1028 -- Step tokenized/paloma/wikitext_103: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:15,714	INFO executor.py:647 -- tokenized/paloma/twitterAAE_HELM_fixed: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:15,714	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:15,714	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:15,714	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:15,717	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:15,717	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:15,825	INFO executor.py:1004 -- Status tokenized/paloma/twitterAAE_HELM_fixed: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:15,827	INFO executor.py:1028 -- Step tokenized/paloma/twitterAAE_HELM_fixed: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:16,026	INFO executor.py:647 -- tokenized/paloma/redpajama: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:16,026	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:16,026	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/redpajama/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:16,026	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:16,029	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:16,029	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:16,128	INFO executor.py:1004 -- Status tokenized/paloma/redpajama: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:16,130	INFO executor.py:1028 -- Step tokenized/paloma/redpajama: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:16,312	INFO executor.py:647 -- tokenized/paloma/ptb: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:16,312	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:16,312	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/ptb/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:16,312	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:16,315	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:16,315	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:16,448	INFO executor.py:1004 -- Status tokenized/paloma/ptb: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:16,450	INFO executor.py:1028 -- Step tokenized/paloma/ptb: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:16,622	INFO executor.py:647 -- tokenized/paloma/mc4: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:16,622	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:16,622	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/mc4/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:16,622	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:16,625	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:16,625	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:16,726	INFO executor.py:1004 -- Status tokenized/paloma/mc4: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:16,728	INFO executor.py:1028 -- Step tokenized/paloma/mc4: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:16,913	INFO executor.py:647 -- tokenized/paloma/manosphere_meta_sep: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:16,913	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:16,913	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:16,913	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:16,916	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:16,916	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:17,013	INFO executor.py:1004 -- Status tokenized/paloma/manosphere_meta_sep: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:17,015	INFO executor.py:1028 -- Step tokenized/paloma/manosphere_meta_sep: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:17,204	INFO executor.py:647 -- tokenized/paloma/m2d2_wikipedia_unsplit: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:17,204	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:17,204	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:17,204	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:17,207	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:17,207	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:17,335	INFO executor.py:1004 -- Status tokenized/paloma/m2d2_wikipedia_unsplit: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:17,336	INFO executor.py:1028 -- Step tokenized/paloma/m2d2_wikipedia_unsplit: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:17,535	INFO executor.py:647 -- tokenized/paloma/m2d2_s2orc_unsplit: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:17,535	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:17,535	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:17,535	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:17,538	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:17,538	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:17,653	INFO executor.py:1004 -- Status tokenized/paloma/m2d2_s2orc_unsplit: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:17,655	INFO executor.py:1028 -- Step tokenized/paloma/m2d2_s2orc_unsplit: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:17,831	INFO executor.py:647 -- tokenized/paloma/gab: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:17,831	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:17,831	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/gab/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:17,831	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:17,833	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:17,833	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:17,929	INFO executor.py:1004 -- Status tokenized/paloma/gab: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:17,931	INFO executor.py:1028 -- Step tokenized/paloma/gab: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:18,127	INFO executor.py:647 -- tokenized/paloma/falcon-refinedweb: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:18,127	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:18,128	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:18,128	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:18,130	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:18,130	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:18,232	INFO executor.py:1004 -- Status tokenized/paloma/falcon-refinedweb: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:18,234	INFO executor.py:1028 -- Step tokenized/paloma/falcon-refinedweb: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:18,408	INFO executor.py:647 -- tokenized/paloma/dolma_100_subreddits: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:18,408	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:18,408	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:18,408	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:18,411	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:18,411	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:18,516	INFO executor.py:1004 -- Status tokenized/paloma/dolma_100_subreddits: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:18,518	INFO executor.py:1028 -- Step tokenized/paloma/dolma_100_subreddits: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:18,689	INFO executor.py:647 -- tokenized/paloma/dolma_100_programing_languages: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:18,689	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:18,689	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:18,689	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:18,692	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:18,692	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:18,789	INFO executor.py:1004 -- Status tokenized/paloma/dolma_100_programing_languages: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:18,791	INFO executor.py:1028 -- Step tokenized/paloma/dolma_100_programing_languages: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:19,002	INFO executor.py:647 -- tokenized/paloma/dolma-v1_5: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:19,003	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:19,003	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:19,003	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:19,005	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:19,005	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:19,112	INFO executor.py:1004 -- Status tokenized/paloma/dolma-v1_5: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:19,114	INFO executor.py:1028 -- Step tokenized/paloma/dolma-v1_5: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:19,296	INFO executor.py:647 -- tokenized/paloma/c4_en: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:19,296	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:19,296	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/c4_en/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:19,296	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:19,298	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:19,299	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:19,416	INFO executor.py:1004 -- Status tokenized/paloma/c4_en: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:19,418	INFO executor.py:1028 -- Step tokenized/paloma/c4_en: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:19,615	INFO executor.py:647 -- tokenized/paloma/c4_100_domains: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:19,615	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:19,616	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:19,616	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:19,618	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:19,618	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:19,733	INFO executor.py:1004 -- Status tokenized/paloma/c4_100_domains: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:19,735	INFO executor.py:1028 -- Step tokenized/paloma/c4_100_domains: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:19,897	INFO executor.py:647 -- tokenized/paloma/4chan: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:19,898	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:19,898	INFO executor.py:649 --   config = {"validation_paths.[0]": "DEP[0]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:19,898	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:19,900	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:19,900	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:20,011	INFO executor.py:1004 -- Status tokenized/paloma/4chan: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:20,013	INFO executor.py:1028 -- Step tokenized/paloma/4chan: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:20,183	INFO executor.py:647 -- tokenized/proofpile_2: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:20,183	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:20,183	INFO executor.py:649 --   config = {"train_paths.[0]": "DEP[0]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:20,183	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:20,186	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:20,186	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:20,304	INFO executor.py:1004 -- Status tokenized/proofpile_2: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:20,306	INFO executor.py:1028 -- Step tokenized/proofpile_2: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:20,493	INFO executor.py:647 -- tokenized/starcoderdata: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:20,493	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:20,493	INFO executor.py:649 --   config = {"train_paths.[0]": "DEP[0]", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:20,493	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:20,496	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:20,496	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:20,589	INFO executor.py:1004 -- Status tokenized/starcoderdata: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:20,591	INFO executor.py:1028 -- Step tokenized/starcoderdata: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:20,807	INFO executor.py:647 -- tokenized/dclm_baseline: marin.processing.tokenize.tokenize.tokenize
2025-08-07 07:23:20,807	INFO executor.py:648 --   output_path = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:20,807	INFO executor.py:649 --   config = {"train_paths.[0]": "DEP[0]/a3b142c", "tokenizer": "meta-llama/Meta-Llama-3.1-8B"}
2025-08-07 07:23:20,807	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:20,810	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:20,810	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:20,942	INFO executor.py:1004 -- Status tokenized/dclm_baseline: marin.processing.tokenize.tokenize.tokenize : SUCCESS.
2025-08-07 07:23:20,944	INFO executor.py:1028 -- Step tokenized/dclm_baseline: marin.processing.tokenize.tokenize.tokenize has already succeeded. Status: SUCCESS
2025-08-07 07:23:21,149	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamv1f7123lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:21,150	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv1f7123lr0.004-alr0.004-wd0.1-minlr0-warmup5-8890bb
2025-08-07 07:23:21,150	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:21,150	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,151	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:21,151	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,151	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:21,151	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,151	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:21,151	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,153	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:21,153	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:21,279	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamv1f7123lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : SUCCESS.
2025-08-07 07:23:21,281	INFO executor.py:1028 -- Step checkpoints/sweep-130m-21B-mudamv1f7123lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train has already succeeded. Status: SUCCESS
2025-08-07 07:23:21,460	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamv628ceflr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:21,461	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv628ceflr0.004-alr0.004-wd0.1-minlr0-warmup5-b27f69
2025-08-07 07:23:21,461	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,461	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:21,462	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,464	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:21,464	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:21,592	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamv628ceflr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : SUCCESS.
2025-08-07 07:23:21,594	INFO executor.py:1028 -- Step checkpoints/sweep-130m-21B-mudamv628ceflr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train has already succeeded. Status: SUCCESS
2025-08-07 07:23:21,805	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamv4a02balr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:21,806	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv4a02balr0.004-alr0.004-wd0.1-minlr0-warmup5-a3ee6b
2025-08-07 07:23:21,806	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:21,806	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:21,809	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:21,809	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:21,906	INFO executor.py:975 -- Status of output_path = 'gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv4a02balr0.004-alr0.004-wd0.1-minlr0-warmup5-a3ee6b' is RUNNING as per GCP. But as per Ray, the task has terminated, it's likely that this task was cancelled previously
2025-08-07 07:23:21,906	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamv4a02balr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : CANCELLED.
2025-08-07 07:23:21,908	INFO executor.py:1019 -- Step checkpoints/sweep-130m-21B-mudamv4a02balr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train was cancelled previously. Retrying.
2025-08-07 07:23:22,092	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamv4d53eelr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:22,092	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv4d53eelr0.004-alr0.004-wd0.1-minlr0-warmup5-008cbd
2025-08-07 07:23:22,092	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,092	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:22,093	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,095	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:22,096	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:22,198	INFO executor.py:975 -- Status of output_path = 'gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv4d53eelr0.004-alr0.004-wd0.1-minlr0-warmup5-008cbd' is RUNNING as per GCP. But as per Ray, the task has terminated, it's likely that this task was cancelled previously
2025-08-07 07:23:22,198	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamv4d53eelr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : CANCELLED.
2025-08-07 07:23:22,200	INFO executor.py:1019 -- Step checkpoints/sweep-130m-21B-mudamv4d53eelr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train was cancelled previously. Retrying.
2025-08-07 07:23:22,361	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamvbcec8dlr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:22,361	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamvbcec8dlr0.004-alr0.004-wd0.1-minlr0-warmup5-0c9dc4
2025-08-07 07:23:22,361	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:22,362	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,365	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:22,365	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:22,435	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamvbcec8dlr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : FAILED.
2025-08-07 07:23:22,437	INFO executor.py:1023 -- Force running checkpoints/sweep-130m-21B-mudamvbcec8dlr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train, previous status: FAILED
2025-08-07 07:23:22,608	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamvc8ffa7lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:22,608	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamvc8ffa7lr0.004-alr0.004-wd0.1-minlr0-warmup5-034336
2025-08-07 07:23:22,608	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:22,608	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,611	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:22,611	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:22,741	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamvc8ffa7lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : SUCCESS.
2025-08-07 07:23:22,743	INFO executor.py:1028 -- Step checkpoints/sweep-130m-21B-mudamvc8ffa7lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train has already succeeded. Status: SUCCESS
2025-08-07 07:23:22,912	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamvc0c6b9lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:22,912	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamvc0c6b9lr0.004-alr0.004-wd0.1-minlr0-warmup5-a3c133
2025-08-07 07:23:22,912	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:22,912	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:22,913	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:22,916	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:22,916	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:23,050	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamvc0c6b9lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : SUCCESS.
2025-08-07 07:23:23,052	INFO executor.py:1028 -- Step checkpoints/sweep-130m-21B-mudamvc0c6b9lr0.004-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train has already succeeded. Status: SUCCESS
2025-08-07 07:23:23,220	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamvef4176lr0.008-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:23,221	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamvef4176lr0.008-alr0.004-wd0.1-minlr0-warmup5-bdca6a
2025-08-07 07:23:23,221	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:23,221	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,224	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:23,224	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:23,317	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamvef4176lr0.008-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : SUCCESS.
2025-08-07 07:23:23,319	INFO executor.py:1028 -- Step checkpoints/sweep-130m-21B-mudamvef4176lr0.008-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train has already succeeded. Status: SUCCESS
2025-08-07 07:23:23,488	INFO executor.py:647 -- checkpoints/sweep-130m-21B-mudamv1879fblr0.002-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train
2025-08-07 07:23:23,489	INFO executor.py:648 --   output_path = gs://marin-eu-west4/checkpoints/sweep-130m-21B-mudamv1879fblr0.002-alr0.004-wd0.1-minlr0-warmup5-2e9fd3
2025-08-07 07:23:23,489	INFO executor.py:649 --   config = {"train_config.data.tokenizer": "meta-llama/Meta-Llama-3.1-8B", "train_config.data.configs.dclm_baseline.cache_dir": "DEP[0]", "train_config.data.configs.dclm_baseline.train_urls.[0]": "DEP[1]/a3b142c", "train_config.data.configs.starcoderdata.cache_dir": "DEP[2]", "train_config.data.configs.starcoderdata.train_urls.[0]": "DEP[3]", "train_config.data.configs.proofpile_2.cache_dir": "DEP[4]", "train_config.data.configs.proofpile_2.train_urls.[0]": "DEP[5]/901a927/huggingface.co/datasets/EleutherAI/proof-pile-2/resolve/901a927", "train_config.data.configs.paloma/4chan.cache_dir": "DEP[6]", "train_config.data.configs.paloma/4chan.validation_urls.[0]": "DEP[7]/65cd6fc/4chan_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_100_domains.cache_dir": "DEP[8]", "train_config.data.configs.paloma/c4_100_domains.validation_urls.[0]": "DEP[9]/65cd6fc/c4_100_domains/val/val*.jsonl.gz", "train_config.data.configs.paloma/c4_en.cache_dir": "DEP[10]", "train_config.data.configs.paloma/c4_en.validation_urls.[0]": "DEP[11]/65cd6fc/c4_en/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma-v1_5.cache_dir": "DEP[12]", "train_config.data.configs.paloma/dolma-v1_5.validation_urls.[0]": "DEP[13]/65cd6fc/dolma-v1_5/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_programing_languages.cache_dir": "DEP[14]", "train_config.data.configs.paloma/dolma_100_programing_languages.validation_urls.[0]": "DEP[15]/65cd6fc/dolma_100_programing_languages/val/val*.jsonl.gz", "train_config.data.configs.paloma/dolma_100_subreddits.cache_dir": "DEP[16]", "train_config.data.configs.paloma/dolma_100_subreddits.validation_urls.[0]": "DEP[17]/65cd6fc/dolma_100_subreddits/val/val*.jsonl.gz", "train_config.data.configs.paloma/falcon-refinedweb.cache_dir": "DEP[18]", "train_config.data.configs.paloma/falcon-refinedweb.validation_urls.[0]": "DEP[19]/65cd6fc/falcon-refinedweb/val/val*.jsonl.gz", "train_config.data.configs.paloma/gab.cache_dir": "DEP[20]", "train_config.data.configs.paloma/gab.validation_urls.[0]": "DEP[21]/65cd6fc/gab/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.cache_dir": "DEP[22]", "train_config.data.configs.paloma/m2d2_s2orc_unsplit.validation_urls.[0]": "DEP[23]/65cd6fc/m2d2_s2orc_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.cache_dir": "DEP[24]", "train_config.data.configs.paloma/m2d2_wikipedia_unsplit.validation_urls.[0]": "DEP[25]/65cd6fc/m2d2_wikipedia_unsplit/val/val*.jsonl.gz", "train_config.data.configs.paloma/manosphere_meta_sep.cache_dir": "DEP[26]", "train_config.data.configs.paloma/manosphere_meta_sep.validation_urls.[0]": "DEP[27]/65cd6fc/manosphere_meta_sep/val/val*.jsonl.gz", "train_config.data.configs.paloma/mc4.cache_dir": "DEP[28]", "train_config.data.configs.paloma/mc4.validation_urls.[0]": "DEP[29]/65cd6fc/mc4/val/val*.jsonl.gz", "train_config.data.configs.paloma/ptb.cache_dir": "DEP[30]", "train_config.data.configs.paloma/ptb.validation_urls.[0]": "DEP[31]/65cd6fc/ptb/val/val*.jsonl.gz", "train_config.data.configs.paloma/redpajama.cache_dir": "DEP[32]", "train_config.data.configs.paloma/redpajama.validation_urls.[0]": "DEP[33]/65cd6fc/redpajama/val/val*.jsonl.gz", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.cache_dir": "DEP[34]", "train_config.data.configs.paloma/twitterAAE_HELM_fixed.validation_urls.[0]": "DEP[35]/65cd6fc/twitterAAE_HELM_fixed/val/val*.jsonl.gz", "train_config.data.configs.paloma/wikitext_103.cache_dir": "DEP[36]", "train_config.data.configs.paloma/wikitext_103.validation_urls.[0]": "DEP[37]/65cd6fc/wikitext_103/val/val*.jsonl.gz", "resources.tpu_type": "v5litepod-128"}
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[0] = gs://marin-eu-west4/tokenized/dclm_baseline-0206f1/
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[1] = gs://marin-eu-west4/raw/dclm
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[2] = gs://marin-eu-west4/tokenized/starcoderdata-12f018/
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[3] = gs://marin-eu-west4/raw/starcoderdata-720c8c
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[4] = gs://marin-eu-west4/tokenized/proofpile_2-4a35c7/
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[5] = gs://marin-eu-west4/raw/proof-pile-2-f1b1d8
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[6] = gs://marin-eu-west4/tokenized/paloma/4chan-496ad5
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[7] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[8] = gs://marin-eu-west4/tokenized/paloma/c4_100_domains-2b6db7
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[9] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[10] = gs://marin-eu-west4/tokenized/paloma/c4_en-cf1f79
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[11] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[12] = gs://marin-eu-west4/tokenized/paloma/dolma-v1_5-d3bed7
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[13] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[14] = gs://marin-eu-west4/tokenized/paloma/dolma_100_programing_languages-369132
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[15] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[16] = gs://marin-eu-west4/tokenized/paloma/dolma_100_subreddits-f25f70
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[17] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[18] = gs://marin-eu-west4/tokenized/paloma/falcon-refinedweb-75d43b
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[19] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[20] = gs://marin-eu-west4/tokenized/paloma/gab-ccaced
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[21] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[22] = gs://marin-eu-west4/tokenized/paloma/m2d2_s2orc_unsplit-7dbcc1
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[23] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[24] = gs://marin-eu-west4/tokenized/paloma/m2d2_wikipedia_unsplit-b33d23
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[25] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[26] = gs://marin-eu-west4/tokenized/paloma/manosphere_meta_sep-a07891
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[27] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[28] = gs://marin-eu-west4/tokenized/paloma/mc4-ea36a2
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[29] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[30] = gs://marin-eu-west4/tokenized/paloma/ptb-628036
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[31] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[32] = gs://marin-eu-west4/tokenized/paloma/redpajama-9d4ddd
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[33] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[34] = gs://marin-eu-west4/tokenized/paloma/twitterAAE_HELM_fixed-2e17c1
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[35] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[36] = gs://marin-eu-west4/tokenized/paloma/wikitext_103-1f5636
2025-08-07 07:23:23,489	INFO executor.py:651 --   DEP[37] = gs://marin-eu-west4/raw/paloma-fc6827
2025-08-07 07:23:23,492	INFO executor.py:658 --   pip_dependencies = ['multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'lm-eval@git+https://github.com/stanford-crfm/lm-evaluation-harness.git', 'tblib', 'sentencepiece', 'tiktoken', 'draccus>=0.11.5', 'google-api-python-client>=2.175.0', 'gcsfs', 'google-cloud-storage', 'google-cloud-storage-transfer', 'cryptography>=45', 's3fs>=2024', 'datasets==3.1.0', 'regex', 'requests', 'numpy', 'torch', 'braceexpand', 'deepdiff', 'tqdm', 'tqdm-loggable', 'toml', 'pandas', 'pyarrow', 'multiprocess==0.70.16', 'levanter>=1.2.dev1359', 'haliax>=1.4.dev348', 'sentencepiece', 'lz4', 'wandb<=0.19.9', 'openai']
2025-08-07 07:23:23,492	WARNING runtime_context.py:207 -- This method is only available when the process is a worker. Current mode: 0
2025-08-07 07:23:23,576	INFO executor.py:1004 -- Status checkpoints/sweep-130m-21B-mudamv1879fblr0.002-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train : SUCCESS.
2025-08-07 07:23:23,578	INFO executor.py:1028 -- Step checkpoints/sweep-130m-21B-mudamv1879fblr0.002-alr0.004-wd0.1-minlr0-warmup5: marin.optimizer_sweep.utils._failure_ok_train has already succeeded. Status: SUCCESS
[36m(run_levanter_train_lm pid=2616, ip=10.164.2.234)[0m fatal: not a git repository (or any parent up to mount point /)
[36m(run_levanter_train_lm pid=2616, ip=10.164.2.234)[0m Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
[36m(run_levanter_train_lm pid=2616, ip=10.164.2.234)[0m Could not infer git commit.
[36m(_doublecheck_paths pid=2802, ip=10.164.2.234)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.param_dtype. Skipping.
[36m(_doublecheck_paths pid=2802, ip=10.164.2.234)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.compute_dtype. Skipping.
[36m(_doublecheck_paths pid=2802, ip=10.164.2.234)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.output_dtype. Skipping.
[36m(_doublecheck_paths pid=2802, ip=10.164.2.234)[0m Found unexpected type <class 'pathlib.PosixPath'> at trainer.log_dir. Skipping.
[36m(run_levanter_train_lm pid=2601, ip=10.164.2.240)[0m fatal: not a git repository (or any parent up to mount point /)
[36m(run_levanter_train_lm pid=2601, ip=10.164.2.240)[0m Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
[36m(run_levanter_train_lm pid=2601, ip=10.164.2.240)[0m Could not infer git commit.
[36m(_doublecheck_paths pid=2802, ip=10.164.2.234)[0m Found unexpected type <class 'datetime.timedelta'> at trainer.checkpointer.save_interval. Skipping.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 0
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(_doublecheck_paths pid=2787, ip=10.164.2.240)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.param_dtype. Skipping.
[36m(_doublecheck_paths pid=2787, ip=10.164.2.240)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.compute_dtype. Skipping.
[36m(_doublecheck_paths pid=2787, ip=10.164.2.240)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.output_dtype. Skipping.
[36m(_doublecheck_paths pid=2787, ip=10.164.2.240)[0m Found unexpected type <class 'pathlib.PosixPath'> at trainer.log_dir. Skipping.
[36m(_doublecheck_paths pid=2787, ip=10.164.2.240)[0m Found unexpected type <class 'datetime.timedelta'> at trainer.checkpointer.save_interval. Skipping.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(run_levanter_train_lm pid=2321, ip=10.164.2.241)[0m fatal: not a git repository (or any parent up to mount point /)
[36m(run_levanter_train_lm pid=2321, ip=10.164.2.241)[0m Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
[36m(run_levanter_train_lm pid=2321, ip=10.164.2.241)[0m Could not infer git commit.
[36m(_doublecheck_paths pid=2507, ip=10.164.2.241)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.param_dtype. Skipping.
[36m(_doublecheck_paths pid=2507, ip=10.164.2.241)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.compute_dtype. Skipping.
[36m(_doublecheck_paths pid=2507, ip=10.164.2.241)[0m Found unexpected type <class 'jax._src.numpy.scalar_types._ScalarMeta'> at trainer.mp.output_dtype. Skipping.
[36m(_doublecheck_paths pid=2507, ip=10.164.2.241)[0m Found unexpected type <class 'pathlib.PosixPath'> at trainer.log_dir. Skipping.
[36m(_doublecheck_paths pid=2507, ip=10.164.2.241)[0m Found unexpected type <class 'datetime.timedelta'> at trainer.checkpointer.save_interval. Skipping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 0
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 0
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool members before adding members: []
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 0 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 10 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 19 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 22 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 25 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 12 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 4 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 28 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 26 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 17 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 18 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 16 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 14 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 30 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 5 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool member 1 started.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool members after adding members: ['0', '10', '19', '22', '25', '12', '4', '28', '26', '17', '18', '16', '14', '30', '5', '1']
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m slice ray-marin-eu-west4-worker-f722b08f-tpu actor pool scaled up to 16 members
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool member ray-marin-eu-west4-worker-f722b08f-tpu started.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool scaled up to 1 members
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(3c4f5bc2f7eca62effffffffffffffffffffffff0c00000001000000), ObjectRef(9d1d6c290deb4a2effffffffffffffffffffffff0c00000001000000), ObjectRef(c8c7016fabfa07d6ffffffffffffffffffffffff0c00000001000000), ObjectRef(3c57509fe925beacffffffffffffffffffffffff0c00000001000000), ObjectRef(a2b7999ad18b29f1ffffffffffffffffffffffff0c00000001000000), ObjectRef(caaddb55360c0428ffffffffffffffffffffffff0c00000001000000), ObjectRef(7e1a882bc2649bd7ffffffffffffffffffffffff0c00000001000000), ObjectRef(4ef91039ebd0c099ffffffffffffffffffffffff0c00000001000000), ObjectRef(9a9b3835f4510113ffffffffffffffffffffffff0c00000001000000), ObjectRef(1de498ec560d10a2ffffffffffffffffffffffff0c00000001000000), ObjectRef(9895c4818c0f90eeffffffffffffffffffffffff0c00000001000000), ObjectRef(0b310feed1b22363ffffffffffffffffffffffff0c00000001000000), ObjectRef(179b9f0aa14b7857ffffffffffffffffffffffff0c00000001000000), ObjectRef(3c6342c48db8a095ffffffffffffffffffffffff0c00000001000000), ObjectRef(c0d61eb78c827350ffffffffffffffffffffffff0c00000001000000), ObjectRef(32e65b61ceb20682ffffffffffffffffffffffff0c00000001000000)]
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 9ba3bcb5c227f8bb909df7670c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, a941309e4c4d63b5ae60129c0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '22', '14', '27', '6', '21', '31', '4', '15', '18', '28', '25', '5', '19', '16', '20']
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '22', '14', '27', '6', '21', '31', '4', '15', '18', '28', '25', '5', '19', '16', '20']
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=1414, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 1
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=1415, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=1415, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=1415, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=1415, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=1415, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=1415, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to gracefully shut down actor in 300 seconds; killing it instead: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 1
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 1216d530d41c3db4fff951ea0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '27', '26', '20', '18', '28', '9', '6', '5', '16', '29', '10', '21', '2', '22', '23']
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '27', '26', '20', '18', '28', '9', '6', '5', '16', '29', '10', '21', '2', '22', '23']
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=1947, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 2
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d5d675f2ce307f23ad89b4cf0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 started.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '23', '11', '5', '1', '16', '21', '7', '9', '18', '2', '29', '3', '22', '13', '25']
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '23', '11', '5', '1', '16', '21', '7', '9', '18', '2', '29', '3', '22', '13', '25']
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 stopping.
[36m(SliceActor pid=1948, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 2
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d0bba1840dd12b017d228b4f0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m 2025-08-07 08:01:40.966164: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m *** SIGABRT received at time=1754578900 on cpu 21 ***
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m PC: @     0x7f71a96049fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m     @     0x7f71a95b0520  (unknown)  (unknown)
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m [2025-08-07 08:01:40,971 E 2787 2787] logging.cc:496: *** SIGABRT received at time=1754578900 on cpu 21 ***
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m [2025-08-07 08:01:40,971 E 2787 2787] logging.cc:496: PC: @     0x7f71a96049fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m [2025-08-07 08:01:40,972 E 2787 2787] logging.cc:496:     @     0x7f71a95b0520  (unknown)  (unknown)
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m Stack (most recent call first):
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2787, ip=10.164.2.240)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, _cffi_backend, grpc._cython.cygrpc, sentencepiece._sentencepiece (total: 214)
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: be05d9c7cd3870ac512c49fb7e97785d9d2d23720c000000 Worker ID: 9f76d15d97a5de39a6e087ea172f32681ece5661caee2c9a14deebd8 Node ID: 967b41dc360253a0b674b58862d58d802645bb996a1eb0cfbb1cf87c Worker IP address: 10.164.2.240 Worker port: 10010 Worker PID: 2787 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m 2025-08-07 08:01:47.261780: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m *** SIGABRT received at time=1754578907 on cpu 62 ***
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m PC: @     0x7fd306b5a9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m     @     0x7fd306b06520  (unknown)  (unknown)
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m [2025-08-07 08:01:47,267 E 2708 2708] logging.cc:496: *** SIGABRT received at time=1754578907 on cpu 62 ***
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m [2025-08-07 08:01:47,267 E 2708 2708] logging.cc:496: PC: @     0x7fd306b5a9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m [2025-08-07 08:01:47,267 E 2708 2708] logging.cc:496:     @     0x7fd306b06520  (unknown)  (unknown)
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m Stack (most recent call first):
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2708, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m 2025-08-07 08:01:47.331597: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m *** SIGABRT received at time=1754578907 on cpu 91 ***
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m PC: @     0x7f94b2db89fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m     @     0x7f94b2d64520  (unknown)  (unknown)
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m [2025-08-07 08:01:47,337 E 2414 2414] logging.cc:496: *** SIGABRT received at time=1754578907 on cpu 91 ***
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m [2025-08-07 08:01:47,337 E 2414 2414] logging.cc:496: PC: @     0x7f94b2db89fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m [2025-08-07 08:01:47,337 E 2414 2414] logging.cc:496:     @     0x7f94b2d64520  (unknown)  (unknown)
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m Stack (most recent call first):
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2414, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2705, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2705, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2705, ip=10.164.2.229)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 1372bb24c9bfe81851aa7bf7e324e242242204840c000000 Worker ID: 37afb265881f24dd380a40c34baf02c6770e510b641a725929b9393f Node ID: 08b55f819516cb8d2b906c83fccbcc2d9275f51d8cb107727892c05d Worker IP address: 10.164.2.247 Worker port: 10009 Worker PID: 2708 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[36m(train_lm_task pid=2709, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.246)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b4f16083f119cc7fc46304c1326b3c9f8c96f8670c000000 Worker ID: 0bfecb358b648cbe549c6fd84743527c318af2f1bc0cc9676b5ab743 Node ID: 6389fb69c4e90234b014aaf6fe31ef93f9e9445b0da5c83e10274d7b Worker IP address: 10.164.2.229 Worker port: 10009 Worker PID: 2705 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m 
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 started.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '27', '5', '31', '20', '6', '30', '9', '19', '25', '16', '17', '2', '8', '26', '7']
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '27', '5', '31', '20', '6', '30', '9', '19', '25', '16', '17', '2', '8', '26', '7']
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 20x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m 2025-08-07 08:01:47.514544: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m *** SIGABRT received at time=1754578907 on cpu 34 ***[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m PC: @     0x7fd49a2bb9fc  (unknown)  pthread_kill[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m     @     0x7fd49a267520  (unknown)  (unknown)[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m [2025-08-07 08:01:47,520 E 2419 2419] logging.cc:496: *** SIGABRT received at time=1754578907 on cpu 34 ***[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m [2025-08-07 08:01:47,520 E 2419 2419] logging.cc:496: PC: @     0x7fd49a2bb9fc  (unknown)  pthread_kill[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m [2025-08-07 08:01:47,520 E 2419 2419] logging.cc:496:     @     0x7fd49a267520  (unknown)  (unknown)[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m Fatal Python error: Aborted[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m Stack (most recent call first):[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2419, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 3x across cluster][0m
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 3
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 stopping.
[36m(SliceActor pid=2480, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, e0fedbc85571afc4013cdc300c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 started.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '26', '27', '31', '14', '28', '20', '13', '19', '21', '1', '30', '17', '11', '5', '15']
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '26', '27', '31', '14', '28', '20', '13', '19', '21', '1', '30', '17', '11', '5', '15']
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 3
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=2482, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, acda047cb6f0ba3a1af439ed0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '15', '26', '28', '22', '12', '18', '25', '5', '21', '23', '24', '31', '14', '4', '1']
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '15', '26', '28', '22', '12', '18', '25', '5', '21', '23', '24', '31', '14', '4', '1']
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 4
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=3009, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 530a57ce16b2ee4ca1ff22a00c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '14', '5', '17', '9', '13', '24', '16', '7', '29', '21', '1', '22', '31', '2', '27']
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '14', '5', '17', '9', '13', '24', '16', '7', '29', '21', '1', '22', '31', '2', '27']
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 4
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=3011, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 87acd09f6dd20b9f99b710e20c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '6', '25', '18', '3', '9', '21', '26', '10', '20', '17', '15', '29', '23', '30', '4']
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '6', '25', '18', '3', '9', '21', '26', '10', '20', '17', '15', '29', '23', '30', '4']
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 5
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=3537, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d172f4051c90f7b6049ae86f0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 4 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '13', '26', '24', '11', '27', '17', '18', '3', '20', '21', '19', '29', '25', '4', '31']
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '13', '26', '24', '11', '27', '17', '18', '3', '20', '21', '19', '29', '25', '4', '31']
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 4 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 stopping.
[36m(SliceActor pid=3538, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 5
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m 2025-08-07 08:31:42.536923: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m *** SIGABRT received at time=1754580702 on cpu 91 ***
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m PC: @     0x7fe7d79459fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m     @     0x7fe7d78f1520  (unknown)  (unknown)
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m [2025-08-07 08:31:42,542 E 2420 2420] logging.cc:496: *** SIGABRT received at time=1754580702 on cpu 79 ***
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m [2025-08-07 08:31:42,542 E 2420 2420] logging.cc:496: PC: @     0x7fe7d79459fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m [2025-08-07 08:31:42,543 E 2420 2420] logging.cc:496:     @     0x7fe7d78f1520  (unknown)  (unknown)
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m Stack (most recent call first):
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2420, ip=10.164.2.232)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2418, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2418, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2418, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2418, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2418, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2418, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.234)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, _cffi_backend, grpc._cython.cygrpc, sentencepiece._sentencepiece (total: 214)
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m 2025-08-07 08:31:42.536424: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3337, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2709, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2709, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2709, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2709, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2708, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2708, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2413, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2413, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2706, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2706, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3042, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3042, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3042, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3042, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3042, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3042, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=2710, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2710, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2710, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2710, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2710, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2710, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2706, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2706, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2706, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b53cf39264101d8a7ae929dd1ea6d2433701c5360c000000 Worker ID: 50b5ad899efc5ae55791a3ec5c616326ea752e37bc4d961a4bd4cfb9 Node ID: da36cb6cbedbfe03fc90eff08240dc46730bfae44de057fb0dfe75a8 Worker IP address: 10.164.2.244 Worker port: 10007 Worker PID: 2415 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 4x across cluster][0m
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m 2025-08-07 08:31:42.536937: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2788, ip=10.164.2.240)[0m *** SIGABRT received at time=1754580702 on cpu 20 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m PC: @     0x7f4b846929fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m     @     0x7f4b8463e520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m [2025-08-07 08:31:42,544 E 2607 2607] logging.cc:496: *** SIGABRT received at time=1754580702 on cpu 11 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m [2025-08-07 08:31:42,544 E 2607 2607] logging.cc:496: PC: @     0x7f4b846929fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m [2025-08-07 08:31:42,544 E 2607 2607] logging.cc:496:     @     0x7f4b8463e520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 13x across cluster][0m
[36m(train_lm_task pid=2507, ip=10.164.2.241)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, _cffi_backend, grpc._cython.cygrpc, sentencepiece._sentencepiece (total: 214)
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, ef57de2e685907f223a0ebb50c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '6', '22', '31', '24', '18', '9', '16', '27', '13', '15', '17', '28', '2', '29', '1']
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '6', '22', '31', '24', '18', '9', '16', '27', '13', '15', '17', '28', '2', '29', '1']
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=4064, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 6
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, bb55a2a60b09ee1a6c9640850c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '22', '27', '21', '16', '5', '13', '31', '18', '25', '2', '8', '23', '14', '6', '26']
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '22', '27', '21', '16', '5', '13', '31', '18', '25', '2', '8', '23', '14', '6', '26']
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=4065, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 6
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 52c6c5ae426da32c7977a8c70c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '3', '2', '4', '20', '27', '25', '30', '21', '19', '22', '17', '9', '6', '18', '31']
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '3', '2', '4', '20', '27', '25', '30', '21', '19', '22', '17', '9', '6', '18', '31']
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=4592, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 7
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 5d885fe08a2f5f3d7b48444c0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '9', '11', '8', '18', '29', '23', '26', '17', '22', '6', '5', '16', '27', '14', '28']
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '9', '11', '8', '18', '29', '23', '26', '17', '22', '6', '5', '16', '27', '14', '28']
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 7
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=4593, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, f2230f8655552779246afa230c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m 2025-08-07 09:01:54.615937: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m *** SIGABRT received at time=1754582514 on cpu 26 ***
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m PC: @     0x7fe35973d9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m     @     0x7fe3596e9520  (unknown)  (unknown)
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m [2025-08-07 09:01:54,621 E 2508 2508] logging.cc:496: *** SIGABRT received at time=1754582514 on cpu 26 ***
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m [2025-08-07 09:01:54,621 E 2508 2508] logging.cc:496: PC: @     0x7fe35973d9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m [2025-08-07 09:01:54,621 E 2508 2508] logging.cc:496:     @     0x7fe3596e9520  (unknown)  (unknown)
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m Stack (most recent call first):
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2508, ip=10.164.2.241)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ea8dc5c3defae3a0f4ca852e319360085da064a00c000000 Worker ID: 6fe24aab2ec2f13cca44991da76a38ea0c08ce0ff681f57142db2575 Node ID: 4bd4174c1d81068d497c0e81b91868e70f0d29b3ec6e3c113b86d184 Worker IP address: 10.164.2.241 Worker port: 10008 Worker PID: 2508 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3043, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3043, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3043, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m 
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '30', '31', '6', '18', '17', '12', '24', '3', '4', '9', '5', '10', '26', '2', '22']
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '30', '31', '6', '18', '17', '12', '24', '3', '4', '9', '5', '10', '26', '2', '22']
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m 2025-08-07 09:01:55.418462: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m *** SIGABRT received at time=1754582515 on cpu 18 ***[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m PC: @     0x7fe2f2d459fc  (unknown)  pthread_kill[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m     @     0x7fe2f2cf1520  (unknown)  (unknown)[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m [2025-08-07 09:01:55,424 E 2709 2709] logging.cc:496: *** SIGABRT received at time=1754582515 on cpu 18 ***[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m [2025-08-07 09:01:55,424 E 2709 2709] logging.cc:496: PC: @     0x7fe2f2d459fc  (unknown)  pthread_kill[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m [2025-08-07 09:01:55,424 E 2709 2709] logging.cc:496:     @     0x7fe2f2cf1520  (unknown)  (unknown)[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m Fatal Python error: Aborted[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m Stack (most recent call first):[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 20x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=2709, ip=10.164.2.235)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 4x across cluster][0m
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 8
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=5119, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 3a8da05c7f5d1f9cec978b720c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 10 started.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '8', '1', '13', '27', '26', '22', '25', '11', '6', '20', '23', '14', '19', '9', '10']
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '8', '1', '13', '27', '26', '22', '25', '11', '6', '20', '23', '14', '19', '9', '10']
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 8
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 10 stopping.
[36m(SliceActor pid=5120, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 80b0958b4ce4c07874f4bf7b0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 7d0e12ec1419e4ac20790a7f0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.[32m [repeated 2x across cluster][0m
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '20', '30', '6', '21', '10', '29', '2', '27', '26', '13', '4', '19', '5', '14', '16']
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '20', '30', '6', '21', '10', '29', '2', '27', '26', '13', '4', '19', '5', '14', '16']
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.[32m [repeated 13x across cluster][0m
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=5646, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 9
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 12 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '8', '9', '21', '18', '20', '31', '16', '14', '6', '29', '22', '12', '2', '7', '1']
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '8', '9', '21', '18', '20', '31', '16', '14', '6', '29', '22', '12', '2', '7', '1']
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 12 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=5647, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 9
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 04d4fddfaefbe5f01c0a81330c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 06f4f61b9589a098056c7c2c0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '31', '9', '14', '1', '16', '17', '21', '6', '24', '15', '18', '20', '11', '22', '3']
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '31', '9', '14', '1', '16', '17', '21', '6', '24', '15', '18', '20', '11', '22', '3']
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 stopping.
[36m(SliceActor pid=6174, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 10
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '3', '4', '14', '28', '6', '31', '2', '13', '12', '26', '5', '9', '19', '22', '18']
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '3', '4', '14', '28', '6', '31', '2', '13', '12', '26', '5', '9', '19', '22', '18']
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=6173, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 10
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m 2025-08-07 09:31:49.868048: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m *** SIGABRT received at time=1754584309 on cpu 31 ***
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m PC: @     0x7f80946b49fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m     @     0x7f8094660520  (unknown)  (unknown)
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m [2025-08-07 09:31:49,874 E 3236 3236] logging.cc:496: *** SIGABRT received at time=1754584309 on cpu 94 ***
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m [2025-08-07 09:31:49,874 E 3236 3236] logging.cc:496: PC: @     0x7f24ef15e9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m [2025-08-07 09:31:49,874 E 3236 3236] logging.cc:496:     @     0x7f24ef10a520  (unknown)  (unknown)
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m Stack (most recent call first):
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, 
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m scipy.integrate._quadpack
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m , scipy.integrate._vode
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing
[36m(train_lm_task pid=3236, ip=10.164.2.236)[0m , scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m , pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m , 
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m scipy.optimize._minpack
[36m(train_lm_task pid=2803, ip=10.164.2.234)[0m , scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2419, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2419, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2419, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2419, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2419, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2419, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3165, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=3165, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=3165, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3165, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3165, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3165, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2903, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2903, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2607, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2607, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2708, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2708, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2708, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m 2025-08-07 09:31:49.867554: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3338, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=2707, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2707, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2707, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2707, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2707, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2707, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2902, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2902, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2902, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2902, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2609, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2609, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2613, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2613, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2613, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2613, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2613, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2613, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2d513ecf213a589da9e24164611a19b934cd9def0c000000 Worker ID: 1632142ce815507c0f98345bc483c9cfcd0ae8cc349b870b8ab46abc Node ID: 9f5d9292f00b4b33e1b11b3820a942102773ccd980e91b96635bd4b8 Worker IP address: 10.164.2.232 Worker port: 10008 Worker PID: 2613 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 5x across cluster][0m
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m 2025-08-07 09:31:49.869696: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2899, ip=10.164.2.229)[0m *** SIGABRT received at time=1754584309 on cpu 14 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m PC: @     0x7f05e34559fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m     @     0x7f05e3401520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m [2025-08-07 09:31:49,873 E 2885 2885] logging.cc:496: *** SIGABRT received at time=1754584309 on cpu 31 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m [2025-08-07 09:31:49,873 E 2885 2885] logging.cc:496: PC: @     0x7f05e34559fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m [2025-08-07 09:31:49,873 E 2885 2885] logging.cc:496:     @     0x7f05e3401520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2885, ip=10.164.2.241)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 13x across cluster][0m
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 3df48ee3324db104c9c6f3f50c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d14db536a52a504dfc9e314a0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 10 started.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '27', '13', '8', '20', '3', '17', '22', '9', '16', '11', '2', '14', '28', '19', '10']
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '27', '13', '8', '20', '3', '17', '22', '9', '16', '11', '2', '14', '28', '19', '10']
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 11
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 10 stopping.
[36m(SliceActor pid=6701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '24', '15', '6', '10', '25', '21', '9', '14', '28', '23', '18', '29', '4', '11', '3']
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '24', '15', '6', '10', '25', '21', '9', '14', '28', '23', '18', '29', '4', '11', '3']
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 11
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=6700, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, edf870339e64c3c54080b4cd0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 7bcf3eb4397a95d77593fb920c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '14', '5', '16', '24', '3', '21', '30', '6', '26', '22', '7', '2', '9', '17', '29']
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '14', '5', '16', '24', '3', '21', '30', '6', '26', '22', '7', '2', '9', '17', '29']
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=7228, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 12
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.[32m [repeated 3x across cluster][0m
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.[32m [repeated 7x across cluster][0m
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '31', '17', '28', '2', '9', '4', '12', '29', '22', '26', '6', '10', '14', '24', '16']
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '31', '17', '28', '2', '9', '4', '12', '29', '22', '26', '6', '10', '14', '24', '16']
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.[32m [repeated 5x across cluster][0m
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=7227, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 12
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, b697b0dff2b2c0c8071bc2cc0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, f4fb11c7a49f93dcfb5f6a5e0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '14', '9', '26', '29', '8', '22', '27', '18', '28', '11', '23', '24', '15', '19', '1']
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '14', '9', '26', '29', '8', '22', '27', '18', '28', '11', '23', '24', '15', '19', '1']
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 13
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=7756, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m 2025-08-07 10:02:01.898192: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m *** SIGABRT received at time=1754586121 on cpu 19 ***
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m PC: @     0x7fa13eabf9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m     @     0x7fa13ea6b520  (unknown)  (unknown)
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m [2025-08-07 10:02:01,903 E 2903 2903] logging.cc:496: *** SIGABRT received at time=1754586121 on cpu 19 ***
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m [2025-08-07 10:02:01,903 E 2903 2903] logging.cc:496: PC: @     0x7fa13eabf9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m [2025-08-07 10:02:01,904 E 2903 2903] logging.cc:496:     @     0x7fa13ea6b520  (unknown)  (unknown)
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m Stack (most recent call first):
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.235)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2903, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: d0333e47fb75549dc44bb0083e5e00c5a07f8f340c000000 Worker ID: 60e652421ebbd262589b5008406f3b976d69f8c6065bd781ca3a0f83 Node ID: cce3a85c8e79e04518e87f6ed7483bad5b61893977114ecb29d98838 Worker IP address: 10.164.2.235 Worker port: 10012 Worker PID: 2903 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Worker crashed
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Preempted 1 times. Continuing to retry.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 1
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(4149a998bd78f27fffffffffffffffffffffffff0c00000001000000), ObjectRef(eaad1ff6565c7595ffffffffffffffffffffffff0c00000001000000), ObjectRef(9302a8833c8898a6ffffffffffffffffffffffff0c00000001000000), ObjectRef(49b306d9dd0addddffffffffffffffffffffffff0c00000001000000), ObjectRef(ebceb37ac5e496f9ffffffffffffffffffffffff0c00000001000000), ObjectRef(285073f51776ce83ffffffffffffffffffffffff0c00000001000000), ObjectRef(e38ea6f0298b0186ffffffffffffffffffffffff0c00000001000000), ObjectRef(855ba4921fe357edffffffffffffffffffffffff0c00000001000000), ObjectRef(0094cc9193f0de70ffffffffffffffffffffffff0c00000001000000), ObjectRef(e1a0954d8b480868ffffffffffffffffffffffff0c00000001000000), ObjectRef(1e71f147d7ef2919ffffffffffffffffffffffff0c00000001000000), ObjectRef(1f04bbef939c077dffffffffffffffffffffffff0c00000001000000), ObjectRef(b1630641c43c5d8bffffffffffffffffffffffff0c00000001000000), ObjectRef(37f232475083b769ffffffffffffffffffffffff0c00000001000000), ObjectRef(309f319951fb0a11ffffffffffffffffffffffff0c00000001000000), ObjectRef(286d8c3d3628b2d4ffffffffffffffffffffffff0c00000001000000)]
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m 2025-08-07 10:02:01.974804: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m *** SIGABRT received at time=1754586121 on cpu 91 ***[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m PC: @     0x7fb4ad3529fc  (unknown)  pthread_kill[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m     @     0x7fb4ad2fe520  (unknown)  (unknown)[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m [2025-08-07 10:02:01,980 E 2610 2610] logging.cc:496: *** SIGABRT received at time=1754586121 on cpu 91 ***[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m [2025-08-07 10:02:01,980 E 2610 2610] logging.cc:496: PC: @     0x7fb4ad3529fc  (unknown)  pthread_kill[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m [2025-08-07 10:02:01,980 E 2610 2610] logging.cc:496:     @     0x7fb4ad2fe520  (unknown)  (unknown)[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m Fatal Python error: Aborted[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m Stack (most recent call first):[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=2610, ip=10.164.2.244)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 2x across cluster][0m
[36m(TPUHostActor pid=2780, ip=10.164.2.241)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 35x across cluster][0m
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '14', '24', '5', '22', '2', '21', '4', '6', '17', '10', '15', '12', '20', '25', '28']
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '14', '24', '5', '22', '2', '21', '4', '6', '17', '10', '15', '12', '20', '25', '28']
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=7754, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 13
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, cca8da199ce235492f31330e0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '1', '22', '24', '9', '14', '5', '8', '7', '27', '11', '26', '23', '28', '16', '6']
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '1', '22', '24', '9', '14', '5', '8', '7', '27', '11', '26', '23', '28', '16', '6']
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.[32m [repeated 14x across cluster][0m
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=8283, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 14
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 248d3b1b1248a9c2d41a8c1e0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '6', '25', '18', '20', '10', '12', '15', '28', '14', '27', '30', '23', '31', '22', '5']
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '6', '25', '18', '20', '10', '12', '15', '28', '14', '27', '30', '23', '31', '22', '5']
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=8281, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 14
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 5c953c96c7ae946f6031ee850c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '14', '1', '18', '29', '11', '22', '26', '7', '19', '25', '9', '30', '23', '21', '28']
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '14', '1', '18', '29', '11', '22', '26', '7', '19', '25', '9', '30', '23', '21', '28']
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 22 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 9 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(SliceActor pid=8810, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 15
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, a7f8888318f930f5350cbb9f0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '3', '10', '21', '15', '30', '2', '26', '20', '4', '25', '12', '11', '19', '1', '28']
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '3', '10', '21', '15', '30', '2', '26', '20', '4', '25', '12', '11', '19', '1', '28']
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 15
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=8808, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m 2025-08-07 10:32:13.807476: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m *** SIGABRT received at time=1754587933 on cpu 43 ***
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m PC: @     0x7f2cb189e9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m     @     0x7f2cb184a520  (unknown)  (unknown)
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m [2025-08-07 10:32:13,813 E 2902 2902] logging.cc:496: *** SIGABRT received at time=1754587933 on cpu 43 ***
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m [2025-08-07 10:32:13,813 E 2902 2902] logging.cc:496: PC: @     0x7f2cb189e9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m [2025-08-07 10:32:13,813 E 2902 2902] logging.cc:496:     @     0x7f2cb184a520  (unknown)  (unknown)
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m Stack (most recent call first):
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2902, ip=10.164.2.237)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3181, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3181, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3181, ip=10.164.2.234)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 7bd8a65df50b5c716fa0d891a56dd499cefebe6f0c000000 Worker ID: 7b6cdf27509933a474de646f873e3114de382f25efc67075a01bb90c Node ID: 8e996b98d1bc020137396778c885e0a7a156395a716626ceeeacac3f Worker IP address: 10.164.2.237 Worker port: 10012 Worker PID: 2902 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 3x across cluster][0m
[36m(train_lm_task pid=2613, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2613, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2613, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3080, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3080, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3080, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3431, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3431, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3431, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3359, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3359, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3359, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m 
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 5be70a2b0b81783f8acec0ca0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m 2025-08-07 10:32:15.040958: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m *** SIGABRT received at time=1754587935 on cpu 67 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m PC: @     0x7f8d342489fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m     @     0x7f8d341f4520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m [2025-08-07 10:32:15,046 E 2801 2801] logging.cc:496: *** SIGABRT received at time=1754587935 on cpu 67 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m [2025-08-07 10:32:15,046 E 2801 2801] logging.cc:496: PC: @     0x7f8d342489fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m [2025-08-07 10:32:15,046 E 2801 2801] logging.cc:496:     @     0x7f8d341f4520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.242)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 8x across cluster][0m
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, f506403426d268e0162333210c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '27', '18', '14', '26', '29', '2', '12', '6', '21', '9', '10', '13', '17', '28', '20']
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '27', '18', '14', '26', '29', '2', '12', '6', '21', '9', '10', '13', '17', '28', '20']
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 16
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=9336, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 21 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 26 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 31 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 10 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 4 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '13', '14', '21', '24', '26', '8', '29', '19', '5', '3', '31', '28', '10', '4', '27']
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '13', '14', '21', '24', '26', '8', '29', '19', '5', '3', '31', '28', '10', '4', '27']
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.[32m [repeated 2x across cluster][0m
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 16
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=9646, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.[32m [repeated 14x across cluster][0m
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, ccfc374c1d1a7b6cef5220110c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 5c1c11bf655016b862befb600c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 started.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '31', '22', '26', '17', '4', '5', '21', '15', '12', '10', '27', '7', '14', '24', '11']
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '31', '22', '26', '17', '4', '5', '21', '15', '12', '10', '27', '7', '14', '24', '11']
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 17
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=9863, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 4 started.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '16', '30', '11', '20', '14', '5', '8', '29', '2', '1', '7', '28', '24', '23', '4']
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '16', '30', '11', '20', '14', '5', '8', '29', '2', '1', '7', '28', '24', '23', '4']
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 11 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 14 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 5 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 8 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 2 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 1 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 28 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 24 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 23 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 4 stopping.
[36m(SliceActor pid=10173, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 17
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 05d425bdc787a6f9850a5b700c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before adding members: []
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 started.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.[32m [repeated 2x across cluster][0m
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.[32m [repeated 3x across cluster][0m
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.[32m [repeated 8x across cluster][0m
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['28', '0', '26', '21', '29', '20', '9', '31', '24', '19', '22', '25', '30', '11', '12', '6']
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['28', '0', '26', '21', '29', '20', '9', '31', '24', '19', '22', '25', '30', '11', '12', '6']
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.[32m [repeated 3x across cluster][0m
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=10390, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 18
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 576c0fc0d19a16e8916aba7c0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m 2025-08-07 11:02:09.030978: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m *** SIGABRT received at time=1754589729 on cpu 29 ***
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m PC: @     0x7f488580b9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m     @     0x7f48857b7520  (unknown)  (unknown)
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m [2025-08-07 11:02:09,037 E 2804 2804] logging.cc:496: *** SIGABRT received at time=1754589729 on cpu 92 ***
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m [2025-08-07 11:02:09,037 E 2804 2804] logging.cc:496: PC: @     0x7f488580b9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m [2025-08-07 11:02:09,037 E 2804 2804] logging.cc:496:     @     0x7f48857b7520  (unknown)  (unknown)
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m Stack (most recent call first):
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.244)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2802, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=2802, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2802, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3375, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3375, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3375, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3375, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3375, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3375, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3097, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3097, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3096, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3096, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=2807, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2807, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=2996, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2996, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3432, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3432, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3432, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3432, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3432, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3432, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=2807, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2807, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2807, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3097, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3097, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m 2025-08-07 11:02:09.030553: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3544, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3095, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3095, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3095, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3095, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3095, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3095, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3093, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3093, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3093, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3093, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3093, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3093, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=2903, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2903, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=2903, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 3d3b69ba31df0b50f3cb23724de87bebba5a0f690c000000 Worker ID: db8a98aad52d0304495d530d849eb79bb9e979020287e39157e83600 Node ID: 6389fb69c4e90234b014aaf6fe31ef93f9e9445b0da5c83e10274d7b Worker IP address: 10.164.2.229 Worker port: 10013 Worker PID: 3093 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 started.
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m 2025-08-07 11:02:09.031070: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3360, ip=10.164.2.240)[0m *** SIGABRT received at time=1754589729 on cpu 68 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m PC: @     0x7f9cc1a3c9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m     @     0x7f9cc19e8520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m [2025-08-07 11:02:09,036 E 3079 3079] logging.cc:496: *** SIGABRT received at time=1754589729 on cpu 29 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m [2025-08-07 11:02:09,036 E 3079 3079] logging.cc:496: PC: @     0x7f9cc1a3c9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m [2025-08-07 11:02:09,036 E 3079 3079] logging.cc:496:     @     0x7f9cc19e8520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3079, ip=10.164.2.241)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 12 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 started.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members after adding members: ['0', '25', '19', '7', '3', '18', '30', '13', '17', '20', '16', '12', '27', '15', '29', '6']
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool members before draining: ['0', '25', '19', '7', '3', '18', '30', '13', '17', '20', '16', '12', '27', '15', '29', '6']
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 0 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 25 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 19 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 7 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 3 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 18 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 30 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 13 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 17 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 20 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 16 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 12 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 27 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 15 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 29 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool member 6 stopping.
[36m(SliceActor pid=10701, ip=10.164.15.213)[0m slice ray-marin-eu-west4-worker-92cb74c5-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 18
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool members before adding members: []
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 0 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 22 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 6 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 12 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 31 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 20 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 25 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 4 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 8 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 14 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 28 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 24 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 15 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 5 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 19 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool member 16 started.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool members after adding members: ['0', '22', '6', '12', '31', '20', '25', '4', '8', '14', '28', '24', '15', '5', '19', '16']
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m slice ray-marin-eu-west4-worker-eb03a5e7-tpu actor pool scaled up to 16 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool member ray-marin-eu-west4-worker-eb03a5e7-tpu started.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after adding members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool scaled up to 1 members
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Futures for slice ray-marin-eu-west4-worker-eb03a5e7-tpu: [ObjectRef(b3bf8e078f29971dffffffffffffffffffffffff0c00000001000000), ObjectRef(c6f745cb1c55acedffffffffffffffffffffffff0c00000001000000), ObjectRef(adc055c1e926fa10ffffffffffffffffffffffff0c00000001000000), ObjectRef(79cb47f2c26cb996ffffffffffffffffffffffff0c00000001000000), ObjectRef(02adeec68823935fffffffffffffffffffffffff0c00000001000000), ObjectRef(bb25ef5795d7bcdaffffffffffffffffffffffff0c00000001000000), ObjectRef(c8adaae5251d4714ffffffffffffffffffffffff0c00000001000000), ObjectRef(dd17696d002ee8a0ffffffffffffffffffffffff0c00000001000000), ObjectRef(9386fbd2c33651ffffffffffffffffffffffffff0c00000001000000), ObjectRef(da4a220c6872105bffffffffffffffffffffffff0c00000001000000), ObjectRef(4219cf8e0db745b5ffffffffffffffffffffffff0c00000001000000), ObjectRef(e7ec993a4b8788a8ffffffffffffffffffffffff0c00000001000000), ObjectRef(8c9d5af9a6b9571bffffffffffffffffffffffff0c00000001000000), ObjectRef(aa6d6d9dcc6cff4effffffffffffffffffffffff0c00000001000000), ObjectRef(e23fe791086b101affffffffffffffffffffffff0c00000001000000), ObjectRef(b0f6f7150116fe99ffffffffffffffffffffffff0c00000001000000)]
[33m(raylet)[0m The node with node id: 29b335ed879a89139fcca51e8bbef2eba0ab4879a238f4f0701108bc and address: 10.164.15.209 and node name: 10.164.15.209 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a 	(1) raylet crashes unexpectedly (OOM, etc.) 
	(2) raylet has lagging heartbeats due to slow network or busy workload.
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 30c630bb0e0b74ae85e1b44a7216c25aa7b24bec0c000000 Worker ID: 039d8d887cc8b699977770d18f74919dd1ba1dc3d1afdbb2234954b3 Node ID: cc8d81d3c80c107f9f031b95ab83677fcb0be2b48c44623b2e5a307e Worker IP address: 10.164.2.234 Worker port: 10016 Worker PID: 3375 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[33m(raylet)[0m The node with node id: 9e8395a9020bb36dcdd153a2f7b3d0ca61247707f136fb926252a0c5 and address: 10.164.15.212 and node name: 10.164.15.212 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a 	(1) raylet crashes unexpectedly (OOM, etc.) 
	(2) raylet has lagging heartbeats due to slow network or busy workload.[32m [repeated 28x across cluster][0m
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 4b714507547cfa8c0b8ad2fc0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '3', '24', '14', '2', '29', '4', '12', '6', '25', '22', '28', '7', '26', '10', '20']
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '3', '24', '14', '2', '29', '4', '12', '6', '25', '22', '28', '7', '26', '10', '20']
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=10917, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 19
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 2c77d2817e57cc9e02adbe780c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '3', '21', '15', '29', '18', '6', '30', '16', '12', '28', '2', '19', '10', '23', '26']
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '3', '21', '15', '29', '18', '6', '30', '16', '12', '28', '2', '19', '10', '23', '26']
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 20
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=11444, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m 2025-08-07 11:32:21.032629: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m *** SIGABRT received at time=1754591541 on cpu 43 ***
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m PC: @     0x7f1af7cdd9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m     @     0x7f1af7c89520  (unknown)  (unknown)
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m [2025-08-07 11:32:21,038 E 3096 3096] logging.cc:496: *** SIGABRT received at time=1754591541 on cpu 43 ***
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m [2025-08-07 11:32:21,038 E 3096 3096] logging.cc:496: PC: @     0x7f1af7cdd9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m [2025-08-07 11:32:21,038 E 3096 3096] logging.cc:496:     @     0x7f1af7c89520  (unknown)  (unknown)
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m Stack (most recent call first):
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.237)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 843defd9cfe9fb7ea52736be865239fb132786f00c000000 Worker ID: 02a6c343eed1c5ddd6ac459e65dd7df40cecbc84aab314a061ec9aba Node ID: 8e996b98d1bc020137396778c885e0a7a156395a716626ceeeacac3f Worker IP address: 10.164.2.237 Worker port: 10014 Worker PID: 3096 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[33m(raylet)[0m The node with node id: 1fd21e5aa797d719c90d4692a1dd8608f052e2730a0c9f113f8038e2 and address: 10.164.15.223 and node name: 10.164.15.223 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a 	(1) raylet crashes unexpectedly (OOM, etc.) 
	(2) raylet has lagging heartbeats due to slow network or busy workload.[32m [repeated 2x across cluster][0m
[36m(train_lm_task pid=3376, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3376, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3376, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3098, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3098, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3098, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3094, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3094, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3094, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3096, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m 
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m 2025-08-07 11:32:22.631749: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m *** SIGABRT received at time=1754591542 on cpu 6 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m PC: @     0x7fc23b43a9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m     @     0x7fc23b3e6520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m [2025-08-07 11:32:22,637 E 3291 3291] logging.cc:496: *** SIGABRT received at time=1754591542 on cpu 6 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m [2025-08-07 11:32:22,637 E 3291 3291] logging.cc:496: PC: @     0x7fc23b43a9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m [2025-08-07 11:32:22,637 E 3291 3291] logging.cc:496:     @     0x7fc23b3e6520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.247)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 8x across cluster][0m
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 2ff0367113315c0c47f100ea0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m 2025-08-07 11:37:09.008105: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m *** SIGABRT received at time=1754591829 on cpu 15 ***
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m PC: @     0x7f17e18059fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m     @     0x7f17e17b1520  (unknown)  (unknown)
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m [2025-08-07 11:37:09,014 E 1507 1507] logging.cc:496: *** SIGABRT received at time=1754591829 on cpu 15 ***
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m [2025-08-07 11:37:09,014 E 1507 1507] logging.cc:496: PC: @     0x7f17e18059fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m [2025-08-07 11:37:09,014 E 1507 1507] logging.cc:496:     @     0x7f17e17b1520  (unknown)  (unknown)
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m Stack (most recent call first):
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1507, ip=10.164.2.214)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1509, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.91)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 7b4b25fb7a67f48bc1b308c5ece647b9e329af220c000000 Worker ID: 36e12cf3a34d38861310c29056ab1d1cb6a111b75547583318189bb9 Node ID: 53602fe43e44b29c60add32b8a4fc6873fd58510d78b4a2479aabf5d Worker IP address: 10.164.2.214 Worker port: 10003 Worker PID: 1507 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1508, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1508, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1508, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m 
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m 2025-08-07 11:37:09.499816: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m *** SIGABRT received at time=1754591829 on cpu 10 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m PC: @     0x7f3589bd29fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m     @     0x7f3589b7e520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m [2025-08-07 11:37:09,505 E 1509 1509] logging.cc:496: *** SIGABRT received at time=1754591829 on cpu 10 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m [2025-08-07 11:37:09,505 E 1509 1509] logging.cc:496: PC: @     0x7f3589bd29fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m [2025-08-07 11:37:09,505 E 1509 1509] logging.cc:496:     @     0x7f3589b7e520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m Fatal Python error: Aborted[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m Stack (most recent call first):[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 30x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.213)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '24', '27', '20', '22', '12', '19', '14', '29', '23', '28', '6', '17', '26', '31', '18']
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '24', '27', '20', '22', '12', '19', '14', '29', '23', '28', '6', '17', '26', '31', '18']
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 21
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=11971, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 8bf7962e50b5d1d92864d2060c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '2', '25', '28', '15', '31', '9', '17', '12', '14', '27', '22', '30', '18', '1', '20']
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '2', '25', '28', '15', '31', '9', '17', '12', '14', '27', '22', '30', '18', '1', '20']
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=12498, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 22
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, f35e17a0a02ac462352a68760c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m 2025-08-07 12:02:17.825284: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m *** SIGABRT received at time=1754593337 on cpu 19 ***
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m PC: @     0x7f3bcbd5a9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m     @     0x7f3bcbd06520  (unknown)  (unknown)
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m [2025-08-07 12:02:17,831 E 3001 3001] logging.cc:496: *** SIGABRT received at time=1754593337 on cpu 19 ***
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m [2025-08-07 12:02:17,831 E 3001 3001] logging.cc:496: PC: @     0x7f3bcbd5a9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m [2025-08-07 12:02:17,831 E 3001 3001] logging.cc:496:     @     0x7f3bcbd06520  (unknown)  (unknown)
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m Stack (most recent call first):
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m , _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m , scipy.sparse.linalg._propack._dpropack
[36m(train_lm_task pid=3001, ip=10.164.2.232)[0m , scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3569, ip=10.164.2.234)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3292, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3292, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3292, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3292, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3292, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3292, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3625, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3625, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3625, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3625, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3625, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3625, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3289, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3289, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2995, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2995, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2808, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2808, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2808, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=2998, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2998, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2998, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2998, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2998, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2998, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2997, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2997, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=2997, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m 2025-08-07 12:02:17.824616: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3273, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3273, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3273, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3273, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3273, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3273, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3553, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3553, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3553, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3553, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3553, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3553, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3290, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3290, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3291, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3291, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3097, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3097, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3097, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: f7c83885114979b770b65bcffb17126abfa3cf8a0c000000 Worker ID: 639a4401f2ab172c1e75e56d66c5d3fee4e28ba90a734af861deb3d1 Node ID: 967b41dc360253a0b674b58862d58d802645bb996a1eb0cfbb1cf87c Worker IP address: 10.164.2.240 Worker port: 10018 Worker PID: 3553 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 7x across cluster][0m
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 97f84a5c4c04ef99419e41b354a2d4ad6ead1c630c000000 Worker ID: 4c6dde20953ee2df262e64e124dd902d35e236adf56f2fe91c6f7aad Node ID: a16e49065a6045ec215812b52ffd813d98570c8a83fcb233b99911cc Worker IP address: 10.164.2.253 Worker port: 10014 Worker PID: 3097 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: recvmsg:Connection reset by peer; RPC Error details: 
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Worker crashed
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Preempted 2 times. Continuing to retry.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 2
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(ae9f75df13f2fd7effffffffffffffffffffffff0c00000001000000), ObjectRef(7d2526d68cb2d3ebffffffffffffffffffffffff0c00000001000000), ObjectRef(645c3b238aade886ffffffffffffffffffffffff0c00000001000000), ObjectRef(aa9ea53f9e35a7a2ffffffffffffffffffffffff0c00000001000000), ObjectRef(d7f56d02cb19ae16ffffffffffffffffffffffff0c00000001000000), ObjectRef(94bb6e95b90ef3abffffffffffffffffffffffff0c00000001000000), ObjectRef(61f4ea94551c8f5affffffffffffffffffffffff0c00000001000000), ObjectRef(2a8ee277f3db9b97ffffffffffffffffffffffff0c00000001000000), ObjectRef(738db05dd14e42e9ffffffffffffffffffffffff0c00000001000000), ObjectRef(44b6e838e5759d04ffffffffffffffffffffffff0c00000001000000), ObjectRef(64aaa6945a0c49faffffffffffffffffffffffff0c00000001000000), ObjectRef(79a50c7fb2e6c0d1ffffffffffffffffffffffff0c00000001000000), ObjectRef(98a717ce867a7fafffffffffffffffffffffffff0c00000001000000), ObjectRef(d5839abc5ae5c29fffffffffffffffffffffffff0c00000001000000), ObjectRef(450f32906aeb2fecffffffffffffffffffffffff0c00000001000000), ObjectRef(a951839c113afebbffffffffffffffffffffffff0c00000001000000)]
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 started.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '2', '4', '24', '6', '10', '15', '29', '31', '20', '30', '17', '23', '28', '1', '7']
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '2', '4', '24', '6', '10', '15', '29', '31', '20', '30', '17', '23', '28', '1', '7']
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m 2025-08-07 12:02:17.825135: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m *** SIGABRT received at time=1754593337 on cpu 15 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m PC: @     0x7f34923829fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m     @     0x7f349232e520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m [2025-08-07 12:02:17,830 E 3288 3288] logging.cc:496: *** SIGABRT received at time=1754593337 on cpu 15 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m [2025-08-07 12:02:17,830 E 3288 3288] logging.cc:496: PC: @     0x7f34923829fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m [2025-08-07 12:02:17,831 E 3288 3288] logging.cc:496:     @     0x7f349232e520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3288, ip=10.164.2.229)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 14x across cluster][0m
[36m(TPUHostActor pid=3075, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 21x across cluster][0m
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 stopping.
[36m(SliceActor pid=13025, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 23
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, 
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m pandas._libs.tslibs.timezones
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m , 
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m pandas._libs.tslibs.tzconversion
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m 2025-08-07 12:07:04.291657: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m *** SIGABRT received at time=1754593624 on cpu 10 ***
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m PC: @     0x7f8c1c6a39fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m     @     0x7f8c1c64f520  (unknown)  (unknown)
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m [2025-08-07 12:07:04,297 E 1510 1510] logging.cc:496: *** SIGABRT received at time=1754593624 on cpu 10 ***
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m [2025-08-07 12:07:04,297 E 1510 1510] logging.cc:496: PC: @     0x7f8c1c6a39fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m [2025-08-07 12:07:04,297 E 1510 1510] logging.cc:496:     @     0x7f8c1c64f520  (unknown)  (unknown)
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m Stack (most recent call first):
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 5x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m , pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m 2025-08-07 12:07:04.292786: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m *** SIGABRT received at time=1754593624 on cpu 82 ***
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m PC: @     0x7fb49fb3f9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m     @     0x7fb49faeb520  (unknown)  (unknown)
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m [2025-08-07 12:07:04,298 E 1512 1512] logging.cc:496: *** SIGABRT received at time=1754593624 on cpu 82 ***
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m [2025-08-07 12:07:04,298 E 1512 1512] logging.cc:496: PC: @     0x7fb49fb3f9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m [2025-08-07 12:07:04,298 E 1512 1512] logging.cc:496:     @     0x7fb49faeb520  (unknown)  (unknown)
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m Stack (most recent call first):
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m , _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m scipy.sparse._csparsetools
[36m(train_lm_task pid=1510, ip=10.164.2.213)[0m , scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m , scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, 
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m sklearn._cyutility
[36m(train_lm_task pid=1509, ip=10.164.2.93)[0m , torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m , sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.134)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1510, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1510, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m 2025-08-07 12:07:04.290793: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1603, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1509, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1509, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1509, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1509, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1509, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1509, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1508, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1508, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1510, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1510, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1510, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1510, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1508, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1508, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1508, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1508, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1508, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1508, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1510, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1508, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1508, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1508, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 0a8b2224f22c186f24f204679e93e72a5738cd870c000000 Worker ID: 9e0928149fde32f836e3af82222514e5ce6c09560a389961db801186 Node ID: 8e123ff8c4c6962833452c673197977fa32e847df5e5d63c617f6859 Worker IP address: 10.164.2.91 Worker port: 10004 Worker PID: 1510 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=1512, ip=10.164.1.96)[0m , 
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m 2025-08-07 12:07:04.293195: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m *** SIGABRT received at time=1754593624 on cpu 96 ***[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m PC: @     0x7f7c6fa839fc  (unknown)  pthread_kill[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m     @     0x7f7c6fa2f520  (unknown)  (unknown)[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m [2025-08-07 12:07:04,299 E 1512 1512] logging.cc:496: *** SIGABRT received at time=1754593624 on cpu 96 ***[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m [2025-08-07 12:07:04,299 E 1512 1512] logging.cc:496: PC: @     0x7f7c6fa839fc  (unknown)  pthread_kill[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m [2025-08-07 12:07:04,299 E 1512 1512] logging.cc:496:     @     0x7f7c6fa2f520  (unknown)  (unknown)[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m Fatal Python error: Aborted[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m Stack (most recent call first):[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=1512, ip=10.164.2.215)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 12x across cluster][0m
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, b694c88db789d2fe6159cc170c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '25', '27', '4', '21', '2', '14', '15', '17', '24', '31', '10', '9', '26', '22', '19']
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '25', '27', '4', '21', '2', '14', '15', '17', '24', '31', '10', '9', '26', '22', '19']
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=13552, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 24
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d220dd6b1fbc872fe68212000c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '27', '10', '21', '31', '29', '14', '6', '9', '22', '15', '3', '4', '26', '20', '18']
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '27', '10', '21', '31', '29', '14', '6', '9', '22', '15', '3', '4', '26', '20', '18']
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=14079, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 25
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=3570, ip=10.164.2.234)[0m 2025-08-07 12:32:29.704702: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3570, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3570, ip=10.164.2.234)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3570, ip=10.164.2.234)[0m *** SIGABRT received at time=1754595149 on cpu 79 ***
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m PC: @     0x7f03b9dcc9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m     @     0x7f03b9d78520  (unknown)  (unknown)
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m [2025-08-07 12:32:29,725 E 3291 3291] logging.cc:496: *** SIGABRT received at time=1754595149 on cpu 18 ***
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m [2025-08-07 12:32:29,725 E 3291 3291] logging.cc:496: PC: @     0x7f03b9dcc9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m [2025-08-07 12:32:29,725 E 3291 3291] logging.cc:496:     @     0x7f03b9d78520  (unknown)  (unknown)
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m Stack (most recent call first):
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.235)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3292, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3292, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3292, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3570, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3570, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3554, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3554, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3554, ip=10.164.2.240)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: f6c053104a7dfdc003ac9ef8d9dfcbd295614b940c000000 Worker ID: 250713a6a8a43a4523435f6f59cda76db314e5d0271ed0909377c51e Node ID: b63938f7700ec22ee4921f2313bb0c1dca41ed70830c0e08d7feeb36 Worker IP address: 10.164.2.246 Worker port: 10016 Worker PID: 3292 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3274, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3274, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3274, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3626, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3626, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3626, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m 
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m 2025-08-07 12:32:30.419638: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m *** SIGABRT received at time=1754595150 on cpu 11 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m PC: @     0x7ff66e2729fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m     @     0x7ff66e21e520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m [2025-08-07 12:32:30,425 E 3001 3001] logging.cc:496: *** SIGABRT received at time=1754595150 on cpu 11 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m [2025-08-07 12:32:30,425 E 3001 3001] logging.cc:496: PC: @     0x7ff66e2729fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m [2025-08-07 12:32:30,425 E 3001 3001] logging.cc:496:     @     0x7ff66e21e520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m Fatal Python error: Aborted[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m Stack (most recent call first):[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 30x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3001, ip=10.164.2.252)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 2bd535377e4f6e6eba1229560c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m 2025-08-07 12:37:16.300290: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m *** SIGABRT received at time=1754595436 on cpu 96 ***
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m PC: @     0x7fee3d0b69fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m     @     0x7fee3d062520  (unknown)  (unknown)
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m [2025-08-07 12:37:16,306 E 1513 1513] logging.cc:496: *** SIGABRT received at time=1754595436 on cpu 96 ***
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m [2025-08-07 12:37:16,306 E 1513 1513] logging.cc:496: PC: @     0x7fee3d0b69fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m [2025-08-07 12:37:16,306 E 1513 1513] logging.cc:496:     @     0x7fee3d062520  (unknown)  (unknown)
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m Stack (most recent call first):
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1513, ip=10.164.2.215)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1511, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1511, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1511, ip=10.164.3.26)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 06003d1b96dd74801f2ff12f6cb89b3b679562110c000000 Worker ID: c531dc0e4e2ef79bc62f2dbda46704a52fbbd47eea5f12c096eb92ca Node ID: eff3224412b52eb0aef16a9dd4c7a629b675d65babe0758a4e08e7b3 Worker IP address: 10.164.2.215 Worker port: 10004 Worker PID: 1513 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1509, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1509, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1511, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1511, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1511, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1830, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1830, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1830, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m 2025-08-07 12:37:21.440488: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m *** SIGABRT received at time=1754595441 on cpu 37 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m PC: @     0x7fae8265a9fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m     @     0x7fae82606520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m [2025-08-07 12:37:21,446 E 1510 1510] logging.cc:496: *** SIGABRT received at time=1754595441 on cpu 37 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m [2025-08-07 12:37:21,446 E 1510 1510] logging.cc:496: PC: @     0x7fae8265a9fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m [2025-08-07 12:37:21,446 E 1510 1510] logging.cc:496:     @     0x7fae82606520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m Fatal Python error: Aborted[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m Stack (most recent call first):[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 35x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=1510, ip=10.164.2.93)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 7x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: aa70ee17f808b4c42945d4c0c0f44eed11dd8c000c000000 Worker ID: 463926ea4018730f7b31728dcc5d61d37b66df777c8ae072a66e7ffb Node ID: 503aa9acb0493a66f02d9da28b4c93b08672d6ce42e441f07a58503b Worker IP address: 10.164.2.93 Worker port: 10004 Worker PID: 1510 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 7x across cluster][0m
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '22', '31', '3', '2', '20', '12', '19', '17', '16', '6', '15', '27', '5', '8', '28']
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '22', '31', '3', '2', '20', '12', '19', '17', '16', '6', '15', '27', '5', '8', '28']
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 26
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=14606, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 85fb97f339dc954c7b3a4d6c0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '30', '14', '15', '2', '29', '5', '21', '6', '17', '12', '16', '7', '8', '4', '27']
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '30', '14', '15', '2', '29', '5', '21', '6', '17', '12', '16', '7', '8', '4', '27']
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=15133, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 27
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 6f3360b5893417bf69e3adf80c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '26', '24', '21', '31', '20', '22', '19', '18', '25', '27', '23', '29', '9', '16', '6']
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '26', '24', '21', '31', '20', '22', '19', '18', '25', '27', '23', '29', '9', '16', '6']
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 28
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=15660, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m 2025-08-07 13:02:24.952498: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m *** SIGABRT received at time=1754596944 on cpu 95 ***
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m PC: @     0x7f12865269fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m     @     0x7f12864d2520  (unknown)  (unknown)
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m [2025-08-07 13:02:24,956 E 3484 3484] logging.cc:496: *** SIGABRT received at time=1754596944 on cpu 18 ***
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m [2025-08-07 13:02:24,956 E 3484 3484] logging.cc:496: PC: @     0x7f12865269fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m [2025-08-07 13:02:24,956 E 3484 3484] logging.cc:496:     @     0x7f12864d2520  (unknown)  (unknown)
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m Stack (most recent call first):
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m , 
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m scipy.linalg._fblas
[36m(train_lm_task pid=3484, ip=10.164.2.235)[0m , scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3486, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3486, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3486, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3002, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3002, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m 2025-08-07 13:02:24.950220: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3750, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3002, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3002, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3002, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3290, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3290, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3290, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3290, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3290, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3748, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3748, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3748, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3748, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3748, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3748, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3763, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3763, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3763, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3763, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3763, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3763, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3485, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3485, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3467, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3467, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3467, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3467, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3467, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3467, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=2996, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2996, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3819, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3819, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3819, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3819, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3819, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3819, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3289, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3289, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3289, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.244)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: d4c35c6008adafdd0097e71771683d546b758a770c000000 Worker ID: 38d2d7cfebe6a86cfd35d10a45fca902b8e23e890290e1bb133d3e1d Node ID: 8e996b98d1bc020137396778c885e0a7a156395a716626ceeeacac3f Worker IP address: 10.164.2.237 Worker port: 10016 Worker PID: 3290 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 1ec58dd48f21edf9be55d6e6fa62f2794c06a8e10c000000 Worker ID: ccbb80b018740ae102997c856467d5bf1d315dbb2465ecd2d98b1ec1 Node ID: da36cb6cbedbfe03fc90eff08240dc46730bfae44de057fb0dfe75a8 Worker IP address: 10.164.2.244 Worker port: 10014 Worker PID: 3191 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m 2025-08-07 13:07:12.108829: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m *** SIGABRT received at time=1754597232 on cpu 55 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m PC: @     0x7fed3ce959fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m     @     0x7fed3ce41520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m [2025-08-07 13:07:12,114 E 1830 1830] logging.cc:496: *** SIGABRT received at time=1754597232 on cpu 55 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m [2025-08-07 13:07:12,114 E 1830 1830] logging.cc:496: PC: @     0x7fed3ce959fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m [2025-08-07 13:07:12,114 E 1830 1830] logging.cc:496:     @     0x7fed3ce41520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.166)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1829, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1829, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1831, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1831, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1828, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1828, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1828, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1828, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1828, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1828, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m 2025-08-07 13:07:12.106425: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1604, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1832, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1832, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1832, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1832, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1832, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1832, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1832, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1832, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1510, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1510, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1510, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1509, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1509, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1509, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1832, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1832, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1832, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1828, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1828, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1828, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1828, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1828, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1828, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1831, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1831, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1831, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1831, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1831, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1831, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1513, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1513, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1513, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1513, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1513, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1513, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1831, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1831, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1830, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1830, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 484d175cb2aa7f3232959a92c587a44012725aae0c000000 Worker ID: becf578ae695bc1abb14b30522afdc18b69cba03c3cc96611e0516e3 Node ID: 53602fe43e44b29c60add32b8a4fc6873fd58510d78b4a2479aabf5d Worker IP address: 10.164.2.214 Worker port: 10005 Worker PID: 1828 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Worker crashed
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Preempted 19 times. Continuing to retry.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 19
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Futures for slice ray-marin-eu-west4-worker-eb03a5e7-tpu: [ObjectRef(87160e7460eb780affffffffffffffffffffffff0c00000001000000), ObjectRef(274bde819ea26590ffffffffffffffffffffffff0c00000001000000), ObjectRef(2e5a0cb9dfcdaf0bffffffffffffffffffffffff0c00000001000000), ObjectRef(0e848a7423d48ee9ffffffffffffffffffffffff0c00000001000000), ObjectRef(36f23345065d9ed1ffffffffffffffffffffffff0c00000001000000), ObjectRef(75680f4e14cd1fd3ffffffffffffffffffffffff0c00000001000000), ObjectRef(83d06919fd73b679ffffffffffffffffffffffff0c00000001000000), ObjectRef(fef0a4bd0c1a199fffffffffffffffffffffffff0c00000001000000), ObjectRef(02480c3bce0a372fffffffffffffffffffffffff0c00000001000000), ObjectRef(f7cf242ac2197824ffffffffffffffffffffffff0c00000001000000), ObjectRef(8a73cb2bb7671e32ffffffffffffffffffffffff0c00000001000000), ObjectRef(991f8f8fff04d10cffffffffffffffffffffffff0c00000001000000), ObjectRef(c1841da56242b4b6ffffffffffffffffffffffff0c00000001000000), ObjectRef(7602c4a5e0dffa6bffffffffffffffffffffffff0c00000001000000), ObjectRef(167a792ffe2d2d64ffffffffffffffffffffffff0c00000001000000), ObjectRef(17011ad45ba6ec45ffffffffffffffffffffffff0c00000001000000)]
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m 2025-08-07 13:07:12.107164: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m *** SIGABRT received at time=1754597232 on cpu 92 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m PC: @     0x7fad3024d9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m     @     0x7fad301f9520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m [2025-08-07 13:07:12,113 E 1831 1831] logging.cc:496: *** SIGABRT received at time=1754597232 on cpu 92 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m [2025-08-07 13:07:12,113 E 1831 1831] logging.cc:496: PC: @     0x7fad3024d9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m [2025-08-07 13:07:12,113 E 1831 1831] logging.cc:496:     @     0x7fad301f9520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1831, ip=10.164.1.84)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 7x across cluster][0m
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, bcbee71c2c6725254113e00b0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '25', '3', '27', '20', '30', '2', '29', '28', '21', '13', '18', '10', '4', '8', '19']
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '25', '3', '27', '20', '30', '2', '29', '28', '21', '13', '18', '10', '4', '8', '19']
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=16187, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 29
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 838c7b3c22880aef9a8433360c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '15', '21', '22', '18', '4', '12', '28', '5', '19', '26', '8', '9', '16', '10', '3']
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '15', '21', '22', '18', '4', '12', '28', '5', '19', '26', '8', '9', '16', '10', '3']
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 30
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=16714, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m 2025-08-07 13:32:36.869834: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m *** SIGABRT received at time=1754598756 on cpu 102 ***
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m PC: @     0x7fe9b5a1f9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m     @     0x7fe9b59cb520  (unknown)  (unknown)
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m [2025-08-07 13:32:36,875 E 3820 3820] logging.cc:496: *** SIGABRT received at time=1754598756 on cpu 102 ***
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m [2025-08-07 13:32:36,875 E 3820 3820] logging.cc:496: PC: @     0x7fe9b5a1f9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m [2025-08-07 13:32:36,875 E 3820 3820] logging.cc:496:     @     0x7fe9b59cb520  (unknown)  (unknown)
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m Stack (most recent call first):
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3820, ip=10.164.2.236)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b12faad42843c7a5d6d7e2ec2c3537e2cc8381250c000000 Worker ID: ca3793b525e9754710f1203e2a9bcb1a80d9ded1e7a9104f5636f205 Node ID: cf0274c0707ad39288496b55cef486f03eb5c718ed8b8e4bfb9b68ef Worker IP address: 10.164.2.236 Worker port: 10018 Worker PID: 3820 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3291, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3291, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3192, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3192, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3192, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3192, ip=10.164.2.244)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape
[36m(train_lm_task pid=3192, ip=10.164.2.244)[0m , pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly
[36m(train_lm_task pid=3192, ip=10.164.2.244)[0m , scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3749, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3749, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3487, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3487, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3487, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3191, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3764, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3764, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3764, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3189, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3189, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3189, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3482, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3482, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3482, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m 
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m 2025-08-07 13:32:38.501421: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m *** SIGABRT received at time=1754598758 on cpu 10 ***[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m PC: @     0x7f819cae19fc  (unknown)  pthread_kill[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m     @     0x7f819ca8d520  (unknown)  (unknown)[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m [2025-08-07 13:32:38,507 E 3195 3195] logging.cc:496: *** SIGABRT received at time=1754598758 on cpu 10 ***[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m [2025-08-07 13:32:38,507 E 3195 3195] logging.cc:496: PC: @     0x7f819cae19fc  (unknown)  pthread_kill[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m [2025-08-07 13:32:38,507 E 3195 3195] logging.cc:496:     @     0x7f819ca8d520  (unknown)  (unknown)[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m Fatal Python error: Aborted[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m Stack (most recent call first):[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 50x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=3195, ip=10.164.2.252)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 9x across cluster][0m
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 826ec0fb68c1922a1ca3653e0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '5', '20', '31', '21', '29', '22', '4', '17', '18', '14', '12', '8', '24', '6', '2']
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '5', '20', '31', '21', '29', '22', '4', '17', '18', '14', '12', '8', '24', '6', '2']
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=17241, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 31
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m 2025-08-07 13:37:23.957750: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m *** SIGABRT received at time=1754599043 on cpu 37 ***
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m PC: @     0x7fbe6e4719fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m     @     0x7fbe6e41d520  (unknown)  (unknown)
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m [2025-08-07 13:37:23,963 E 1830 1830] logging.cc:496: *** SIGABRT received at time=1754599043 on cpu 37 ***
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m [2025-08-07 13:37:23,963 E 1830 1830] logging.cc:496: PC: @     0x7fbe6e4719fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m [2025-08-07 13:37:23,963 E 1830 1830] logging.cc:496:     @     0x7fbe6e41d520  (unknown)  (unknown)
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m Stack (most recent call first):
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.93)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=1829, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=1829, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1829, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1829, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1831, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1833, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1833, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=1833, ip=10.164.2.215)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 696af921ef8c845d4b2122d6fe7bf24bce03af350c000000 Worker ID: 26c9ab29a506c6d5bc6a5f0a2ef561ad76f0fd1eb614c211a551b588 Node ID: 503aa9acb0493a66f02d9da28b4c93b08672d6ce42e441f07a58503b Worker IP address: 10.164.2.93 Worker port: 10006 Worker PID: 1830 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=1830, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=1833, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1833, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1833, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1834, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1834, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1834, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1828, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1828, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1828, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m 
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m 2025-08-07 13:37:24.748343: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m *** SIGABRT received at time=1754599044 on cpu 1 ***[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m PC: @     0x7f4031e169fc  (unknown)  pthread_kill[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m     @     0x7f4031dc2520  (unknown)  (unknown)[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m [2025-08-07 13:37:24,754 E 2025 2025] logging.cc:496: *** SIGABRT received at time=1754599044 on cpu 1 ***[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m [2025-08-07 13:37:24,754 E 2025 2025] logging.cc:496: PC: @     0x7f4031e169fc  (unknown)  pthread_kill[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m [2025-08-07 13:37:24,754 E 2025 2025] logging.cc:496:     @     0x7f4031dc2520  (unknown)  (unknown)[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m Fatal Python error: Aborted[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m Stack (most recent call first):[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 50x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2025, ip=10.164.3.21)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 10x across cluster][0m
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d32a0436a727c145991cbee90c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '6', '18', '30', '28', '3', '26', '15', '24', '27', '12', '25', '29', '10', '21', '2']
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '6', '18', '30', '28', '3', '26', '15', '24', '27', '12', '25', '29', '10', '21', '2']
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=17769, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 32
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 3ceb6c91953c0870d72fd6940c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '10', '31', '2', '14', '15', '6', '26', '28', '22', '19', '30', '13', '24', '25', '23']
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '10', '31', '2', '14', '15', '6', '26', '28', '22', '19', '30', '13', '24', '25', '23']
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=18296, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 33
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m 2025-08-07 14:02:33.775268: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m *** SIGABRT received at time=1754600553 on cpu 13 ***
[36m(train_lm_task pid=3384, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3384, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3384, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3486, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3486, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m PC: @     0x7f2fff0c89fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m     @     0x7f2fff074520  (unknown)  (unknown)
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m [2025-08-07 14:02:33,780 E 3385 3385] logging.cc:496: *** SIGABRT received at time=1754600553 on cpu 95 ***
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m [2025-08-07 14:02:33,780 E 3385 3385] logging.cc:496: PC: @     0x7f2fff0c89fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m [2025-08-07 14:02:33,781 E 3385 3385] logging.cc:496:     @     0x7f2fff074520  (unknown)  (unknown)
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m Stack (most recent call first):
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.244)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m 2025-08-07 14:02:33.774522: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3955, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3680, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3680, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3468, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3468, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3468, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3468, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3468, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3468, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3942, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3942, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3942, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3942, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3942, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3942, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3190, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3190, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3190, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3483, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3483, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4013, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4013, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4013, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4013, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4013, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4013, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3196, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3196, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3196, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3196, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3196, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3196, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3678, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3678, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3484, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3484, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3195, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3195, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3195, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=3483, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3483, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3483, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3957, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3486, ip=10.164.2.247)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 568736545341974bce8d498f0f0ee313b54f60620c000000 Worker ID: 1deb3c7c0d544b18223975aac9aaf53036fc9b1dc68f13c14b9ddeba Node ID: a3d125607a4511ba998e68a8cddb165a669e924aacb773c1b5a7a3d5 Worker IP address: 10.164.2.252 Worker port: 10015 Worker PID: 3196 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 11x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: bf251c2d0ba79669f36e12a674a148e316650c990c000000 Worker ID: eea43f3f13a73187b218206ceb82c4647bc2b0de4a5028648e749078 Node ID: 08b55f819516cb8d2b906c83fccbcc2d9275f51d8cb107727892c05d Worker IP address: 10.164.2.247 Worker port: 10018 Worker PID: 3486 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: recvmsg:Connection reset by peer; RPC Error details: 
[36m(TPUHostActor pid=2616, ip=10.164.2.235)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Worker crashed
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Preempted 3 times. Continuing to retry.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 3
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(9d8b8879713fac45ffffffffffffffffffffffff0c00000001000000), ObjectRef(ee1b51a38ff872e2ffffffffffffffffffffffff0c00000001000000), ObjectRef(b252224d5d8294f3ffffffffffffffffffffffff0c00000001000000), ObjectRef(ab121bfee266dca7ffffffffffffffffffffffff0c00000001000000), ObjectRef(a920c5f776fc28cdffffffffffffffffffffffff0c00000001000000), ObjectRef(40fbe3119fa6572effffffffffffffffffffffff0c00000001000000), ObjectRef(78e6edd4c5ec8f3fffffffffffffffffffffffff0c00000001000000), ObjectRef(653cd2744d37a7c7ffffffffffffffffffffffff0c00000001000000), ObjectRef(a3a132846dd159d6ffffffffffffffffffffffff0c00000001000000), ObjectRef(3779fada094f46abffffffffffffffffffffffff0c00000001000000), ObjectRef(2ad1781ebf255ceaffffffffffffffffffffffff0c00000001000000), ObjectRef(3beececf78c769b0ffffffffffffffffffffffff0c00000001000000), ObjectRef(a756ec26b31ef12dffffffffffffffffffffffff0c00000001000000), ObjectRef(382c3bc945f0c99cffffffffffffffffffffffff0c00000001000000), ObjectRef(0fdb76ce8fa53dffffffffffffffffffffffffff0c00000001000000), ObjectRef(403e6a6ded7c0ba5ffffffffffffffffffffffff0c00000001000000)]
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m 2025-08-07 14:07:20.130399: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m *** SIGABRT received at time=1754600840 on cpu 111 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m PC: @     0x7f20754519fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m     @     0x7f20753fd520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m [2025-08-07 14:07:20,136 E 2024 2024] logging.cc:496: *** SIGABRT received at time=1754600840 on cpu 111 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m [2025-08-07 14:07:20,136 E 2024 2024] logging.cc:496: PC: @     0x7f20754519fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m [2025-08-07 14:07:20,136 E 2024 2024] logging.cc:496:     @     0x7f20753fd520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.166)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 34x across cluster][0m
[36m(train_lm_task pid=2026, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2026, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2026, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2026, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2026, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2026, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=1833, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=1833, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=1833, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1833, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1833, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1833, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=1832, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=1832, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=1832, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1832, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1832, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1832, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=1829, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=1829, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=1829, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1829, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1829, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=1829, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2024, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2024, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2027, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2027, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2027, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2027, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2027, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2027, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2022, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2022, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2022, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2022, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2022, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2022, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2022, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2022, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2022, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2022, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2022, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2022, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2025, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2025, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m 2025-08-07 14:07:20.129782: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1936, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2024, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2024, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2023, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2023, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=2024, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=2024, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:18
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:26
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: c32ee5018ec4241169486df5f705c30c6386dacd0c000000 Worker ID: 825e42bb19faf883fef0d202cc4e50ef545d0a938953b93afc15c619 Node ID: fbd81f85517d3c493f0fd7dbcb2f701c1d7a04d0772508fd09225266 Worker IP address: 10.164.3.7 Worker port: 10006 Worker PID: 1829 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m 2025-08-07 14:07:20.131762: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m *** SIGABRT received at time=1754600840 on cpu 25 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m PC: @     0x7f670c1eb9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m     @     0x7f670c197520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m [2025-08-07 14:07:20,137 E 1829 1829] logging.cc:496: *** SIGABRT received at time=1754600840 on cpu 25 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m [2025-08-07 14:07:20,137 E 1829 1829] logging.cc:496: PC: @     0x7f670c1eb9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m [2025-08-07 14:07:20,137 E 1829 1829] logging.cc:496:     @     0x7f670c197520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=1829, ip=10.164.2.134)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, c5b19355a576880dafd748780c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '17', '14', '3', '20', '5', '2', '25', '12', '19', '30', '15', '13', '28', '23', '10']
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '17', '14', '3', '20', '5', '2', '25', '12', '19', '30', '15', '13', '28', '23', '10']
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=18823, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 34
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 6b8da3867c0c10b7315871220c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '24', '17', '14', '3', '30', '27', '12', '26', '5', '28', '19', '13', '15', '25', '10']
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '24', '17', '14', '3', '30', '27', '12', '26', '5', '28', '19', '13', '15', '25', '10']
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=19350, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 35
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m 2025-08-07 14:32:45.630921: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m *** SIGABRT received at time=1754602365 on cpu 47 ***
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m PC: @     0x7f838ede89fc  (unknown)  pthread_kill
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m     @     0x7f838ed94520  (unknown)  (unknown)
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m [2025-08-07 14:32:45,636 E 4014 4014] logging.cc:496: *** SIGABRT received at time=1754602365 on cpu 47 ***
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m [2025-08-07 14:32:45,636 E 4014 4014] logging.cc:496: PC: @     0x7f838ede89fc  (unknown)  pthread_kill
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m [2025-08-07 14:32:45,636 E 4014 4014] logging.cc:496:     @     0x7f838ed94520  (unknown)  (unknown)
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m Stack (most recent call first):
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4014, ip=10.164.2.236)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3196, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3196, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3196, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3485, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3681, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3681, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3681, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3943, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3943, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3943, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 70d67c703b84be47db915888062130779d8907c00c000000 Worker ID: 8eb2216b42576bf453f0b90217315210a1135fb19c60c782e46e4408 Node ID: b63938f7700ec22ee4921f2313bb0c1dca41ed70830c0e08d7feeb36 Worker IP address: 10.164.2.246 Worker port: 10020 Worker PID: 3681 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 46cc2dc4b27044aef874272b0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m 2025-08-07 14:32:45.651285: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m *** SIGABRT received at time=1754602365 on cpu 67 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m PC: @     0x7f3d4ed809fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m     @     0x7f3d4ed2c520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m [2025-08-07 14:32:45,657 E 3385 3385] logging.cc:496: *** SIGABRT received at time=1754602365 on cpu 67 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m [2025-08-07 14:32:45,657 E 3385 3385] logging.cc:496: PC: @     0x7f3d4ed809fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m [2025-08-07 14:32:45,657 E 3385 3385] logging.cc:496:     @     0x7f3d4ed2c520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m Fatal Python error: Aborted[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m Stack (most recent call first):[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 30x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '14', '28', '5', '30', '3', '27', '6', '15', '20', '22', '13', '29', '23', '1', '12']
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '14', '28', '5', '30', '3', '27', '6', '15', '20', '22', '13', '29', '23', '1', '12']
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=19877, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 36
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=2028, ip=10.164.2.215)[0m 2025-08-07 14:37:32.182342: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2028, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2028, ip=10.164.2.215)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2028, ip=10.164.2.215)[0m *** SIGABRT received at time=1754602652 on cpu 96 ***
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m PC: @     0x7ff00e8d79fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m     @     0x7ff00e883520  (unknown)  (unknown)
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m [2025-08-07 14:37:32,148 E 1830 1830] logging.cc:496: *** SIGABRT received at time=1754602652 on cpu 25 ***
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m [2025-08-07 14:37:32,148 E 1830 1830] logging.cc:496: PC: @     0x7ff00e8d79fc  (unknown)  pthread_kill
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m [2025-08-07 14:37:32,148 E 1830 1830] logging.cc:496:     @     0x7ff00e883520  (unknown)  (unknown)
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m Stack (most recent call first):
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=1830, ip=10.164.2.134)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2026, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2026, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2026, ip=10.164.2.213)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 11468c12d83297d4ef1830b89b20b0edc9bfd5f60c000000 Worker ID: 6297fe822981ab2fcdcafb5f182978dd289c9fa07372455d6c69909f Node ID: 57bb13b527476e87a24c2a0901a2fe8dc56fa01a92f14c263b30ae6d Worker IP address: 10.164.2.134 Worker port: 10006 Worker PID: 1830 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2028, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2028, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2027, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2027, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2027, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2023, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2023, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2023, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2025, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2025, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2025, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m 
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m 2025-08-07 14:37:33.705096: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m *** SIGABRT received at time=1754602653 on cpu 86 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m PC: @     0x7f46e355e9fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m     @     0x7f46e350a520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m [2025-08-07 14:37:33,710 E 2022 2022] logging.cc:496: *** SIGABRT received at time=1754602653 on cpu 86 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m [2025-08-07 14:37:33,710 E 2022 2022] logging.cc:496: PC: @     0x7f46e355e9fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m [2025-08-07 14:37:33,711 E 2022 2022] logging.cc:496:     @     0x7f46e350a520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m Fatal Python error: Aborted[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m Stack (most recent call first):[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 45x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2022, ip=10.164.3.7)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 9x across cluster][0m
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 2d58abb5a8ce54d66b8217e20c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '5', '10', '18', '21', '14', '15', '16', '31', '27', '17', '6', '2', '4', '29', '1']
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '5', '10', '18', '21', '14', '15', '16', '31', '27', '17', '6', '2', '4', '29', '1']
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 37
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=20404, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, e8dfc1f05d99f583088a89490c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '15', '21', '30', '3', '5', '9', '10', '20', '4', '2', '14', '27', '18', '29', '26']
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '15', '21', '30', '3', '5', '9', '10', '20', '4', '2', '14', '27', '18', '29', '26']
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 38
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=20931, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m 2025-08-07 15:02:40.860254: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m *** SIGABRT received at time=1754604160 on cpu 17 ***
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m PC: @     0x7f9fe87e69fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m     @     0x7f9fe8792520  (unknown)  (unknown)
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m [2025-08-07 15:02:40,867 E 3958 3958] logging.cc:496: *** SIGABRT received at time=1754604160 on cpu 29 ***
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m [2025-08-07 15:02:40,867 E 3958 3958] logging.cc:496: PC: @     0x7f9fe87e69fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m [2025-08-07 15:02:40,867 E 3958 3958] logging.cc:496:     @     0x7f9fe8792520  (unknown)  (unknown)
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m Stack (most recent call first):
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3958, ip=10.164.2.234)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3484, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3484, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3484, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3484, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m 2025-08-07 15:02:40.859730: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3956, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3578, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3578, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3676, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3676, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3676, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3676, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3676, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3676, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4136, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=4136, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=4136, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4136, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4136, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4136, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3874, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3874, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4207, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=4207, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=4207, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4207, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4207, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4207, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3386, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3386, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3386, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3386, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3386, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3386, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3389, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3389, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3872, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3872, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3384, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3384, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3384, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3679, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3679, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3661, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=3661, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=3661, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3661, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3661, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3661, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3389, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 6ce0dfd1e93227fb9177d46d90930cd2444948900c000000 Worker ID: 14998c09e08c68bdb9e19f45850210ba5e93a42a4ed0c3e58ceeca24 Node ID: c230ed505bb0f399bfe68ddcf3c6c500050664501c095b12254a4e62 Worker IP address: 10.164.2.250 Worker port: 10019 Worker PID: 3956 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m 2025-08-07 15:07:28.027717: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m The first task at the barrier: /job:jax_worker/replica:0/task:12. Some timed out task names:[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m *** SIGABRT received at time=1754604448 on cpu 28 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m PC: @     0x7f6b4ae329fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m     @     0x7f6b4adde520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m [2025-08-07 15:02:40,866 E 3678 3678] logging.cc:496: *** SIGABRT received at time=1754604160 on cpu 24 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m [2025-08-07 15:02:40,866 E 3678 3678] logging.cc:496: PC: @     0x7f6b4ae329fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m [2025-08-07 15:02:40,866 E 3678 3678] logging.cc:496:     @     0x7f6b4adde520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3678, ip=10.164.2.253)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2220, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2220, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2220, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2220, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2220, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2220, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2219, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2219, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2219, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2219, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2219, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2219, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m 2025-08-07 15:07:28.025657: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=1937, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2217, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2217, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2221, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2221, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2221, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2221, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2221, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2221, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2218, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2218, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2025, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2025, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2217, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2217, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2217, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2217, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2217, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2217, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2026, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2026, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2026, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2026, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2026, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2026, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2219, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2219, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2023, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2023, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2023, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2023, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2023, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2023, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2218, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2218, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2027, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2027, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2027, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2027, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2027, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2027, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2025, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2025, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2025, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: a171d184927721f29c83d430601f95f6e898ddac0c000000 Worker ID: cfc45df1876cf8d9c67d97475758232b4156719bb4e412211723910f Node ID: 9324d786cd5c86c1bd3c41c24b6cb6076822d214f33adf5faba24291 Worker IP address: 10.164.1.96 Worker port: 10007 Worker PID: 2027 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Worker crashed
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Preempted 20 times. Continuing to retry.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 20
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Futures for slice ray-marin-eu-west4-worker-eb03a5e7-tpu: [ObjectRef(80eb68e73955ead3ffffffffffffffffffffffff0c00000001000000), ObjectRef(7ee7f0b6165cb798ffffffffffffffffffffffff0c00000001000000), ObjectRef(43ca2ddeff4bd084ffffffffffffffffffffffff0c00000001000000), ObjectRef(ef9b530f76858615ffffffffffffffffffffffff0c00000001000000), ObjectRef(afb1ab8c3806a7baffffffffffffffffffffffff0c00000001000000), ObjectRef(95b0536720260e52ffffffffffffffffffffffff0c00000001000000), ObjectRef(242a594ba6cfc544ffffffffffffffffffffffff0c00000001000000), ObjectRef(e0bc248c8838c532ffffffffffffffffffffffff0c00000001000000), ObjectRef(830361943241356effffffffffffffffffffffff0c00000001000000), ObjectRef(af37d3180b284271ffffffffffffffffffffffff0c00000001000000), ObjectRef(3e420f8dbd8dee02ffffffffffffffffffffffff0c00000001000000), ObjectRef(ddfc377e3ffd1742ffffffffffffffffffffffff0c00000001000000), ObjectRef(472154efacddbba4ffffffffffffffffffffffff0c00000001000000), ObjectRef(938d79fcd641b5d2ffffffffffffffffffffffff0c00000001000000), ObjectRef(60feb51732d2ef86ffffffffffffffffffffffff0c00000001000000), ObjectRef(cbf86116f53cbcd2ffffffffffffffffffffffff0c00000001000000)]
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m 2025-08-07 15:07:28.026501: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m The first task at the barrier: /job:jax_worker/replica:0/task:12. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2023, ip=10.164.2.134)[0m *** SIGABRT received at time=1754604448 on cpu 26 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m PC: @     0x7f7d2461a9fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m     @     0x7f7d245c6520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m [2025-08-07 15:07:28,033 E 2024 2024] logging.cc:496: *** SIGABRT received at time=1754604448 on cpu 28 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m [2025-08-07 15:07:28,033 E 2024 2024] logging.cc:496: PC: @     0x7f7d2461a9fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m [2025-08-07 15:07:28,033 E 2024 2024] logging.cc:496:     @     0x7f7d245c6520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2024, ip=10.164.2.82)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 7x across cluster][0m
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, d663fa36f9b9492200d4b3c90c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 started.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '27', '30', '20', '4', '18', '28', '15', '13', '17', '23', '26', '6', '7', '14', '11']
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '27', '30', '20', '4', '18', '28', '15', '13', '17', '23', '26', '6', '7', '14', '11']
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 39
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 7 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=21458, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, aab22339af9dd6930da593350c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '15', '27', '26', '17', '28', '5', '12', '18', '22', '29', '10', '3', '2', '11', '20']
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '15', '27', '26', '17', '28', '5', '12', '18', '22', '29', '10', '3', '2', '11', '20']
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=21985, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 40
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 403b1db6c9415880bdfcbe7a0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m 2025-08-07 15:32:52.826313: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m *** SIGABRT received at time=1754605972 on cpu 24 ***
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m PC: @     0x7f1a7e28c9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m     @     0x7f1a7e238520  (unknown)  (unknown)
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m [2025-08-07 15:32:52,832 E 3679 3679] logging.cc:496: *** SIGABRT received at time=1754605972 on cpu 24 ***
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m [2025-08-07 15:32:52,832 E 3679 3679] logging.cc:496: PC: @     0x7f1a7e28c9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m [2025-08-07 15:32:52,832 E 3679 3679] logging.cc:496:     @     0x7f1a7e238520  (unknown)  (unknown)
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m Stack (most recent call first):
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3679, ip=10.164.2.253)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=4208, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4208, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4208, ip=10.164.2.236)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: dd26195d1dae2a94e5cbe05b08e5de212ca224700c000000 Worker ID: cb1274d7514764c5766fb8e6c9bc2692d7bd00b002123b5742cd14ad Node ID: a16e49065a6045ec215812b52ffd813d98570c8a83fcb233b99911cc Worker IP address: 10.164.2.253 Worker port: 10020 Worker PID: 3679 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3385, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3385, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3875, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3875, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3875, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4137, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4137, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4137, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3677, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3677, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3677, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3390, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3390, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3390, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3390, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3390, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3390, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3677, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3677, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3677, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m 
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m 2025-08-07 15:32:54.089423: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m *** SIGABRT received at time=1754605974 on cpu 28 ***[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m PC: @     0x7fd82056e9fc  (unknown)  pthread_kill[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m     @     0x7fd82051a520  (unknown)  (unknown)[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m [2025-08-07 15:32:54,095 E 4151 4151] logging.cc:496: *** SIGABRT received at time=1754605974 on cpu 28 ***[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m [2025-08-07 15:32:54,095 E 4151 4151] logging.cc:496: PC: @     0x7fd82056e9fc  (unknown)  pthread_kill[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m [2025-08-07 15:32:54,095 E 4151 4151] logging.cc:496:     @     0x7fd82051a520  (unknown)  (unknown)[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m Fatal Python error: Aborted[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m Stack (most recent call first):[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 60x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 12x across cluster][0m
[36m(train_lm_task pid=4151, ip=10.164.2.234)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 12x across cluster][0m
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '4', '31', '22', '29', '14', '5', '9', '2', '3', '6', '1', '21', '10', '17', '18']
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '4', '31', '22', '29', '14', '5', '9', '2', '3', '6', '1', '21', '10', '17', '18']
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=22512, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 41
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m 2025-08-07 15:37:39.924361: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m *** SIGABRT received at time=1754606259 on cpu 26 ***
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m PC: @     0x7f887b13b9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m     @     0x7f887b0e7520  (unknown)  (unknown)
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m [2025-08-07 15:37:39,930 E 2024 2024] logging.cc:496: *** SIGABRT received at time=1754606259 on cpu 26 ***
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m [2025-08-07 15:37:39,930 E 2024 2024] logging.cc:496: PC: @     0x7f887b13b9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m [2025-08-07 15:37:39,930 E 2024 2024] logging.cc:496:     @     0x7f887b0e7520  (unknown)  (unknown)
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m Stack (most recent call first):
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2024, ip=10.164.2.134)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2219, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2221, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2221, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2221, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2220, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2220, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2220, ip=10.164.3.21)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 987a323c79b312a0be57b133737bb8275bcf28100c000000 Worker ID: 7d869a0c8d9d162395d678ac7a10380c3e394f611d5b3d3f744263b2 Node ID: a4cd1da26a84e5e12cecd974d2acd594e800c56b5d3773dfc963fcb7 Worker IP address: 10.164.2.166 Worker port: 10010 Worker PID: 2219 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 13x across cluster][0m
[36m(train_lm_task pid=2219, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2216, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2216, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2216, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2221, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2221, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2221, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m 
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m 2025-08-07 15:37:40.678529: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m *** SIGABRT received at time=1754606260 on cpu 37 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m PC: @     0x7f90fc4ae9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m     @     0x7f90fc45a520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m [2025-08-07 15:37:40,684 E 2218 2218] logging.cc:496: *** SIGABRT received at time=1754606260 on cpu 37 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m [2025-08-07 15:37:40,684 E 2218 2218] logging.cc:496: PC: @     0x7f90fc4ae9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m [2025-08-07 15:37:40,684 E 2218 2218] logging.cc:496:     @     0x7f90fc45a520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2218, ip=10.164.2.93)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 8x across cluster][0m
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, e0960305cdc7b3bd974a47480c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '20', '14', '21', '31', '12', '22', '10', '28', '18', '25', '23', '2', '26', '16', '5']
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '20', '14', '21', '31', '12', '22', '10', '28', '18', '25', '23', '2', '26', '16', '5']
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 42
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=23039, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 3f304b8633961943a69858260c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '3', '5', '17', '14', '26', '12', '24', '27', '25', '15', '30', '4', '9', '23', '29']
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '3', '5', '17', '14', '26', '12', '24', '27', '25', '15', '30', '4', '9', '23', '29']
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=23566, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 43
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m 2025-08-07 16:02:49.006063: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m *** SIGABRT received at time=1754607769 on cpu 40 ***
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m PC: @     0x7fca3462a9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m     @     0x7fca345d6520  (unknown)  (unknown)
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m [2025-08-07 16:02:49,013 E 3680 3680] logging.cc:496: *** SIGABRT received at time=1754607769 on cpu 42 ***
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m [2025-08-07 16:02:49,013 E 3680 3680] logging.cc:496: PC: @     0x7fca3462a9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m [2025-08-07 16:02:49,013 E 3680 3680] logging.cc:496:     @     0x7fca345d6520  (unknown)  (unknown)
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m Stack (most recent call first):
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m , 
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m pandas._libs.tslibs.timestamps
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m , pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m , scipy.interpolate._fitpack
[36m(train_lm_task pid=3680, ip=10.164.2.247)[0m , scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3583, ip=10.164.2.252)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3580, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3580, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3580, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3580, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3580, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3580, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4066, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4066, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3872, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3872, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m 2025-08-07 16:02:49.005562: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4401, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4401, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4401, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4401, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4401, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4401, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3870, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3870, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3870, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3870, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3870, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3870, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3662, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3662, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3662, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3662, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3662, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3662, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3578, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3578, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3578, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3678, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3678, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3678, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3772, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3772, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=3584, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=3584, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4152, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4152, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4152, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4152, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4152, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4152, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:2
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b419f71c93fcbf2dc3390736d77f2acb61d2be470c000000 Worker ID: 93b3cb78d0ea2ebcaf7211252d60d19a2ce89f0ad9b62be339be2553 Node ID: 9f5d9292f00b4b33e1b11b3820a942102773ccd980e91b96635bd4b8 Worker IP address: 10.164.2.232 Worker port: 10018 Worker PID: 3584 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 6db5b2f241684b887375375796dd4868f614aab70c000000 Worker ID: 41686edb1a725c9496b4e906349353519efa7b186e5c12942d5a779d Node ID: 08b55f819516cb8d2b906c83fccbcc2d9275f51d8cb107727892c05d Worker IP address: 10.164.2.247 Worker port: 10020 Worker PID: 3680 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: recvmsg:Connection reset by peer; RPC Error details: 
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Worker crashed
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Preempted 4 times. Continuing to retry.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 4
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(4ef6cd4b5d0e143fffffffffffffffffffffffff0c00000001000000), ObjectRef(e8afe3654d4aac81ffffffffffffffffffffffff0c00000001000000), ObjectRef(4d7cc07a7325ea17ffffffffffffffffffffffff0c00000001000000), ObjectRef(3c3e537b925f7458ffffffffffffffffffffffff0c00000001000000), ObjectRef(1a4980dc7b04096cffffffffffffffffffffffff0c00000001000000), ObjectRef(8490436eab210f9fffffffffffffffffffffffff0c00000001000000), ObjectRef(2222249edbe376b4ffffffffffffffffffffffff0c00000001000000), ObjectRef(2f748bd09cf6dacaffffffffffffffffffffffff0c00000001000000), ObjectRef(574c3e26b72ee924ffffffffffffffffffffffff0c00000001000000), ObjectRef(613c4566904550e5ffffffffffffffffffffffff0c00000001000000), ObjectRef(b5027378000a5d22ffffffffffffffffffffffff0c00000001000000), ObjectRef(29011e1c93c0addcffffffffffffffffffffffff0c00000001000000), ObjectRef(4090406a733ccf7effffffffffffffffffffffff0c00000001000000), ObjectRef(1db3a48a40393681ffffffffffffffffffffffff0c00000001000000), ObjectRef(c89e8bcb6bf510abffffffffffffffffffffffff0c00000001000000), ObjectRef(99afc6a36cbae2efffffffffffffffffffffffff0c00000001000000)]
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m 2025-08-07 16:02:49.006389: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4068, ip=10.164.2.246)[0m *** SIGABRT received at time=1754607769 on cpu 102 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m PC: @     0x7f7fe9fa39fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m     @     0x7f7fe9f4f520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m [2025-08-07 16:02:49,011 E 4330 4330] logging.cc:496: *** SIGABRT received at time=1754607769 on cpu 40 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m [2025-08-07 16:02:49,011 E 4330 4330] logging.cc:496: PC: @     0x7f7fe9fa39fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m [2025-08-07 16:02:49,012 E 4330 4330] logging.cc:496:     @     0x7f7fe9f4f520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4330, ip=10.164.2.240)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 14x across cluster][0m
[36m(TPUHostActor pid=3245, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 18x across cluster][0m
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m 2025-08-07 16:07:35.991699: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m *** SIGABRT received at time=1754608055 on cpu 12 ***
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m PC: @     0x7f8cfcd849fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m     @     0x7f8cfcd30520  (unknown)  (unknown)
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m [2025-08-07 16:07:35,997 E 2218 2218] logging.cc:496: *** SIGABRT received at time=1754608055 on cpu 12 ***
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m [2025-08-07 16:07:35,997 E 2218 2218] logging.cc:496: PC: @     0x7f8cfcd849fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m [2025-08-07 16:07:35,997 E 2218 2218] logging.cc:496:     @     0x7f8cfcd30520  (unknown)  (unknown)
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m Stack (most recent call first):
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.214)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2217, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2217, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2217, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2217, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2217, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2217, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2220, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2220, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2220, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2220, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2220, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2220, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2219, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2219, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2413, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2413, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2219, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2219, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2413, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2413, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2413, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2413, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2413, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2413, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2218, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2218, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2028, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2028, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2028, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2028, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2028, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2028, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m 2025-08-07 16:07:35.989416: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2142, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2220, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2220, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2220, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2220, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2220, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2220, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2217, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2217, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2217, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2414, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2414, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2414, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2414, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2414, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2414, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2218, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2218, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2218, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2218, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2218, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2218, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2412, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2412, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 54c78552d268c8e92bd73d59bf82c6706ffa56e00c000000 Worker ID: 513d116459389d93cddd4b005dd9e992e04cf10f5d98da6948d4ef01 Node ID: 8e123ff8c4c6962833452c673197977fa32e847df5e5d63c617f6859 Worker IP address: 10.164.2.91 Worker port: 10011 Worker PID: 2412 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 9208a7101ff15c2fca3bec560c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '21', '31', '20', '24', '26', '10', '30', '28', '29', '23', '27', '17', '22', '6', '2']
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '21', '31', '20', '24', '26', '10', '30', '28', '29', '23', '27', '17', '22', '6', '2']
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m 2025-08-07 16:07:35.991423: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m *** SIGABRT received at time=1754608055 on cpu 97 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m PC: @     0x7fc7cc4169fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m     @     0x7fc7cc3c2520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m [2025-08-07 16:07:35,997 E 2222 2222] logging.cc:496: *** SIGABRT received at time=1754608055 on cpu 97 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m [2025-08-07 16:07:35,997 E 2222 2222] logging.cc:496: PC: @     0x7fc7cc4169fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m [2025-08-07 16:07:35,997 E 2222 2222] logging.cc:496:     @     0x7fc7cc3c2520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2222, ip=10.164.2.215)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 44
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=24093, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 6f146a4f8fd8ead40caa26310c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '17', '3', '25', '2', '18', '14', '9', '5', '29', '21', '22', '26', '6', '12', '15']
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '17', '3', '25', '2', '18', '14', '9', '5', '29', '21', '22', '26', '6', '12', '15']
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=24620, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 45
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 5a52e7bafbce0ccd250c69ae0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '14', '5', '15', '30', '29', '25', '6', '17', '31', '3', '27', '4', '13', '22', '9']
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '14', '5', '15', '30', '29', '25', '6', '17', '31', '3', '27', '4', '13', '22', '9']
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=25148, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 46
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m 2025-08-07 16:33:00.858881: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m *** SIGABRT received at time=1754609580 on cpu 64 ***
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m PC: @     0x7f645abd69fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m     @     0x7f645ab82520  (unknown)  (unknown)
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m [2025-08-07 16:33:00,864 E 3773 3773] logging.cc:496: *** SIGABRT received at time=1754609580 on cpu 64 ***
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m [2025-08-07 16:33:00,864 E 3773 3773] logging.cc:496: PC: @     0x7f645abd69fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m [2025-08-07 16:33:00,864 E 3773 3773] logging.cc:496:     @     0x7f645ab82520  (unknown)  (unknown)
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m Stack (most recent call first):
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3873, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.235)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9a07f8225cfc63362ff458de4e291a499e84ca690c000000 Worker ID: 062969988767bf39721643caaf322fa94ea85e066d50dbd9dc678808 Node ID: c486bda5304c7e6dab8fc7f779f6d75f5c447a93a27e1e44bbcd922b Worker IP address: 10.164.2.231 Worker port: 10021 Worker PID: 3773 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3773, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4402, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4402, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4402, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3585, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3585, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3585, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m , 
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m aiohttp._http_writer
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m , aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast
[36m(train_lm_task pid=3873, ip=10.164.2.247)[0m , lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=4331, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4331, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4331, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m 2025-08-07 16:37:47.929765: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m *** SIGABRT received at time=1754609867 on cpu 72 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m PC: @     0x7fbed25569fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m     @     0x7fbed2502520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m [2025-08-07 16:37:47,935 E 2413 2413] logging.cc:496: *** SIGABRT received at time=1754609867 on cpu 72 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m [2025-08-07 16:37:47,935 E 2413 2413] logging.cc:496: PC: @     0x7fbed25569fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m [2025-08-07 16:37:47,935 E 2413 2413] logging.cc:496:     @     0x7fbed2502520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m Fatal Python error: Aborted[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m Stack (most recent call first):[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 45x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.91)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 8x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ecae804075ea253f8c1e4434b2c1123cc6a902b70c000000 Worker ID: 5292805be92af9a1131f9f576f32de2c82cd668f038e85f341ce7630 Node ID: 8e123ff8c4c6962833452c673197977fa32e847df5e5d63c617f6859 Worker IP address: 10.164.2.91 Worker port: 10012 Worker PID: 2413 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2414, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2414, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2414, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2218, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2410, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2410, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2410, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2415, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m  in 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m train_lm_task
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m   File 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m "
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m "
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m , line 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m 946 in 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m main_loop
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m   File 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m "
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m "
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m , line 330 in <module>
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, 
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m torch._C._dynamo.eval_frame
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m , torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m , scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m 2025-08-07 16:37:49.492187: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m *** SIGABRT received at time=1754609869 on cpu 55 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m PC: @     0x7fe06fca39fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m     @     0x7fe06fc4f520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m [2025-08-07 16:37:49,497 E 2412 2412] logging.cc:496: *** SIGABRT received at time=1754609869 on cpu 55 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m [2025-08-07 16:37:49,497 E 2412 2412] logging.cc:496: PC: @     0x7fe06fca39fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m [2025-08-07 16:37:49,498 E 2412 2412] logging.cc:496:     @     0x7fe06fc4f520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m Fatal Python error: Aborted[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m Stack (most recent call first):[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 35x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.223)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=2411, ip=10.164.2.214)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 8ef6a536d78d26d820539ec00c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '21', '10', '5', '6', '18', '19', '14', '4', '31', '16', '23', '27', '3', '26', '30']
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '21', '10', '5', '6', '18', '19', '14', '4', '31', '16', '23', '27', '3', '26', '30']
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 19 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=25675, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 47
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 0dba57f6767fb061748666d00c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '2', '3', '4', '26', '15', '25', '28', '20', '6', '14', '29', '18', '1', '16', '24']
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '2', '3', '4', '26', '15', '25', '28', '20', '6', '14', '29', '18', '1', '16', '24']
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 48
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=26203, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m 2025-08-07 17:02:57.316418: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m The first task at the barrier: /job:jax_worker/replica:0/task:25. Some timed out task names:
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m *** SIGABRT received at time=1754611377 on cpu 95 ***
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m PC: @     0x7f0837e659fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m     @     0x7f0837e11520  (unknown)  (unknown)
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m [2025-08-07 17:02:57,322 E 3774 3774] logging.cc:496: *** SIGABRT received at time=1754611377 on cpu 95 ***
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m [2025-08-07 17:02:57,322 E 3774 3774] logging.cc:496: PC: @     0x7f0837e659fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m [2025-08-07 17:02:57,322 E 3774 3774] logging.cc:496:     @     0x7f0837e11520  (unknown)  (unknown)
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m Stack (most recent call first):
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3774, ip=10.164.2.244)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3584, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3584, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3584, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3584, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4345, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4345, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4345, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4345, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4345, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4345, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4069, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4069, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4069, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4069, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4069, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4069, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3579, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3579, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3579, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m 2025-08-07 17:02:57.315498: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4163, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3872, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3872, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3872, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4066, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4066, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3874, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3874, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3874, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3778, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3778, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3855, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3855, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3855, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3855, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3855, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3855, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4260, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4260, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3966, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3966, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4595, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4595, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4595, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4595, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4595, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4595, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4524, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4524, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=4524, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4524, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4524, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4524, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 5bbeeb2553451f0c4998013a41be5cc200cfc4e00c000000 Worker ID: 7cdba63dbfa22f20cfb51bb0e0fef5deeca0756bed8d06e3a2d02ccc Node ID: 6389fb69c4e90234b014aaf6fe31ef93f9e9445b0da5c83e10274d7b Worker IP address: 10.164.2.229 Worker port: 10022 Worker PID: 3871 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 8x across cluster][0m
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m 2025-08-07 17:02:57.317870: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m The first task at the barrier: /job:jax_worker/replica:0/task:25. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m *** SIGABRT received at time=1754611377 on cpu 11 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m PC: @     0x7ff3dcd0a9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m     @     0x7ff3dccb6520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m [2025-08-07 17:02:57,323 E 3871 3871] logging.cc:496: *** SIGABRT received at time=1754611377 on cpu 11 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m [2025-08-07 17:02:57,323 E 3871 3871] logging.cc:496: PC: @     0x7ff3dcd0a9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m [2025-08-07 17:02:57,323 E 3871 3871] logging.cc:496:     @     0x7ff3dccb6520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=3871, ip=10.164.2.229)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 9c8e6598d16d88f825fc3ee70c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '4', '26', '3', '2', '6', '9', '22', '13', '10', '17', '1', '18', '23', '21', '27']
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '4', '26', '3', '2', '6', '9', '22', '13', '10', '17', '1', '18', '23', '21', '27']
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 4 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 26 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 1 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=26730, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 49
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m 2025-08-07 17:07:44.118514: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m *** SIGABRT received at time=1754611664 on cpu 18 ***
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m PC: @     0x7f72226679fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m     @     0x7f7222613520  (unknown)  (unknown)
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m [2025-08-07 17:07:44,124 E 2414 2414] logging.cc:496: *** SIGABRT received at time=1754611664 on cpu 18 ***
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m [2025-08-07 17:07:44,124 E 2414 2414] logging.cc:496: PC: @     0x7f72226679fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m [2025-08-07 17:07:44,124 E 2414 2414] logging.cc:496:     @     0x7f7222613520  (unknown)  (unknown)
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m Stack (most recent call first):
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, 
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m charset_normalizer.md
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m , uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, 
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m scipy._cyutility
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m , scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2411, ip=10.164.1.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2414, ip=10.164.1.84)[0m , sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2221, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2221, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2221, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2221, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2221, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2221, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m 2025-08-07 17:07:44.116197: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2143, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2411, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2411, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2411, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2411, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2411, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2411, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2411, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2411, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2411, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2219, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2219, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2219, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2413, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2413, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2607, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2607, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2607, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2607, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2607, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2607, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2414, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2414, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2412, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2412, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2416, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2416, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2416, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2416, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2416, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2416, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2415, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2415, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2415, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2415, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2415, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2415, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2606, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2606, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2414, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2414, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2414, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 84f0c36a3d503f40f7979022da53935ec75f22ea0c000000 Worker ID: 490c7b59d308004d0bfe20d31dc2b9382a3ea0080eb26b499a969cb8 Node ID: e71003f557d91e4f4d6e7cb58e7b6dcb8cf33e493fc96048231b820f Worker IP address: 10.164.2.208 Worker port: 10009 Worker PID: 2143 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Worker crashed
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Preempted 21 times. Continuing to retry.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 21
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Futures for slice ray-marin-eu-west4-worker-eb03a5e7-tpu: [ObjectRef(e6a088c3c4ef0854ffffffffffffffffffffffff0c00000001000000), ObjectRef(73dba364df601b2dffffffffffffffffffffffff0c00000001000000), ObjectRef(92f790bc742838a3ffffffffffffffffffffffff0c00000001000000), ObjectRef(8d8b1932b155221fffffffffffffffffffffffff0c00000001000000), ObjectRef(17588035c7924684ffffffffffffffffffffffff0c00000001000000), ObjectRef(1be5ffa0bd0bd2d3ffffffffffffffffffffffff0c00000001000000), ObjectRef(904b1a70f4709925ffffffffffffffffffffffff0c00000001000000), ObjectRef(e9f3474d0e6d468dffffffffffffffffffffffff0c00000001000000), ObjectRef(ab259f0e922c381effffffffffffffffffffffff0c00000001000000), ObjectRef(46c9ee8aea445c85ffffffffffffffffffffffff0c00000001000000), ObjectRef(f2a8a6e23503adabffffffffffffffffffffffff0c00000001000000), ObjectRef(8ddc417e31dbbdfbffffffffffffffffffffffff0c00000001000000), ObjectRef(ee8301a17a95aaa1ffffffffffffffffffffffff0c00000001000000), ObjectRef(a5cc725da66698cdffffffffffffffffffffffff0c00000001000000), ObjectRef(7c9e20fe0ae927eeffffffffffffffffffffffff0c00000001000000), ObjectRef(3840d49a151346f3ffffffffffffffffffffffff0c00000001000000)]
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m 2025-08-07 17:07:44.117127: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m *** SIGABRT received at time=1754611664 on cpu 58 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m PC: @     0x7f35d4e159fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m     @     0x7f35d4dc1520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m [2025-08-07 17:07:44,122 E 2412 2412] logging.cc:496: *** SIGABRT received at time=1754611664 on cpu 58 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m [2025-08-07 17:07:44,122 E 2412 2412] logging.cc:496: PC: @     0x7f35d4e159fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m [2025-08-07 17:07:44,123 E 2412 2412] logging.cc:496:     @     0x7f35d4dc1520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2412, ip=10.164.2.214)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 14x across cluster][0m
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 12x across cluster][0m
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, 484ae9aec72bd7add0f8d7e90c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '15', '3', '17', '24', '5', '2', '29', '27', '14', '30', '16', '31', '21', '11', '8']
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '15', '3', '17', '24', '5', '2', '29', '27', '14', '30', '16', '31', '21', '11', '8']
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 29 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 14 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 11 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=27257, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 50
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool actor Actor(SliceActor, f5f479b4d1578e3313aa3e7a0c000000) failed to start: Get timed out: some object(s) not ready.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 273, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     actor_info: ActorInfoT = ray.get(actor_info_awaitable, timeout=_START_ACTOR_TIMEOUT)  # TODO: make this overridable
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 904, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ] = self.core_worker.get_objects(
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/_raylet.pyx", line 3197, in ray._raylet.CoreWorker.get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "python/ray/includes/common.pxi", line 85, in ray._raylet.check_status
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.GetTimeoutError: Get timed out: some object(s) not ready.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '31', '2', '18', '25', '22', '3', '30', '28', '16', '5', '8', '15', '13', '23', '6']
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before draining: ['0', '31', '2', '18', '25', '22', '3', '30', '28', '16', '5', '8', '15', '13', '23', '6']
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 31 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 25 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 22 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 3 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 16 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 8 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 15 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 23 stopping.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 0 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failed to prune dead slices or create new actors
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 534, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     slice_pool_manager.scale_actor_pool(num_slices)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 289, in scale_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     self._add_members_to_actor_pool(desired_num_actors)  # TODO: Clean this up
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 285, in _add_members_to_actor_pool
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise Exception(f"Wanted {desired_num_actors} hosts in slice {self.get_actor_pool_name()} but only acquired {len(self._actor_pool)} hosts.")
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Exception: Wanted 1 hosts in slice v5litepod-128 slices but only acquired 0 hosts.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 51
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: []
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 0 members, but we want 1. Creating more.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool waiting for 1 new actors to start...
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 stopping.
[36m(SliceActor pid=27784, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool drained.
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m 2025-08-07 17:33:09.405851: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m *** SIGABRT received at time=1754613189 on cpu 67 ***
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m PC: @     0x7fdd1364c9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m     @     0x7fdd135f8520  (unknown)  (unknown)
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m [2025-08-07 17:33:09,411 E 3967 3967] logging.cc:496: *** SIGABRT received at time=1754613189 on cpu 67 ***
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m [2025-08-07 17:33:09,411 E 3967 3967] logging.cc:496: PC: @     0x7fdd1364c9fc  (unknown)  pthread_kill
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m [2025-08-07 17:33:09,411 E 3967 3967] logging.cc:496:     @     0x7fdd135f8520  (unknown)  (unknown)
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m Stack (most recent call first):
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3779, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3779, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3779, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4346, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4346, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4346, ip=10.164.2.234)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 5d8981bed4c876bb5372055db586a4916a17e8490c000000 Worker ID: 4033b3d2a5975e48753ba2c74d2860ab9cab6e526d6db2f8995e1f58 Node ID: c486bda5304c7e6dab8fc7f779f6d75f5c447a93a27e1e44bbcd922b Worker IP address: 10.164.2.231 Worker port: 10023 Worker PID: 3967 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4261, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4596, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4596, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4596, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m 
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before removing unhealthy members: []
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after unhealthy members: []
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members before adding members: []
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool has 0 members, but we want 16. Creating more.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool waiting for 16 new actors to start...
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 0 started.
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m 2025-08-07 17:33:10.858027: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m *** SIGABRT received at time=1754613190 on cpu 45 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m PC: @     0x7f288c66a9fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m     @     0x7f288c616520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m [2025-08-07 17:33:10,863 E 4262 4262] logging.cc:496: *** SIGABRT received at time=1754613190 on cpu 45 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m [2025-08-07 17:33:10,863 E 4262 4262] logging.cc:496: PC: @     0x7f288c66a9fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m [2025-08-07 17:33:10,863 E 4262 4262] logging.cc:496:     @     0x7f288c616520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m Fatal Python error: Aborted[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m Stack (most recent call first):[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 35x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4262, ip=10.164.2.246)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 7x across cluster][0m
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 30 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 20 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 21 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 17 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 12 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 10 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 18 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 9 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 28 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 27 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 24 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 13 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 5 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 6 started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool member ray-marin-eu-west4-worker-bbd095cf-tpu started.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after adding members: ['ray-marin-eu-west4-worker-bbd095cf-tpu']
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool scaled up to 1 members
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool member 2 started.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool members after adding members: ['0', '30', '20', '21', '17', '12', '10', '18', '9', '28', '27', '24', '13', '5', '6', '2']
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m slice ray-marin-eu-west4-worker-bbd095cf-tpu actor pool scaled up to 16 members
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Futures for slice ray-marin-eu-west4-worker-bbd095cf-tpu: [ObjectRef(aa18e7272434f6e8ffffffffffffffffffffffff0c00000001000000), ObjectRef(ec5c9c283c77e858ffffffffffffffffffffffff0c00000001000000), ObjectRef(9cd73b2167dfbd70ffffffffffffffffffffffff0c00000001000000), ObjectRef(939d095d72a68d4cffffffffffffffffffffffff0c00000001000000), ObjectRef(a415e92353e9e4fdffffffffffffffffffffffff0c00000001000000), ObjectRef(58d56a1f0e4a078cffffffffffffffffffffffff0c00000001000000), ObjectRef(d6a03de385c09fb4ffffffffffffffffffffffff0c00000001000000), ObjectRef(ff83e8f76ab4c7deffffffffffffffffffffffff0c00000001000000), ObjectRef(eca3dc8904cabceeffffffffffffffffffffffff0c00000001000000), ObjectRef(277993619a971b63ffffffffffffffffffffffff0c00000001000000), ObjectRef(a73f5abe6340d4c2ffffffffffffffffffffffff0c00000001000000), ObjectRef(b569d265af088bc2ffffffffffffffffffffffff0c00000001000000), ObjectRef(b7f270093a992511ffffffffffffffffffffffff0c00000001000000), ObjectRef(e6ac1e547f06cde1ffffffffffffffffffffffff0c00000001000000), ObjectRef(4b34ab0d67eb8b79ffffffffffffffffffffffff0c00000001000000), ObjectRef(7dff8e64c2a355fbffffffffffffffffffffffff0c00000001000000)]
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m 2025-08-07 17:37:55.982772: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m *** SIGABRT received at time=1754613475 on cpu 55 ***
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m PC: @     0x7f86f16d79fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m     @     0x7f86f1683520  (unknown)  (unknown)
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m [2025-08-07 17:37:55,988 E 2607 2607] logging.cc:496: *** SIGABRT received at time=1754613475 on cpu 55 ***
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m [2025-08-07 17:37:55,988 E 2607 2607] logging.cc:496: PC: @     0x7f86f16d79fc  (unknown)  pthread_kill
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m [2025-08-07 17:37:55,988 E 2607 2607] logging.cc:496:     @     0x7f86f1683520  (unknown)  (unknown)
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m Fatal Python error: Aborted
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m Stack (most recent call first):
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 150 in initialize
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py", line 276 in initialize
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/distributed.py", line 354 in initialize
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 777 in initialize
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.91)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2222, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2222, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2222, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2412, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2412, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2412, ip=10.164.1.231)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 642c1265bad8266111ba251496b8201e80bc656e0c000000 Worker ID: 0f150d4b7135baf3ae1c5bb5cb837112a9a566d7aad8c3e570059594 Node ID: 9324d786cd5c86c1bd3c41c24b6cb6076822d214f33adf5faba24291 Worker IP address: 10.164.1.96 Worker port: 10010 Worker PID: 2222 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2413, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m , 
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m sklearn.metrics._pairwise_distances_reduction._middle_term_computer
[36m(train_lm_task pid=2412, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.213)[0m , sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2605, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2605, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2605, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m 2025-08-07 18:03:06.012992: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m _csparsetools
[36m(train_lm_task pid=2607, ip=10.164.2.223)[0m 2025-08-07 17:37:58.103251: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.223)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m *** SIGABRT received at time=1754614986 on cpu 40 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m PC: @     0x7f364c59c9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m     @     0x7f364c548520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m [2025-08-07 18:03:06,018 E 4525 4525] logging.cc:496: *** SIGABRT received at time=1754614986 on cpu 40 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m [2025-08-07 18:03:06,018 E 4525 4525] logging.cc:496: PC: @     0x7f364c59c9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m [2025-08-07 18:03:06,018 E 4525 4525] logging.cc:496:     @     0x7f364c548520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2607, ip=10.164.2.223)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=4539, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4539, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4539, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4539, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4539, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4539, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4525, ip=10.164.2.240)[0m , _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=3856, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=3856, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=3856, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3856, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3856, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3856, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=3968, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=3968, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=3968, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3968, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3968, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3968, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=3777, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=3777, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=3777, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3777, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3777, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3777, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4789, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4789, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4789, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4789, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4789, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4789, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=3972, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=3972, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=3972, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3972, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3972, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3972, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4455, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4455, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4068, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4068, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4068, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4067, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4067, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4067, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4160, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4160, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4065, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4065, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4263, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4263, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4263, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4263, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4263, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4263, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=3772, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=3772, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3772, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m 2025-08-07 18:03:06.010841: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4369, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:24
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 16fc4b943a5594228be47899c7aec3dc152fd36d0c000000 Worker ID: 3b26ed84d40d4bc5519a82f93a18268feafd084638cec9887731be38 Node ID: 9f5d9292f00b4b33e1b11b3820a942102773ccd980e91b96635bd4b8 Worker IP address: 10.164.2.232 Worker port: 10022 Worker PID: 3972 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 8x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4c93d00480339cad72ece205e50860b9e0437aed0c000000 Worker ID: 37cfcddbd6f3efa8d9e1f88ca78ebc7bdf0c882b898435a62f431de9 Node ID: da36cb6cbedbfe03fc90eff08240dc46730bfae44de057fb0dfe75a8 Worker IP address: 10.164.2.244 Worker port: 10023 Worker PID: 3968 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: recvmsg:Connection reset by peer; RPC Error details: 
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Worker crashed
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Preempted 5 times. Continuing to retry.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 5
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(b6a39a71732a778fffffffffffffffffffffffff0c00000001000000), ObjectRef(cb2015dcfd678eafffffffffffffffffffffffff0c00000001000000), ObjectRef(d7ebbd6c5ae670b2ffffffffffffffffffffffff0c00000001000000), ObjectRef(72f4b83d2268d649ffffffffffffffffffffffff0c00000001000000), ObjectRef(f2da3ed5c91962f3ffffffffffffffffffffffff0c00000001000000), ObjectRef(81bc1e7828dfaf6fffffffffffffffffffffffff0c00000001000000), ObjectRef(361e4d27acee9e84ffffffffffffffffffffffff0c00000001000000), ObjectRef(1f9031d47161ffd8ffffffffffffffffffffffff0c00000001000000), ObjectRef(9ad1ab1a4f28d332ffffffffffffffffffffffff0c00000001000000), ObjectRef(3b4cd8c91a545da5ffffffffffffffffffffffff0c00000001000000), ObjectRef(54dd4cbceb4c6e6affffffffffffffffffffffff0c00000001000000), ObjectRef(a93d8e74aac551d7ffffffffffffffffffffffff0c00000001000000), ObjectRef(320ca2ee0dbdefc3ffffffffffffffffffffffff0c00000001000000), ObjectRef(cb6cb47fe11c10f7ffffffffffffffffffffffff0c00000001000000), ObjectRef(9fbefe72c93298e9ffffffffffffffffffffffff0c00000001000000), ObjectRef(707760608e4e3029ffffffffffffffffffffffff0c00000001000000)]
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m 2025-08-07 18:03:06.012824: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4064, ip=10.164.2.229)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m 2025-08-07 18:05:52.722688: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m *** SIGABRT received at time=1754615152 on cpu 29 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m PC: @     0x7f946f6b09fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m     @     0x7f946f65c520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m [2025-08-07 18:05:52,728 E 11090 11090] logging.cc:496: *** SIGABRT received at time=1754615152 on cpu 29 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m [2025-08-07 18:05:52,728 E 11090 11090] logging.cc:496: PC: @     0x7f946f6b09fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m [2025-08-07 18:05:52,728 E 11090 11090] logging.cc:496:     @     0x7f946f65c520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11090, ip=10.164.2.209)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(TPUHostActor pid=2322, ip=10.164.2.244)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 19x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: f2906ad35627a06dfc7d77c2a065214f029fe7eb0c000000 Worker ID: e783c6deb005c1c03c49937c4abfe578cea9b0a9fb02ceb38c897d88 Node ID: eb41f40c8018171c340b5a43142dad9d8c47e877a4f312b85c46a7d6 Worker IP address: 10.164.2.209 Worker port: 10025 Worker PID: 11090 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14569, ip=10.164.2.73)[0m 2025-08-07 18:05:52.913909: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=14569, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14569, ip=10.164.2.73)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=14569, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14569, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=17614, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17614, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17614, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=12832, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=12832, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=12832, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13700, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=13700, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=13700, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=12830, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=12830, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=12830, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=15005, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15005, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15005, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m 2025-08-07 18:07:51.931542: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m *** SIGABRT received at time=1754615271 on cpu 23 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m PC: @     0x7fec931c69fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m     @     0x7fec93172520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m [2025-08-07 18:05:53,275 E 16309 16309] logging.cc:496: *** SIGABRT received at time=1754615153 on cpu 7 ***[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m [2025-08-07 18:05:53,275 E 16309 16309] logging.cc:496: PC: @     0x7fec931c69fc  (unknown)  pthread_kill[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m [2025-08-07 18:05:53,276 E 16309 16309] logging.cc:496:     @     0x7fec93172520  (unknown)  (unknown)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m Fatal Python error: Aborted[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m Stack (most recent call first):[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 35x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m 2025-08-07 18:05:53.269830: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=16309, ip=10.164.2.217)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m 2025-08-07 18:07:51.931282: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2608, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2608, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2606, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2606, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2607, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2607, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2608, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2608, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2608, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2608, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2608, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2413, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2413, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2413, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2412, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2412, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2412, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2605, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2605, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2605, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2605, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2605, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2605, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2609, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2609, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2609, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2604, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2604, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2604, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2604, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2604, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2604, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2415, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2415, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2606, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2606, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2800, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2800, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 2025-08-07 18:07:51.930792: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:30
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:7
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2415, ip=10.164.1.96)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 3568bcd95e43e69304168bc8258d85458e95158e0c000000 Worker ID: 9a9e1183e5fd07839ba18bd117e2777614ac753449846447c72e2e71 Node ID: 6bf7c75aac3437b9b002acfc6064c734914b3b5dfb1a36c111ecaa9d Worker IP address: 10.164.2.223 Worker port: 10013 Worker PID: 2606 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m *** SIGABRT received at time=1754616797 on cpu 103 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m PC: @     0x7f9141a9d9fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m     @     0x7f9141a49520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m [2025-08-07 18:33:17,841 E 4790 4790] logging.cc:496: *** SIGABRT received at time=1754616797 on cpu 103 ***[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m [2025-08-07 18:33:17,841 E 4790 4790] logging.cc:496: PC: @     0x7f9141a9d9fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m [2025-08-07 18:33:17,841 E 4790 4790] logging.cc:496:     @     0x7f9141a49520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m Fatal Python error: Aborted[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m Stack (most recent call first):[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 85x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m 2025-08-07 18:33:17.835500: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=4790, ip=10.164.2.236)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m 2025-08-07 18:07:51.932579: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2348, ip=10.164.2.208)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=4161, ip=10.164.2.231)[0m 2025-08-07 18:33:17.857858: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=4161, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.231)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=4161, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4540, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4540, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4540, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3773, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3778, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3973, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3973, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=3973, ip=10.164.2.232)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 41c5a8a3f138c0a4c3b1bcaa0a8941de0eba75f50c000000 Worker ID: d5cc495e0f1d89a16639d830aa65671738ebf5194f29264fa18f9537 Node ID: 26473405fdb18962befcba7ecbd64534732cd01c6fe71de7208eac27 Worker IP address: 10.164.2.242 Worker port: 10021 Worker PID: 3773 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4456, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4456, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4456, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m , multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m *** SIGABRT received at time=1754616948 on cpu 2 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m PC: @     0x7f817fb3b9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m     @     0x7f817fae7520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m [2025-08-07 18:33:18,616 E 4719 4719] logging.cc:496: *** SIGABRT received at time=1754616798 on cpu 41 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m [2025-08-07 18:33:18,616 E 4719 4719] logging.cc:496: PC: @     0x7f817fb3b9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m [2025-08-07 18:33:18,616 E 4719 4719] logging.cc:496:     @     0x7f817fae7520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4161, ip=10.164.2.244)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m 2025-08-07 18:35:48.689807: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m 2025-08-07 18:33:18.610996: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4719, ip=10.164.2.240)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m 2025-08-07 18:35:48.687984: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m , 
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m pandas._libs.tslibs.period
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py"
[36m(train_lm_task pid=12833, ip=10.164.2.160)[0m , pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m , line 276 in initialize
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite
[36m(train_lm_task pid=13700, ip=10.164.2.151)[0m , sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=14570, ip=10.164.2.73)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=14570, ip=10.164.2.73)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=14570, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14570, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14570, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14570, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=10655, ip=10.164.2.101)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=10655, ip=10.164.2.101)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=10655, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10655, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10655, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10655, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=11091, ip=10.164.2.209)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=11091, ip=10.164.2.209)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=11091, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11091, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11091, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11091, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=15007, ip=10.164.1.230)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=15007, ip=10.164.1.230)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=15007, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15007, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15007, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15007, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=16310, ip=10.164.2.217)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=16310, ip=10.164.2.217)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=16310, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16310, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16310, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16310, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=13702, ip=10.164.1.220)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=13702, ip=10.164.1.220)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=13702, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=13702, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=13702, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=13702, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=7176, ip=10.164.1.200)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=7176, ip=10.164.1.200)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=7176, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7176, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7176, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7176, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=13701, ip=10.164.1.246)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=13701, ip=10.164.1.246)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=13701, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=13701, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=13701, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=13701, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=12829, ip=10.164.2.102)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=12829, ip=10.164.2.102)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=12829, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=12829, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=12829, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=12829, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.76)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=14570, ip=10.164.1.76)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=14570, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=17615, ip=10.164.2.148)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=17615, ip=10.164.2.148)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=17615, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17615, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17615, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17615, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m 2025-08-07 18:35:48.687354: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28495, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m /job:jax_worker/replica:0/task:29
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m /job:jax_worker/replica:0/task:31
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14570, ip=10.164.1.223)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 57b0f3e0a226d7c8b25a814d4650bacbfe4322dd0c000000 Worker ID: 9e7563b271ac1e9c4019cd3629c55042141ce4ee8c5c0bcdc1235c27 Node ID: 8ad2e720bb0e9736f3f7bdecbb1816791f5c41b6933e6f876605b460 Worker IP address: 10.164.2.102 Worker port: 10030 Worker PID: 12829 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m *** SIGABRT received at time=1754617083 on cpu 31 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m PC: @     0x7f8da77c49fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m     @     0x7f8da7770520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m [2025-08-07 18:38:03,889 E 2608 2608] logging.cc:496: *** SIGABRT received at time=1754617083 on cpu 31 ***[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m [2025-08-07 18:38:03,890 E 2608 2608] logging.cc:496: PC: @     0x7f8da77c49fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m [2025-08-07 18:38:03,890 E 2608 2608] logging.cc:496:     @     0x7f8da7770520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m Fatal Python error: Aborted[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m Stack (most recent call first):[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 84x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m 2025-08-07 18:38:03.884192: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2608, ip=10.164.2.166)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m 2025-08-07 18:35:48.688049: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=15006, ip=10.164.1.225)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m 2025-08-07 18:38:03.911045: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m , sklearn.metrics._pairwise_distances_reduction._middle_term_computer
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m , sklearn.utils._heap, 
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m sklearn.utils._sorting
[36m(train_lm_task pid=2605, ip=10.164.3.7)[0m , sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2607, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.93)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 4894bbf5c3f4955196fc44154a6042cee9b4c1460c000000 Worker ID: 689b2db35a959b2f679c932a1f9811bf7f6988e13aee08815658b762 Node ID: a4cd1da26a84e5e12cecd974d2acd594e800c56b5d3773dfc963fcb7 Worker IP address: 10.164.2.166 Worker port: 10014 Worker PID: 2608 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2609, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2609, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2609, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2606, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2606, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2606, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2605, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2605, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2605, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m *** SIGABRT received at time=1754618593 on cpu 24 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m PC: @     0x7f976c3369fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m     @     0x7f976c2e2520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m [2025-08-07 19:03:13,647 E 4260 4260] logging.cc:496: *** SIGABRT received at time=1754618593 on cpu 24 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m [2025-08-07 19:03:13,647 E 4260 4260] logging.cc:496: PC: @     0x7f976c3369fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m [2025-08-07 19:03:13,648 E 4260 4260] logging.cc:496:     @     0x7f976c2e2520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m Fatal Python error: Aborted[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m Stack (most recent call first):[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 45x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m 2025-08-07 19:03:13.642209: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4260, ip=10.164.2.253)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2799, ip=10.164.2.214)[0m 2025-08-07 18:38:05.345859: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2799, ip=10.164.2.214)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m 2025-08-07 19:03:13.642196: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4049, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4066, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4066, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4066, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4983, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4983, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4983, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4983, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4983, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4983, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4649, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4649, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4649, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4649, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4649, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4649, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4261, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4261, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=3971, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3971, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=3971, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3971, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3971, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3971, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4166, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4166, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m 2025-08-07 19:03:13.640111: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4370, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4720, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4720, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4720, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4720, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4720, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4720, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=3966, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=3966, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3966, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4354, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4354, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4354, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4354, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4354, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4354, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4065, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4065, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4065, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4733, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4733, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4733, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4733, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4733, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4733, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4162, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4162, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4162, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:27
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: f8bdef3becfab1b4d988097c9a0228c9db5750be0c000000 Worker ID: e280bb02ca2d62389861dc3c78ec16add5914f4ad1eb8cedec982448 Node ID: 08b55f819516cb8d2b906c83fccbcc2d9275f51d8cb107727892c05d Worker IP address: 10.164.2.247 Worker port: 10025 Worker PID: 4261 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m *** SIGABRT received at time=1754618760 on cpu 57 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m PC: @     0x7f9935c7b9fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m     @     0x7f9935c27520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m [2025-08-07 19:06:00,980 E 10656 10656] logging.cc:496: *** SIGABRT received at time=1754618760 on cpu 57 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m [2025-08-07 19:06:00,980 E 10656 10656] logging.cc:496: PC: @     0x7f9935c7b9fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m [2025-08-07 19:06:00,980 E 10656 10656] logging.cc:496:     @     0x7f9935c27520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m 2025-08-07 19:06:00.974959: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=10656, ip=10.164.2.101)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m 2025-08-07 19:03:13.642655: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4456, ip=10.164.2.246)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 766d58aa583f48d30411551b82cdd389e7e608f70c000000 Worker ID: 7a23720652e7e26c77e0898cbdd6ec831207d7fa3c06ea71bf0f7bee Node ID: bc57a5129e00ac2b9202fed9ae396211286d29d1a8f6559389c356b4 Worker IP address: 10.164.2.101 Worker port: 10025 Worker PID: 10656 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=13703, ip=10.164.1.220)[0m 2025-08-07 19:06:01.126252: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=13703, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=13703, ip=10.164.1.220)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=13703, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=13703, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=15008, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15008, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15008, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=7177, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7177, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7177, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=14571, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14571, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14571, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=13701, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=13701, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=13701, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14571, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14571, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14571, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14010, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14010, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14010, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=13142, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13142, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13142, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=14879, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14879, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14879, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=13139, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13139, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13139, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m *** SIGABRT received at time=1754618879 on cpu 23 ***[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m PC: @     0x7f1b3a9059fc  (unknown)  pthread_kill[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m     @     0x7f1b3a8b1520  (unknown)  (unknown)[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m [2025-08-07 19:07:59,191 E 2416 2416] logging.cc:496: *** SIGABRT received at time=1754618879 on cpu 23 ***[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m [2025-08-07 19:07:59,191 E 2416 2416] logging.cc:496: PC: @     0x7f1b3a9059fc  (unknown)  pthread_kill[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m [2025-08-07 19:07:59,191 E 2416 2416] logging.cc:496:     @     0x7f1b3a8b1520  (unknown)  (unknown)[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m Fatal Python error: Aborted[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m Stack (most recent call first):[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 55x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m 2025-08-07 19:07:59.185543: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2416, ip=10.164.1.96)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=13139, ip=10.164.2.102)[0m 2025-08-07 19:06:02.068223: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=13139, ip=10.164.2.102)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m 2025-08-07 19:07:59.184127: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2801, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2801, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2801, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2801, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2606, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2606, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2606, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2607, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2607, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2607, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2610, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2610, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2610, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2799, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2799, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2799, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2799, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2799, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2799, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2800, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2800, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2800, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2800, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2994, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2994, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2802, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2802, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2798, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2798, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2798, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2798, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2798, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2798, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m 2025-08-07 19:07:59.183520: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2349, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2801, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2801, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2801, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2801, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2801, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2801, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:21
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:9
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 156073751b63ccbd0bef8486aea2a6c96ffb63730c000000 Worker ID: a5e80f0781499610b7452ac847dbf17a1999353ff60b2de489b779a0 Node ID: e71003f557d91e4f4d6e7cb58e7b6dcb8cf33e493fc96048231b820f Worker IP address: 10.164.2.208 Worker port: 10011 Worker PID: 2349 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 11x across cluster][0m
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=1417, ip=10.164.2.208)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Worker crashed
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Preempted 22 times. Continuing to retry.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m     raise value
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Running on 1 x TPU v5litepod-128. Attempt 22
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-eb03a5e7-tpu']
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Futures for slice ray-marin-eu-west4-worker-eb03a5e7-tpu: [ObjectRef(c93915a597082be8ffffffffffffffffffffffff0c00000001000000), ObjectRef(760e38bb9eeae9faffffffffffffffffffffffff0c00000001000000), ObjectRef(6b74b0b3db94a341ffffffffffffffffffffffff0c00000001000000), ObjectRef(ebc3e3e708cd8607ffffffffffffffffffffffff0c00000001000000), ObjectRef(b63f919f5ded96c4ffffffffffffffffffffffff0c00000001000000), ObjectRef(24c73bdfaf9145deffffffffffffffffffffffff0c00000001000000), ObjectRef(52e699b248237b2cffffffffffffffffffffffff0c00000001000000), ObjectRef(15e4930e14c62f57ffffffffffffffffffffffff0c00000001000000), ObjectRef(bcd14d3076f52b58ffffffffffffffffffffffff0c00000001000000), ObjectRef(6960bd4f8ac350e4ffffffffffffffffffffffff0c00000001000000), ObjectRef(73bc76584ecde7c4ffffffffffffffffffffffff0c00000001000000), ObjectRef(a5e31bd7439eec8cffffffffffffffffffffffff0c00000001000000), ObjectRef(104c10e42ce8c296ffffffffffffffffffffffff0c00000001000000), ObjectRef(b4f4a74b9a0ea184ffffffffffffffffffffffff0c00000001000000), ObjectRef(963483dcef876cebffffffffffffffffffffffff0c00000001000000), ObjectRef(20376f8ae009d9f6ffffffffffffffffffffffff0c00000001000000)]
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m *** SIGABRT received at time=1754620405 on cpu 18 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m PC: @     0x7fa30b1c99fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m     @     0x7fa30b175520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m [2025-08-07 19:33:25,662 E 4650 4650] logging.cc:496: *** SIGABRT received at time=1754620405 on cpu 18 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m [2025-08-07 19:33:25,662 E 4650 4650] logging.cc:496: PC: @     0x7fa30b1c99fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m [2025-08-07 19:33:25,662 E 4650 4650] logging.cc:496:     @     0x7fa30b175520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m 2025-08-07 19:33:25.656814: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=4650, ip=10.164.2.235)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m 2025-08-07 19:07:59.185821: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2608, ip=10.164.1.84)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(run_on_pod_ray pid=2940, ip=10.164.2.240)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=3972, ip=10.164.2.252)[0m 2025-08-07 19:33:25.663381: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=3972, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3972, ip=10.164.2.252)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=3972, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=3972, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4167, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4167, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4167, ip=10.164.2.232)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fd3b4659225e4d97f9da7fa6b53cda1941e813720c000000 Worker ID: d6249ff5c89e8f389cd60908ce69510f8beb30a1aab453ad6ab4be2d Node ID: a3d125607a4511ba998e68a8cddb165a669e924aacb773c1b5a7a3d5 Worker IP address: 10.164.2.252 Worker port: 10023 Worker PID: 3972 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4984, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4984, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4984, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4261, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4050, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4050, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4050, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m *** SIGABRT received at time=1754620556 on cpu 55 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m PC: @     0x7fa7e34909fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m     @     0x7fa7e343c520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m [2025-08-07 19:35:56,354 E 15317 15317] logging.cc:496: *** SIGABRT received at time=1754620556 on cpu 55 ***[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m [2025-08-07 19:35:56,354 E 15317 15317] logging.cc:496: PC: @     0x7fa7e34909fc  (unknown)  pthread_kill[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m [2025-08-07 19:35:56,355 E 15317 15317] logging.cc:496:     @     0x7fa7e343c520  (unknown)  (unknown)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m Fatal Python error: Aborted[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m Stack (most recent call first):[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 30x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m 2025-08-07 19:35:56.349038: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=15317, ip=10.164.1.230)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4050, ip=10.164.2.241)[0m 2025-08-07 19:33:25.997679: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=4050, ip=10.164.2.241)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 4x across cluster][0m
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 2025-08-07 19:35:56.348456: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 2025-08-07 19:35:56.350546: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28496, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=10965, ip=10.164.2.101)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=10965, ip=10.164.2.101)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=10965, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10965, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10965, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10965, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=14010, ip=10.164.2.151)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=14010, ip=10.164.2.151)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=14010, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14010, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14010, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14010, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=16619, ip=10.164.2.217)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=16619, ip=10.164.2.217)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=16619, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16619, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16619, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16619, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=11400, ip=10.164.2.209)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=11400, ip=10.164.2.209)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=11400, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11400, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11400, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11400, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=13143, ip=10.164.2.160)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=13143, ip=10.164.2.160)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=13143, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13143, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13143, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13143, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=17925, ip=10.164.2.148)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=17925, ip=10.164.2.148)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=17925, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17925, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17925, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17925, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=13140, ip=10.164.2.102)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=13140, ip=10.164.2.102)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=13140, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13140, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13140, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13140, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=15315, ip=10.164.1.225)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=15315, ip=10.164.1.225)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=15315, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15315, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15315, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15315, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=14880, ip=10.164.2.73)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=14880, ip=10.164.2.73)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=14880, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14880, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14880, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=14880, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=7486, ip=10.164.1.200)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=7486, ip=10.164.1.200)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=7486, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7486, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7486, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7486, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=14011, ip=10.164.1.246)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=14011, ip=10.164.1.246)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=14011, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14011, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14011, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14011, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.76)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=14880, ip=10.164.1.76)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=14880, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14012, ip=10.164.1.220)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=14012, ip=10.164.1.220)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=14012, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14012, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14012, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14012, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m /job:jax_worker/replica:0/task:16
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m /job:jax_worker/replica:0/task:23
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 6ff2c0daa433d0fea9a5cd99ee3bfbcc61a3fc990c000000 Worker ID: 951a1648f1646deff2d658af90b3fbe8866e9a17fc10ed8904fc7836 Node ID: 07ab08d7958d84ea05333b1a6f0538afda2ac339e661c4b67cdf28aa Worker IP address: 10.164.2.148 Worker port: 10042 Worker PID: 17925 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 6x across cluster][0m
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(SliceActor pid=28311, ip=10.164.2.83)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Worker crashed
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise value
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Preempted 52 times. Continuing to retry.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m     raise value
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Running on 1 x TPU v5litepod-128. Attempt 52
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-bbd095cf-tpu']
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-bbd095cf-tpu']
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-bbd095cf-tpu']
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Futures for slice ray-marin-eu-west4-worker-bbd095cf-tpu: [ObjectRef(2f87e8a15a736cfeffffffffffffffffffffffff0c00000001000000), ObjectRef(f7c1eab41e0f37fcffffffffffffffffffffffff0c00000001000000), ObjectRef(7eb098339a468afcffffffffffffffffffffffff0c00000001000000), ObjectRef(565d02f8224c420effffffffffffffffffffffff0c00000001000000), ObjectRef(48253881b0c253acffffffffffffffffffffffff0c00000001000000), ObjectRef(80f7c3689b9d727fffffffffffffffffffffffff0c00000001000000), ObjectRef(5dc3e29a0799de4dffffffffffffffffffffffff0c00000001000000), ObjectRef(4e8731663d2092dcffffffffffffffffffffffff0c00000001000000), ObjectRef(3ffedbfe6672df1affffffffffffffffffffffff0c00000001000000), ObjectRef(3bf88f070084c509ffffffffffffffffffffffff0c00000001000000), ObjectRef(d45e57cdc0007e3dffffffffffffffffffffffff0c00000001000000), ObjectRef(35f75f18377b33d7ffffffffffffffffffffffff0c00000001000000), ObjectRef(fd8ab344c616cb85ffffffffffffffffffffffff0c00000001000000), ObjectRef(fa1826aa0cff4594ffffffffffffffffffffffff0c00000001000000), ObjectRef(4275d854a4f6cdccffffffffffffffffffffffff0c00000001000000), ObjectRef(606fdf308bb6a043ffffffffffffffffffffffff0c00000001000000)]
[36m(train_lm_task pid=2802, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.166)[0m *** SIGABRT received at time=1754620690 on cpu 83 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m PC: @     0x7fc1511be9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m     @     0x7fc15116a520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m [2025-08-07 19:35:56,354 E 14880 14880] logging.cc:496: *** SIGABRT received at time=1754620556 on cpu 18 ***[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m [2025-08-07 19:35:56,354 E 14880 14880] logging.cc:496: PC: @     0x7fc1511be9fc  (unknown)  pthread_kill[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m [2025-08-07 19:35:56,354 E 14880 14880] logging.cc:496:     @     0x7fc15116a520  (unknown)  (unknown)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m Fatal Python error: Aborted[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m Stack (most recent call first):[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 75x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2802, ip=10.164.2.166)[0m 2025-08-07 19:38:10.998254: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2802, ip=10.164.2.166)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m 2025-08-07 19:35:56.349077: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=14880, ip=10.164.1.223)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(run_on_pod_ray pid=2660, ip=10.164.2.241)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2802, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.213)[0m 2025-08-07 19:38:11.061260: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2802, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.213)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2802, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2802, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2802, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2800, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2800, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2800, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2803, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2803, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2803, ip=10.164.3.26)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: dc73a55285b98cf5445d2cad0f752d262c6669cc0c000000 Worker ID: cd758df7f687ba33bd1ee26b0fa638d301fdfaa9e4278ff93fd1082c Node ID: a4cd1da26a84e5e12cecd974d2acd594e800c56b5d3773dfc963fcb7 Worker IP address: 10.164.2.166 Worker port: 10016 Worker PID: 2802 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2609, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2609, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2609, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m *** SIGABRT received at time=1754622202 on cpu 67 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m PC: @     0x7f45ddf7f9fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m     @     0x7f45ddf2b520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m [2025-08-07 19:38:11,104 E 2801 2801] logging.cc:496: *** SIGABRT received at time=1754620691 on cpu 36 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m [2025-08-07 19:38:11,104 E 2801 2801] logging.cc:496: PC: @     0x7f45ddf7f9fc  (unknown)  pthread_kill[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m [2025-08-07 19:38:11,104 E 2801 2801] logging.cc:496:     @     0x7f45ddf2b520  (unknown)  (unknown)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m Fatal Python error: Aborted[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m Stack (most recent call first):[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 45x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m 2025-08-07 20:03:22.141603: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m 2025-08-07 19:38:11.098441: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.93)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m 2025-08-07 20:03:22.140360: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4843, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4913, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4913, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4913, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4913, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4913, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4913, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4454, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4454, ip=10.164.2.253)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4454, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4454, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4454, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4454, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4244, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4244, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4244, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4244, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4244, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4244, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4259, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4259, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=3967, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3967, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=3967, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4262, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4262, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4262, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4457, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4457, ip=10.164.2.246)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4457, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4457, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4457, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4457, ip=10.164.2.246)[0m 
[36m(train_lm_task pid=4258, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4258, ip=10.164.2.229)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4258, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4258, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4258, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4258, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=5177, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=5177, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=5177, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5177, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5177, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5177, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4355, ip=10.164.2.244)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4355, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.244)[0m 
[36m(train_lm_task pid=4360, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4360, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4360, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4360, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4360, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4360, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4165, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4165, ip=10.164.2.252)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4165, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4165, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4165, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4165, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4734, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4734, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4734, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4734, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4734, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4734, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 2025-08-07 20:03:22.139457: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register'] [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:15
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4355, ip=10.164.2.231)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b94d2cddf414db29be0f728aa1b27d38ecda69fc0c000000 Worker ID: 92a586ae479f35e4b1cda02395ee73e849b5a13ae6661697dd3b69e3 Node ID: 26473405fdb18962befcba7ecbd64534732cd01c6fe71de7208eac27 Worker IP address: 10.164.2.242 Worker port: 10023 Worker PID: 3967 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 043b9a06f13aae6189073fe8d69e7a284e8356a80c000000 Worker ID: 3952cf0356baf5cfa72f7b9763bf1676539dd3b67cb89864ca3859d5 Node ID: cc8d81d3c80c107f9f031b95ab83677fcb0be2b48c44623b2e5a307e Worker IP address: 10.164.2.234 Worker port: 10031 Worker PID: 4734 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. RPC Error message: recvmsg:Connection reset by peer; RPC Error details: 
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Worker crashed
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Failure detected. Cancelling 15 futures.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m TPU job was cancelled, probably because something else failed.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Preempted 6 times. Continuing to retry.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Traceback (most recent call last):
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/infra/ray_tpu.py", line 579, in run_on_pod_ray
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     tpu_results[future_to_index[f]] = TpuSuccess(ray.get(f))
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                                  ^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return fn(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     return func(*args, **kwargs)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m     raise value
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Running on 1 x TPU v5litepod-128. Attempt 6
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before removing unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members after unhealthy members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool members before adding members: ['ray-marin-eu-west4-worker-f722b08f-tpu']
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m v5litepod-128 slices actor pool has 1 members, and we wanted 1. Skipping adding members.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Futures for slice ray-marin-eu-west4-worker-f722b08f-tpu: [ObjectRef(b7ef854b80f28b99ffffffffffffffffffffffff0c00000001000000), ObjectRef(14d638fefbd2c4a4ffffffffffffffffffffffff0c00000001000000), ObjectRef(db2a84e0c838d44bffffffffffffffffffffffff0c00000001000000), ObjectRef(4c1267af1fb7edebffffffffffffffffffffffff0c00000001000000), ObjectRef(449e39a29150deaaffffffffffffffffffffffff0c00000001000000), ObjectRef(86c50d2c0ef31328ffffffffffffffffffffffff0c00000001000000), ObjectRef(5ef2943532cbf90cffffffffffffffffffffffff0c00000001000000), ObjectRef(52e9b201a9f2bf0cffffffffffffffffffffffff0c00000001000000), ObjectRef(897ac515ecb22767ffffffffffffffffffffffff0c00000001000000), ObjectRef(284988a697ff8dc4ffffffffffffffffffffffff0c00000001000000), ObjectRef(b219e0bf32bce9c5ffffffffffffffffffffffff0c00000001000000), ObjectRef(fd7d79a68db8a815ffffffffffffffffffffffff0c00000001000000), ObjectRef(ea3aa58a5663849fffffffffffffffffffffffff0c00000001000000), ObjectRef(13fddf6095d8582affffffffffffffffffffffff0c00000001000000), ObjectRef(7dea6bfe7a6349eaffffffffffffffffffffffff0c00000001000000), ObjectRef(cecacc06eb62286dffffffffffffffffffffffff0c00000001000000)]
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(run_on_pod_ray pid=2955, ip=10.164.2.234)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m *** SIGABRT received at time=1754622368 on cpu 81 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m PC: @     0x7fb5b38f09fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m     @     0x7fb5b389c520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m [2025-08-07 20:06:08,131 E 11401 11401] logging.cc:496: *** SIGABRT received at time=1754622368 on cpu 81 ***[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m [2025-08-07 20:06:08,131 E 11401 11401] logging.cc:496: PC: @     0x7fb5b38f09fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m [2025-08-07 20:06:08,131 E 11401 11401] logging.cc:496:     @     0x7fb5b389c520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m Fatal Python error: Aborted[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m Stack (most recent call first):[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 85x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m 2025-08-07 20:06:08.125283: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=11401, ip=10.164.2.209)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m 2025-08-07 20:03:22.141400: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=4575, ip=10.164.2.250)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(SliceActor pid=3153, ip=10.164.2.250)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m 2025-08-07 20:06:08.201488: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers
[36m(train_lm_task pid=10966, ip=10.164.2.101)[0m , sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=17926, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17926, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=17926, ip=10.164.2.148)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 9477b70533bd500fca30f10ed11e013983cad29a0c000000 Worker ID: b66ec7995473dfac294e840351e095f52561672698289710f8090d1a Node ID: eb41f40c8018171c340b5a43142dad9d8c47e877a4f312b85c46a7d6 Worker IP address: 10.164.2.209 Worker port: 10028 Worker PID: 11401 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=7487, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7487, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7487, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=16620, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16620, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16620, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=14013, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14013, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14013, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=13333, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13333, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13333, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13333, ip=10.164.2.102)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs
[36m(train_lm_task pid=13333, ip=10.164.2.102)[0m , pyarrow._s3fs
[36m(train_lm_task pid=13333, ip=10.164.2.102)[0m , multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m *** SIGABRT received at time=1754622486 on cpu 31 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m PC: @     0x7f94323f59fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m     @     0x7f94323a1520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m [2025-08-07 20:06:09,002 E 14204 14204] logging.cc:496: *** SIGABRT received at time=1754622368 on cpu 7 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m [2025-08-07 20:06:09,002 E 14204 14204] logging.cc:496: PC: @     0x7f94323f59fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m [2025-08-07 20:06:09,003 E 14204 14204] logging.cc:496:     @     0x7f94323a1520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 6x across cluster][0m
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m 2025-08-07 20:08:06.976776: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m 2025-08-07 20:06:08.997118: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=14204, ip=10.164.1.246)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m   File 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m "
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m "
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m , line 330 in <module>
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m , regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas
[36m(train_lm_task pid=2995, ip=10.164.2.166)[0m , sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m 2025-08-07 20:08:06.976964: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2995, ip=10.164.2.213)[0m 
[36m(train_lm_task pid=2802, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2802, ip=10.164.1.84)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2802, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2802, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2802, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2802, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2996, ip=10.164.3.26)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2996, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.26)[0m 
[36m(train_lm_task pid=2993, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2993, ip=10.164.2.214)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2993, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2993, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2993, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2993, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2803, ip=10.164.2.215)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2803, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2803, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2994, ip=10.164.2.223)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2994, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.223)[0m 
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m 2025-08-07 20:08:06.976153: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2554, ip=10.164.2.208)[0m 
[36m(train_lm_task pid=2993, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2993, ip=10.164.1.231)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2993, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2993, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2993, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2993, ip=10.164.1.231)[0m 
[36m(train_lm_task pid=2609, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2609, ip=10.164.1.96)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2609, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2609, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2609, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2609, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2799, ip=10.164.2.82)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2799, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2799, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2799, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2799, ip=10.164.3.7)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2799, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2799, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2799, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2799, ip=10.164.3.7)[0m 
[36m(train_lm_task pid=2995, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2995, ip=10.164.3.21)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2995, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2995, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2995, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2995, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=3188, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=3188, ip=10.164.2.91)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=3188, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=3188, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=3188, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=3188, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2994, ip=10.164.2.93)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2994, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.93)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:1
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m /job:jax_worker/replica:0/task:11
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: f93ca4c5409233e81576c221cc33ef8bf37331360c000000 Worker ID: 53ada80c95b96d851b723362a5cb7a60159e464e92e97758c27bb3a0 Node ID: 751218239a0e0e984c1e9dc6390dd9cb9f6944cf749024e3bf4dc32b Worker IP address: 10.164.3.21 Worker port: 10017 Worker PID: 2995 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m *** SIGABRT received at time=1754624013 on cpu 103 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m PC: @     0x7f9896aa09fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m     @     0x7f9896a4c520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m [2025-08-07 20:33:34,007 E 5178 5178] logging.cc:496: *** SIGABRT received at time=1754624014 on cpu 103 ***[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m [2025-08-07 20:33:34,007 E 5178 5178] logging.cc:496: PC: @     0x7f9896aa09fc  (unknown)  pthread_kill[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m [2025-08-07 20:33:34,007 E 5178 5178] logging.cc:496:     @     0x7f9896a4c520  (unknown)  (unknown)[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m Fatal Python error: Aborted[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m Stack (most recent call first):[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 85x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 17x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m 2025-08-07 20:33:34.001586: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=5178, ip=10.164.2.236)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m 2025-08-07 20:08:06.978327: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=2800, ip=10.164.2.134)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m 2025-08-07 20:33:34.021578: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, 
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m numpy.random._pcg64
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m , numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast
[36m(train_lm_task pid=4455, ip=10.164.2.253)[0m , sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=4260, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4260, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4259, ip=10.164.2.229)[0m 
[36m(train_lm_task pid=4361, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4361, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4361, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.252)[0m 
[36m(train_lm_task pid=4166, ip=10.164.2.252)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 5cc151dde5bee4acf61d0855c53cafd95721e1d90c000000 Worker ID: cdaa0549334a4f63c8e296cfc06020ba07d862cc61c9ebb84bf9df25 Node ID: 6389fb69c4e90234b014aaf6fe31ef93f9e9445b0da5c83e10274d7b Worker IP address: 10.164.2.229 Worker port: 10026 Worker PID: 4259 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=4844, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4844, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4844, ip=10.164.2.235)[0m 
[36m(train_lm_task pid=4245, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4245, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4245, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4927, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4927, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4927, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4548, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4548, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4548, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4160, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m 
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m *** SIGABRT received at time=1754624164 on cpu 56 ***[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m PC: @     0x7ffacbfa29fc  (unknown)  pthread_kill[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m     @     0x7ffacbf4e520  (unknown)  (unknown)[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m [2025-08-07 20:36:04,385 E 11159 11159] logging.cc:496: *** SIGABRT received at time=1754624164 on cpu 56 ***[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m [2025-08-07 20:36:04,385 E 11159 11159] logging.cc:496: PC: @     0x7ffacbfa29fc  (unknown)  pthread_kill[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m [2025-08-07 20:36:04,385 E 11159 11159] logging.cc:496:     @     0x7ffacbf4e520  (unknown)  (unknown)[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m Fatal Python error: Aborted[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m Stack (most recent call first):[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 55x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 10x across cluster][0m
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m 2025-08-07 20:36:04.379664: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=11159, ip=10.164.2.101)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4160, ip=10.164.2.242)[0m 2025-08-07 20:33:34.797135: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=4160, ip=10.164.2.242)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m 2025-08-07 20:36:04.379737: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=14206, ip=10.164.1.220)[0m 
[36m(train_lm_task pid=15074, ip=10.164.1.76)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=15074, ip=10.164.1.76)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=15074, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=15074, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=15074, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=15074, ip=10.164.1.76)[0m 
[36m(train_lm_task pid=13334, ip=10.164.2.102)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=13334, ip=10.164.2.102)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=13334, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13334, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13334, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=13334, ip=10.164.2.102)[0m 
[36m(train_lm_task pid=15316, ip=10.164.1.225)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=15316, ip=10.164.1.225)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=15316, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15316, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15316, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=15316, ip=10.164.1.225)[0m 
[36m(train_lm_task pid=11594, ip=10.164.2.209)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=11594, ip=10.164.2.209)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=11594, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11594, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11594, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=11594, ip=10.164.2.209)[0m 
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m 2025-08-07 20:36:04.378930: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=28818, ip=10.164.2.83)[0m 
[36m(train_lm_task pid=7680, ip=10.164.1.200)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=7680, ip=10.164.1.200)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=7680, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7680, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7680, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=7680, ip=10.164.1.200)[0m 
[36m(train_lm_task pid=14011, ip=10.164.2.151)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=14011, ip=10.164.2.151)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=14011, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14011, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14011, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=14011, ip=10.164.2.151)[0m 
[36m(train_lm_task pid=15073, ip=10.164.2.73)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=15073, ip=10.164.2.73)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=15073, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=15073, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=15073, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=15073, ip=10.164.2.73)[0m 
[36m(train_lm_task pid=15318, ip=10.164.1.230)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=15318, ip=10.164.1.230)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=15318, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15318, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15318, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=15318, ip=10.164.1.230)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.223)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=14881, ip=10.164.1.223)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=14881, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=14881, ip=10.164.1.223)[0m 
[36m(train_lm_task pid=16814, ip=10.164.2.217)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=16814, ip=10.164.2.217)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=16814, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16814, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16814, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=16814, ip=10.164.2.217)[0m 
[36m(train_lm_task pid=13336, ip=10.164.2.160)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=13336, ip=10.164.2.160)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=13336, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13336, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13336, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=13336, ip=10.164.2.160)[0m 
[36m(train_lm_task pid=14205, ip=10.164.1.246)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=14205, ip=10.164.1.246)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=14205, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14205, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14205, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=14205, ip=10.164.1.246)[0m 
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m /job:jax_worker/replica:0/task:3
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m /job:jax_worker/replica:0/task:22
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m 
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: b8c0441e19689be5a336c2a4f8b0ea6780c3b00a0c000000 Worker ID: 73388c70297aa3ade9c46e68a573e262df9664f3a58a86ea23023e57 Node ID: bc57a5129e00ac2b9202fed9ae396211286d29d1a8f6559389c356b4 Worker IP address: 10.164.2.101 Worker port: 10028 Worker PID: 11159 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 11x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m 
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m *** SIGABRT received at time=1754624299 on cpu 9 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m PC: @     0x7f7ea17309fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m     @     0x7f7ea16dc520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m [2025-08-07 20:38:19,016 E 2996 2996] logging.cc:496: *** SIGABRT received at time=1754624299 on cpu 9 ***[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m [2025-08-07 20:38:19,016 E 2996 2996] logging.cc:496: PC: @     0x7f7ea17309fc  (unknown)  pthread_kill[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m [2025-08-07 20:38:19,016 E 2996 2996] logging.cc:496:     @     0x7f7ea16dc520  (unknown)  (unknown)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m Fatal Python error: Aborted[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m Stack (most recent call first):[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 80x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m 2025-08-07 20:38:19.010448: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2996, ip=10.164.3.21)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m 2025-08-07 20:36:04.379553: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.[32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m # of tasks that reached the barrier: 16/32.[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:[32m [repeated 15x across cluster][0m
[36m(train_lm_task pid=18119, ip=10.164.2.148)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError=''][32m [repeated 14x across cluster][0m
[36m(train_lm_task pid=2996, ip=10.164.2.166)[0m 2025-08-07 20:38:19.012294: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded
[36m(train_lm_task pid=2996, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.166)[0m RPC: /tensorflow.CoordinationService/RegisterTask
[36m(train_lm_task pid=2996, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=2996, ip=10.164.2.166)[0m 
[36m(train_lm_task pid=3189, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=3189, ip=10.164.2.91)[0m 
[36m(train_lm_task pid=3189, ip=10.164.2.91)[0m 
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 0435f4061b889bda904eefe94b14a0bbf9e0e3420c000000 Worker ID: 3ca7ea69fada0933f37fd068ec611736673801f64754721cd7a6ea76 Node ID: a4cd1da26a84e5e12cecd974d2acd594e800c56b5d3773dfc963fcb7 Worker IP address: 10.164.2.166 Worker port: 10018 Worker PID: 2996 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 16x across cluster][0m
[36m(train_lm_task pid=2801, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2801, ip=10.164.2.134)[0m 
[36m(train_lm_task pid=2803, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2803, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2803, ip=10.164.1.84)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2804, ip=10.164.2.215)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2800, ip=10.164.2.82)[0m 
[36m(train_lm_task pid=2610, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2610, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2610, ip=10.164.1.96)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m 
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m 
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m *** SIGABRT received at time=1754625809 on cpu 13 ***[32m [repeated 9x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m PC: @     0x7f313307e9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m     @     0x7f313302a520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m [2025-08-07 20:38:19,588 E 2994 2994] logging.cc:496: *** SIGABRT received at time=1754624299 on cpu 2 ***[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m [2025-08-07 20:38:19,588 E 2994 2994] logging.cc:496: PC: @     0x7f313307e9fc  (unknown)  pthread_kill[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m [2025-08-07 20:38:19,588 E 2994 2994] logging.cc:496:     @     0x7f313302a520  (unknown)  (unknown)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m Fatal Python error: Aborted[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m Stack (most recent call first):[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/trainer.py", line 983 in initialize[32m [repeated 40x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/submodules/levanter/src/levanter/main/train_lm.py", line 100 in main[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m   File "/tmp/ray/session_2025-07-26_17-57-50_381449_755/runtime_resources/working_dir_files/_ray_pkg_59b88c775c4e0044/marin/training/training.py", line 194 in train_lm_task[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 946 in main_loop[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/workers/default_worker.py", line 330 in <module>[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)[32m [repeated 8x across cluster][0m
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m 2025-08-07 21:03:29.353981: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4928, ip=10.164.2.234)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m 2025-08-07 20:38:19.582610: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Deadline Exceeded[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=2994, ip=10.164.2.214)[0m RPC: /tensorflow.CoordinationService/RegisterTask[32m [repeated 7x across cluster][0m
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m 2025-08-07 21:03:29.354136: F external/xla/xla/pjrt/distributed/client.h:88] Terminating process because the JAX distributed service detected fatal errors. This most likely indicates that another task died; see the other task logs for more details. Disable Python buffering, i.e. `python -u`, to be sure to see all the previous output. absl::Status: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m # of tasks that reached the barrier: 16/32.
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m The first task at the barrier: /job:jax_worker/replica:0/task:0. Some timed out task names:
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m RPC: /tensorflow.CoordinationService/RegisterTask [type.googleapis.com/tensorflow.CoordinationServiceError='']
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py"
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m , line 946
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m  in 
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m main_loop
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m 
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m , pandas._libs.tslibs.fields
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m "
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m /home/ray/anaconda3/lib/python3.11/site-packages/jax/_src/distributed.py
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m "
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m 
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m , pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy
[36m(train_lm_task pid=4438, ip=10.164.2.241)[0m , scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m , line 150 in initialize
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m 
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, charset_normalizer.md, uvloop.loop, ray._raylet, jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, lz4._version, lz4.frame._frame, pyarrow.lib, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._json, regex._regex, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, _cyutility, scipy._cyutility, scipy.sparse._csparsetools, scipy.special._ufuncs_cxx
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m , 
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m scipy.special._ellip_harm_2
[36m(train_lm_task pid=4549, ip=10.164.2.231)[0m , scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=4914, ip=10.164.2.240)[0m , scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, scipy.stats._qmvnt_cy, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, lxml._elementpath, lxml.etree, sentencepiece._sentencepiece (total: 212)
[36m(train_lm_task pid=4554, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4554, ip=10.164.2.232)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4554, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4554, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4554, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4554, ip=10.164.2.232)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4161, ip=10.164.2.242)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4161, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4161, ip=10.164.2.242)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4455, ip=10.164.2.247)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4455, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4455, ip=10.164.2.247)[0m 
[36m(train_lm_task pid=4453, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4453, ip=10.164.2.237)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4453, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4453, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4453, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=4453, ip=10.164.2.237)[0m 
[36m(train_lm_task pid=5371, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=5371, ip=10.164.2.236)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=5371, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5371, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5371, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=5371, ip=10.164.2.236)[0m 
[36m(train_lm_task pid=4576, ip=10.164.2.250)[0m 2025-08-07 21:03:29.353505: E external/xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:1436] Stopping coordination service as cluster registration failed. This may be due to 1) some tasks crashed earlier before connecting, 2) some tasks were never scheduled, or 3) scheduling delays. Consider setting a longer initialization timeout if such delays are expected, the timeout is currently set to: 1h.
[36m(train_lm_task pid=4576, ip=10.164.2.250)[0m 
[36m(train_lm_task pid=4576, ip=10.164.2.250)[0m Original error: DEADLINE_EXCEEDED: Barrier timed out. Id: [Init]Wait_for_all_tasks_to_register::0. This usually happens because a task triggered the barrier too early or too slowly. Please look at the task logs (both timed out and first task) to debug further.
[36m(train_lm_task pid=4576, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:13
[36m(train_lm_task pid=4576, ip=10.164.2.250)[0m /job:jax_worker/replica:0/task:6
[36m(train_lm_task pid=4576, ip=10.164.2.250)[0m  [type.googleapis.com/tensorflow.CoordinationServiceError=''] [type.googleapis.com/tensorflow.BarrierError='\n$[Init]Wait_for_all_tasks_to_register']
